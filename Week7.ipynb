{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df_abstract = pd.read_csv('my_data/df_CSS_paper_abstract.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "text = PlaintextCorpusReader('my_data/', '.*')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\August\\\\Documents\\\\Github\\\\comsocsci2023\\\\my_data\\\\test_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest_text.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:76\u001B[0m, in \u001B[0;36mPlaintextCorpusReader.words\u001B[1;34m(self, fileids)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwords\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     68\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;124;03m    :return: the given file(s) as a list of words\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m        and punctuation symbols.\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    :rtype: list(str)\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m concat(\n\u001B[0;32m     74\u001B[0m         [\n\u001B[0;32m     75\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mCorpusView(path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_word_block, encoding\u001B[38;5;241m=\u001B[39menc)\n\u001B[1;32m---> 76\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m (path, enc, fileid) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabspaths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m         ]\n\u001B[0;32m     78\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:195\u001B[0m, in \u001B[0;36mCorpusReader.abspaths\u001B[1;34m(self, fileids, include_encoding, include_fileid)\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fileids, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    193\u001B[0m     fileids \u001B[38;5;241m=\u001B[39m [fileids]\n\u001B[1;32m--> 195\u001B[0m paths \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_root\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_encoding \u001B[38;5;129;01mand\u001B[39;00m include_fileid:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(paths, [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids], fileids))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:195\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fileids, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    193\u001B[0m     fileids \u001B[38;5;241m=\u001B[39m [fileids]\n\u001B[1;32m--> 195\u001B[0m paths \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_root\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids]\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_encoding \u001B[38;5;129;01mand\u001B[39;00m include_fileid:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(paths, [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids], fileids))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:334\u001B[0m, in \u001B[0;36mFileSystemPathPointer.join\u001B[1;34m(self, fileid)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjoin\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileid):\n\u001B[0;32m    333\u001B[0m     _path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path, fileid)\n\u001B[1;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFileSystemPathPointer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\compat.py:41\u001B[0m, in \u001B[0;36mpy3_data.<locals>._decorator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decorator\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     40\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args[\u001B[38;5;241m0\u001B[39m], add_py3_data(args[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m args[\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minit_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:312\u001B[0m, in \u001B[0;36mFileSystemPathPointer.__init__\u001B[1;34m(self, _path)\u001B[0m\n\u001B[0;32m    310\u001B[0m _path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(_path)\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(_path):\n\u001B[1;32m--> 312\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo such file or directory: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m _path)\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path \u001B[38;5;241m=\u001B[39m _path\n",
      "\u001B[1;31mOSError\u001B[0m: No such file or directory: 'C:\\\\Users\\\\August\\\\Documents\\\\Github\\\\comsocsci2023\\\\my_data\\\\test_text.txt'"
     ]
    }
   ],
   "source": [
    "text.words('test_text.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('my_data/test_text.txt') as text_file:\n",
    "    raw = text_file.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "#tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "tokens = [token.lower() for token in tokens]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def remove_digits(l):\n",
    "    pattern = r'[0-9\\p{P}]'\n",
    "    l = [re.sub(pattern, '', token) for token in l]\n",
    "    return l"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    tokens = remove_digits(tokens)\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def clean_strings(strings):\n",
    "    cleaned_strings = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for string in strings:\n",
    "        # Remove URLs\n",
    "        string = re.sub(r'http\\S+', '', string)\n",
    "\n",
    "        # Remove numbers\n",
    "        string = re.sub(r'[0-9]', '', string)\n",
    "\n",
    "        # Keep only what is not punctuation\n",
    "        string = re.sub(r'[^\\w\\s]', '', string)\n",
    "\n",
    "        # Remove stop words and moke them\n",
    "        if string not in stop_words: string = string.lower()\n",
    "\n",
    "        # Remove empty strings\n",
    "        if len(string):\n",
    "            cleaned_strings.append(string)\n",
    "\n",
    "    return cleaned_strings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(clean_strings(tokens))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\August\\AppData\\Local\\Temp\\ipykernel_12496\\4017611902.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract['tokens'][i] = clean_tokens\n",
      "35593it [00:33, 1053.20it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = []\n",
    "df_abstract['tokens'] = None\n",
    "for i, row in tqdm(df_abstract.iterrows()):\n",
    "    abstract = row['abstract']\n",
    "    if type(abstract) == str:\n",
    "\n",
    "        tokens = nltk.word_tokenize(row['abstract'])\n",
    "        clean_tokens = clean_strings(tokens)\n",
    "        cleaned_corpus.append(clean_tokens)\n",
    "        df_abstract['tokens'][i] = clean_tokens\n",
    "    else:\n",
    "        cleaned_corpus.append(None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "graph = nx.read_graphml('my_data/CSS_graph.graphml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def louvain_communities(graph):\n",
    "    # Use the louvain method to find communities\n",
    "    partition = community.best_partition(graph) # community function that uses Louvain-algorithm\n",
    "\n",
    "    # Reformat the partitioning\n",
    "    communities = {}\n",
    "    for node, community_id in partition.items():\n",
    "        if community_id in communities:\n",
    "            communities[community_id].append(node)\n",
    "        else:\n",
    "            communities[community_id] = [node]\n",
    "\n",
    "    return list(communities.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "communities = np.load('my_data/CSS_louvain_groups.npy', allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "0        [the, term, linked, data, refers, to, a, set, ...\n1        [the, term, linked, data, refers, to, a, set, ...\n2        [the, term, linked, data, refers, to, a, set, ...\n3                                                     None\n4        [the, era, of, big, data, has, begun, computer...\n                               ...                        \n35588    [the, digitization, of, matrimonial, matchmaki...\n35589    [the, misuse, of, social, media, platforms, to...\n35590    [with, twitter, growing, as, a, preferred, cha...\n35591    [with, twitter, growing, as, a, preferred, cha...\n35592    [this, chapter, concludes, that, health, promo...\nName: tokens, Length: 35593, dtype: object"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abstract['tokens']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 7162/35593 [07:26<29:32, 16.04it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m authors \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauthors\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m---> 11\u001B[0m     authors \u001B[38;5;241m=\u001B[39m \u001B[43mast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mliteral_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauthors\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m authorID \u001B[38;5;129;01min\u001B[39;00m authors:\n\u001B[0;32m     13\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:110\u001B[0m, in \u001B[0;36mliteral_eval\u001B[1;34m(node_or_string)\u001B[0m\n\u001B[0;32m    108\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m left \u001B[38;5;241m-\u001B[39m right\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _convert_signed_num(node)\n\u001B[1;32m--> 110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_or_string\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:84\u001B[0m, in \u001B[0;36mliteral_eval.<locals>._convert\u001B[1;34m(node)\u001B[0m\n\u001B[0;32m     82\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m operand\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _convert_num(node)\n\u001B[1;32m---> 84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert\u001B[39m(node):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node, Constant):\n\u001B[0;32m     86\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m node\u001B[38;5;241m.\u001B[39mvalue\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "all_communities = []\n",
    "#Test if pandas explode is more efficient\n",
    "# Den her kan gøre meget bedre tror jeg, skal bare lige tænke mig lidt om\n",
    "    for i, row in tqdm(df_abstract.iterrows(), total = df_abstract.shape[0]):\n",
    "        for community in communities:\n",
    "            community_tokens = []\n",
    "            for authorID in community:\n",
    "                authors = []\n",
    "                if type(row['authors']) == str:\n",
    "                    authors = ast.literal_eval(row['authors'])\n",
    "                if authorID in authors:\n",
    "                    tokens = None\n",
    "                    if type(row['tokens']) == str:\n",
    "                        tokens = ast.literal_eval(row['tokens'])\n",
    "                    community_tokens.append(tokens)\n",
    "            all_communities.append(community_tokens)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "community_docs = np.load(\"my_data/community_documents.npy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "CSS_giant = nx.read_graphml('my_data/CSS_giant.graphml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35593it [00:06, 5915.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokens of the community documents if they are already saved\n",
    "DATA_PATH = \"my_data/\"\n",
    "\n",
    "# Create a document for each community that contains the combined abstracts of all papers in that community\n",
    "community_documents = [[] for _ in range(len(communities))]\n",
    "\"\"\"\n",
    "Nyt\n",
    "\"\"\"\n",
    "titles_in_comm = [[] for _ in range(len(communities))] # bruger jeg senere, kan nok gøres smarter\n",
    "# Create a dictionary that maps nodes to communities\n",
    "node_to_community = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        node_to_community[node] = i\n",
    "\n",
    "# Loop over all papers\n",
    "for i, paper in tqdm(df_abstract.iterrows()):\n",
    "    # We need the authors and the tokens\n",
    "    authors = []\n",
    "    if type(paper['authors']) == str:\n",
    "        authors = ast.literal_eval(paper['authors'])\n",
    "    if type(paper['tokens']) == str:\n",
    "        tokens = ast.literal_eval(paper['tokens'])\n",
    "\n",
    "    # Loop over all authors of the paper\n",
    "    for author in authors:\n",
    "    # If the author is in the giant component, add the tokens to the community document\n",
    "        if author in CSS_giant.nodes:\n",
    "            community_documents[node_to_community[author]] += tokens\n",
    "            titles_in_comm[node_to_community[author]].append(paper['title'])\n",
    "\n",
    "# Save the community documents to a file\n",
    "#np.save(DATA_PATH + 'community_documents.npy', community_documents)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#get top 5 largest communities and corresponding tokens\n",
    "sorted_lists = sorted(zip(communities, community_documents), key=lambda x: len(x[0]))\n",
    "\n",
    "sorted_communities, sorted_community_documents = zip(*sorted_lists)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_communities[-5:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:07<00:00, 20.53s/it]\n"
     ]
    }
   ],
   "source": [
    "token_counter_dict = {}\n",
    "for i, comm_tokens in enumerate(reversed(sorted_community_documents[-15:])):\n",
    "    token_counter_dict[i] = {}\n",
    "    unique_tokens = list(set(comm_tokens))\n",
    "    for unique_token in unique_tokens:\n",
    "        token_counter_dict[i][unique_token] = 0\n",
    "\n",
    "\n",
    "for i, comm_tokens in tqdm(enumerate(reversed(sorted_community_documents[-15:])), total = 15):\n",
    "    unique_tokens = list(set(comm_tokens))\n",
    "    for unique_token in unique_tokens:\n",
    "        token_counter_dict[i][unique_token] += comm_tokens.count(unique_token)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def get_top_words_community(token_counter_dict, community, n_community):\n",
    "    values = token_counter_dict[community].values()\n",
    "    keys = token_counter_dict[community].keys()\n",
    "    sorted_lists = sorted(list(zip(values, keys)), key=lambda x: x[0])\n",
    "    values, keys = zip(*sorted_lists)\n",
    "\n",
    "    return (values[-n_community:], keys[-n_community:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((14326, 16423, 23205, 23567, 26353), ('in', 'to', 'and', 'of', 'the'))\n",
      "((14326, 16423, 23205, 23567, 26353), ('in', 'to', 'and', 'of', 'the'))\n",
      "((14326, 16423, 23205, 23567, 26353), ('in', 'to', 'and', 'of', 'the'))\n",
      "((14326, 16423, 23205, 23567, 26353), ('in', 'to', 'and', 'of', 'the'))\n",
      "((14326, 16423, 23205, 23567, 26353), ('in', 'to', 'and', 'of', 'the'))\n"
     ]
    }
   ],
   "source": [
    "for community in range(5):\n",
    "    print(get_top_words_community(token_counter_dict, 0, 5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kan her ses at det er dårlig måde at adskille dem fordi der er for mange ord som er for generelle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "term_in_doc_counter = {}\n",
    "for i, paper in df_abstract.iterrows():\n",
    "    if type(paper['tokens']) == str:\n",
    "        tokens = set(ast.literal_eval(paper['tokens']))\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                term_in_doc_counter[token] += 1\n",
    "            except:\n",
    "                term_in_doc_counter[token] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "idf_dict = {}\n",
    "n_documents = len(df_abstract) #though there are papers with no tokens...\n",
    "for term in term_in_doc_counter.keys():\n",
    "    idf_dict[term] = np.log(n_documents/term_in_doc_counter[term]) #Don't know which log to use, or the difference it makes\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "community_documents_counter = {}\n",
    "for i, comm_titles in enumerate(titles_in_comm):\n",
    "    community_documents_counter[i] = len(set(comm_titles))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "sorted_lists = sorted(zip(communities, community_documents_counter), key=lambda x: len(x[0]))\n",
    "\n",
    "sorted_communities, sorted_community_documents_counter = zip(*sorted_lists)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_in_doc_counter = {}\n",
    "for i, paper in df_abstract.iterrows():\n",
    "    if type(paper['tokens']) == str:\n",
    "        tokens = set(ast.literal_eval(paper['tokens']))\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                term_in_doc_counter[token] += 1\n",
    "            except:\n",
    "                term_in_doc_counter[token] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

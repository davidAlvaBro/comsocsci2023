{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in list of authorIds and outputs the corresponding top fields in the same order \n",
    "def author_field(ids, df_author):\n",
    "    \"\"\"\n",
    "    Compute the top field of the given authors \n",
    "\n",
    "    Args:\n",
    "        ids (list): containing the ids of the authors in question\n",
    "        df_author (pandas df): data frame of type author that contains the given ids \n",
    "    \n",
    "    return: \n",
    "        fields (list): list of top fields of the given authors \n",
    "\n",
    "    \"\"\"\n",
    "    # Get boolean array that indicates where the authors are \n",
    "    mask = df_author[\"id\"].isin(ids)\n",
    "    \n",
    "    # Get the fields \n",
    "    fields = list(df_author[\"field\"][mask]) \n",
    "    \n",
    "    return fields \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argument_paper_dataframe(df_papers, df_author):\n",
    "    \"\"\"\n",
    "    Takes in a paper data frame, arguments it with a new column and puts the fields of the author in that column.\n",
    "\n",
    "    Args:\n",
    "        df_papers (pandas dataframe): paper dataframe (like before)\n",
    "        df_author (pandas dataframe): author dataframe (like before)\n",
    "\n",
    "    Returns:\n",
    "        df_papers (pandas dataframe): the dataframe from before, argumentet with the new column\n",
    "    \"\"\"\n",
    "    df_papers[\"author_field\"] = None \n",
    "\n",
    "    for index, row in tqdm(df_papers.iterrows()):\n",
    "        # Currently authors are stored as a string representation of the list so we make it a list again \n",
    "        authors = ast.literal_eval(row[\"authors\"]) \n",
    "        # Now we need to turn the authors into a list of integers, because the df_paper dataframe stores them as such \n",
    "        authors = [eval(id) for id in authors if id is not None]\n",
    "        #authors = [eval(id) for id in authors]\n",
    "        # Find the fields of the authors \n",
    "        authors_fields = author_field(authors, df_author=df_author)\n",
    "        df_papers[\"author_field\"][index] = authors_fields\n",
    "        # print(authors_fields) # debugging \n",
    "    # print(df_papers.head()) # debugging \n",
    "        \n",
    "    return df_papers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the paper datafram to a social science dataframe\n",
    "def generate_CCS_papers_1(df_papers, social_science_fields, quantitative_fields, verbose=False):\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if the fields are not not included in social science fields\n",
    "    for index, row in df_papers.iterrows():\n",
    "        is_in_SCF = False \n",
    "        # Go through the entire list of fields for each paper \n",
    "        try: \n",
    "            for field in ast.literal_eval(row[\"field\"]):\n",
    "                if field[\"category\"] in social_science_fields:\n",
    "                    is_in_SCF = True\n",
    "        except: \n",
    "            is_in_SCF = False \n",
    "        \n",
    "        # if the paper was not in social science fields drop it \n",
    "        if not is_in_SCF: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because thier fields not in Social Science Fields. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop rows that are before 2008\n",
    "    for index, row in tqdm(df_papers.iterrows()):\n",
    "        if row[\"year\"] <= 2008: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because they were to old. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "        \n",
    "    # Drop rows that do not contain a DOI\n",
    "    for index, row in df_papers.iterrows():\n",
    "        if row[\"doi\"] == None: \n",
    "            rows_to_drop.append(index)\n",
    "        else: \n",
    "            try:\n",
    "                ast.literal_eval(row[\"doi\"])[0][\"DOI\"]\n",
    "            except: \n",
    "                rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because they did not have a DOI. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if it includes biology\n",
    "    for index, row in df_papers.iterrows():\n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] == \"Biology\":\n",
    "                rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because biology was in the field. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    return df_papers\n",
    "\n",
    "\n",
    "# It is time consuming to add a row in the dataframe, hence it helps that the dataframe is 100 times smaller \n",
    "def generate_CCS_papers_2(df_papers, social_science_fields, quantitative_fields, verbose=False):\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop the papers with more than 9 Computational Social Science authors? TODO what does she mean! \n",
    "    for index, row in df_papers.iterrows():\n",
    "        if len(row[\"author_field\"]) > 9: # Count authors, should I check if they are in the author data frame? That's gonna take a while\n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because there are more than 9 CSS authors. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if the fields are not not included in quantitative data and authors aren't either\n",
    "    for index, row in df_papers.iterrows():\n",
    "        is_in_SCF = False \n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] in quantitative_fields:\n",
    "                is_in_SCF = True\n",
    "        \n",
    "        # Check if the authors are in the quantitative_fields\n",
    "        for author_field in row[\"author_field\"]: \n",
    "            if author_field in quantitative_fields: \n",
    "                is_in_SCF = True\n",
    "       \n",
    "        # if the paper was not in social science fields drop it \n",
    "        if not is_in_SCF: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because thier fields and authors are not in Quantitative Fields. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    return df_papers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially we have 1078817 papers.\n",
      "1010697 papers removed because thier fields not in Social Science Fields. (94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68120it [00:02, 28072.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20011 papers removed because they were to old. (29%)\n",
      "11330 papers removed because they did not have a DOI. (24%)\n",
      "98 papers removed because biology was in the field. (0%)\n",
      "There are 36681 papers left of 1078817 which is 3.40%\n"
     ]
    }
   ],
   "source": [
    "# Heuristic \n",
    "social_science_fields = {\"Political Science\", \"Sociology\", \"Economics\"}\n",
    "quantitative_fields = {\"Mathematics\", \"Physics\", \"Computer Science\"}\n",
    "\n",
    "# Load dataframes\n",
    "df_author = pd.read_csv(\"df_author.csv\")\n",
    "df_papers = pd.read_csv(\"df_paper.csv\")\n",
    "\n",
    "# Drop papers, but not based on the authors\n",
    "n_papers_before = len(df_papers)\n",
    "\n",
    "print(f\"Initially we have {n_papers_before} papers.\")\n",
    "df_papers = generate_CCS_papers_1(df_papers, social_science_fields, quantitative_fields, verbose=True)\n",
    "\n",
    "# Papers removed so far \n",
    "print(f\"There are {len(df_papers)} papers left of {n_papers_before} which is {100*len(df_papers)/n_papers_before:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_11632/3790806285.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_papers[\"author_field\"][index] = authors_fields\n",
      "36681it [00:44, 821.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now the time consuming part \n",
    "# Argment paper dataframe \n",
    "df_papers = argument_paper_dataframe(df_papers=df_papers, df_author=df_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 papers removed because there are more than 9 CSS authors. (0%)\n",
      "31084 papers removed because thier fields and authors are not in Quantitative Fields. (85%)\n",
      "There are 5432 papers left of 1078817 which is 0.50%\n"
     ]
    }
   ],
   "source": [
    "# Drop papers based on authors \n",
    "df_papers = generate_CCS_papers_2(df_papers, social_science_fields, quantitative_fields, verbose=True)\n",
    "\n",
    "print(f\"There are {len(df_papers)} papers left of {n_papers_before} which is {100*len(df_papers)/n_papers_before:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 403 that were duplicates. (92.58%)\n",
      "There are 5029 left\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates \n",
    "n_papers = len(df_papers)\n",
    "df_papers.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"], inplace=True) # weird columns that were somehow created? \n",
    "df_papers.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "print(f\"Removed {n_papers - len(df_papers)} that were duplicates. ({100*(n_papers - len(df_papers)) / n_papers :.2f}%)\")\n",
    "print(f\"There are {len(df_papers)} left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agumentet dataframe \n",
    "pd.DataFrame.to_csv(df_papers, \"df_CSS_paper.csv\")\n",
    "\n",
    "# TODO Check how many unique authors have written these papers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CRITICAL QUESTIONS FOR BIG DATA' has 3422 citations.\n",
      "'I tweet honestly, I tweet passionately: Twitter users, context collapse, and the imagined audience' has 3042 citations.\n",
      "'Exposure to ideologically diverse news and opinion on Facebook' has 1925 citations.\n",
      "'The sharing economy: Why people participate in collaborative consumption' has 1901 citations.\n",
      "'The role of social networks in information diffusion' has 1430 citations.\n",
      "'Social Network Sites as Networked Publics: Affordances, Dynamics, and Implications' has 1349 citations.\n",
      "'Multiscale mobility networks and the spatial spreading of infectious diseases' has 1156 citations.\n",
      "'The Leverage Cycle' has 1145 citations.\n",
      "'The ethics of algorithms: Mapping the debate' has 973 citations.\n",
      "'The Presentation of Self in the Age of Social Media: Distinguishing Performances and Exhibitions Online' has 954 citations.\n"
     ]
    }
   ],
   "source": [
    "# Print 10 top papers \n",
    "df_papers.sort_values(by=[\"citationCount\"], ascending=False, inplace=True)\n",
    "for i, paper in df_papers.head(10).iterrows():\n",
    "    print(f\"'{paper['title']}' has {int(paper['citationCount'])} citations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 5029 papers there are 13338 unique authors, but only 1466 are from our original data frame.\n"
     ]
    }
   ],
   "source": [
    "# Get unique authors \n",
    "authors = set() \n",
    "\n",
    "for i, paper in df_papers.iterrows(): \n",
    "    paper_authors = set(ast.literal_eval(paper[\"authors\"]))\n",
    "    authors.update(paper_authors)\n",
    "    \n",
    "if None in authors: # If no is in the author list \n",
    "    authors.remove(None)\n",
    "\n",
    "authors = set([int(id) for id in authors]) # convert strings to ints to compare them\n",
    "total_CSS_authors = set([int(author) for author in df_author[\"id\"]])\n",
    "\n",
    "CSS_authors = authors & total_CSS_authors\n",
    "    \n",
    "print(f\"From the {len(df_papers)} papers there are {len(authors)} unique authors, but only {len(CSS_authors)} are from our original data frame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eedd9fc418f36e2614580cbf31de2386521dabe9ec8a80375aa0fadc40a33bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

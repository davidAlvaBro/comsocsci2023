{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in list of authorIds and outputs the corresponding top fields in the same order \n",
    "def author_field(ids, df_author):\n",
    "    \"\"\"\n",
    "    Compute the top field of the given authors \n",
    "\n",
    "    Args:\n",
    "        ids (list): containing the ids of the authors in question\n",
    "        df_author (pandas df): data frame of type author that contains the given ids \n",
    "    \n",
    "    return: \n",
    "        fields (list): list of top fields of the given authors \n",
    "\n",
    "    \"\"\"\n",
    "    # Get boolean array that indicates where the authors are \n",
    "    mask = df_author[\"id\"].isin(ids)\n",
    "    \n",
    "    # Get the fields \n",
    "    fields = list(df_author[\"field\"][mask]) \n",
    "    \n",
    "    return fields \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argument_paper_dataframe(df_papers, df_author):\n",
    "    \"\"\"\n",
    "    Takes in a paper data frame, arguments it with a new column and puts the fields of the author in that column.\n",
    "\n",
    "    Args:\n",
    "        df_papers (pandas dataframe): paper dataframe (like before)\n",
    "        df_author (pandas dataframe): author dataframe (like before)\n",
    "\n",
    "    Returns:\n",
    "        df_papers (pandas dataframe): the dataframe from before, argumentet with the new column\n",
    "    \"\"\"\n",
    "    df_papers[\"author_field\"] = None \n",
    "\n",
    "    for index, row in tqdm(df_papers.iterrows()):\n",
    "        # Currently authors are stored as a string representation of the list so we make it a list again \n",
    "        authors = ast.literal_eval(row[\"authors\"]) \n",
    "        # Now we need to turn the authors into a list of integers, because the df_paper dataframe stores them as such \n",
    "        authors = [eval(id) for id in authors if id is not None]\n",
    "        #authors = [eval(id) for id in authors]\n",
    "        # Find the fields of the authors \n",
    "        authors_fields = author_field(authors, df_author=df_author)\n",
    "        df_papers[\"author_field\"][index] = authors_fields\n",
    "        # print(authors_fields) # debugging \n",
    "    # print(df_papers.head()) # debugging \n",
    "        \n",
    "    return df_papers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the paper datafram to a social science dataframe\n",
    "def generate_CCS_papers_1(df_papers, social_science_fields, quantitative_fields, verbose=False):\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if the fields are not not included in social science fields\n",
    "    for index, row in df_papers.iterrows():\n",
    "        is_in_SCF = False \n",
    "        # Go through the entire list of fields for each paper \n",
    "        try: \n",
    "            for field in ast.literal_eval(row[\"field\"]):\n",
    "                if field[\"category\"] in social_science_fields:\n",
    "                    is_in_SCF = True\n",
    "        except: \n",
    "            is_in_SCF = False \n",
    "        \n",
    "        # if the paper was not in social science fields drop it \n",
    "        if not is_in_SCF: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because thier fields not in Social Science Fields. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop rows that are before 2008\n",
    "    for index, row in tqdm(df_papers.iterrows()):\n",
    "        if row[\"year\"] <= 2008: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because they were to old. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "        \n",
    "    # Drop rows that do not contain a DOI\n",
    "    for index, row in df_papers.iterrows():\n",
    "        if row[\"doi\"] == None: \n",
    "            rows_to_drop.append(index)\n",
    "        else: \n",
    "            try:\n",
    "                ast.literal_eval(row[\"doi\"])[0][\"DOI\"]\n",
    "            except: \n",
    "                rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because they did not have a DOI. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if it includes biology\n",
    "    for index, row in df_papers.iterrows():\n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] == \"Biology\":\n",
    "                rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because biology was in the field. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    return df_papers\n",
    "\n",
    "\n",
    "# It is time consuming to add a row in the dataframe, hence it helps that the dataframe is 100 times smaller \n",
    "def generate_CCS_papers_2(df_papers, social_science_fields, quantitative_fields, verbose=False):\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop the papers with more than 9 Computational Social Science authors? TODO what does she mean! \n",
    "    for index, row in df_papers.iterrows():\n",
    "        if len(row[\"author_field\"]) > 9: # Count authors, should I check if they are in the author data frame? That's gonna take a while\n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because there are more than 9 CSS authors. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if the fields are not not included in quantitative data and authors aren't either\n",
    "    for index, row in df_papers.iterrows():\n",
    "        is_in_SCF = False \n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] in quantitative_fields:\n",
    "                is_in_SCF = True\n",
    "        \n",
    "        # Check if the authors are in the quantitative_fields\n",
    "        for author_field in row[\"author_field\"]: \n",
    "            if author_field in quantitative_fields: \n",
    "                is_in_SCF = True\n",
    "       \n",
    "        # if the paper was not in social science fields drop it \n",
    "        if not is_in_SCF: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because thier fields and authors are not in Quantitative Fields. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    return df_papers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially we have 1078817 papers.\n",
      "1010697 papers removed because thier fields not in Social Science Fields. (94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68120it [00:02, 28072.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20011 papers removed because they were to old. (29%)\n",
      "11330 papers removed because they did not have a DOI. (24%)\n",
      "98 papers removed because biology was in the field. (0%)\n",
      "There are 36681 papers left of 1078817 which is 3.40%\n"
     ]
    }
   ],
   "source": [
    "# Heuristic \n",
    "social_science_fields = {\"Political Science\", \"Sociology\", \"Economics\"}\n",
    "quantitative_fields = {\"Mathematics\", \"Physics\", \"Computer Science\"}\n",
    "\n",
    "# Load dataframes\n",
    "df_author = pd.read_csv(\"df_author.csv\")\n",
    "df_papers = pd.read_csv(\"df_paper.csv\")\n",
    "\n",
    "# Drop papers, but not based on the authors\n",
    "n_papers_before = len(df_papers)\n",
    "\n",
    "print(f\"Initially we have {n_papers_before} papers.\")\n",
    "df_papers = generate_CCS_papers_1(df_papers, social_science_fields, quantitative_fields, verbose=True)\n",
    "\n",
    "# Papers removed so far \n",
    "print(f\"There are {len(df_papers)} papers left of {n_papers_before} which is {100*len(df_papers)/n_papers_before:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argument_paper_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19432/1311575419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now the time consuming part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Argment paper dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_papers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margument_paper_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_papers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_papers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_author\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_author\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'argument_paper_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# Now the time consuming part \n",
    "# Argment paper dataframe \n",
    "df_papers = argument_paper_dataframe(df_papers=df_papers, df_author=df_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 papers removed because there are more than 9 CSS authors. (0%)\n",
      "31084 papers removed because thier fields and authors are not in Quantitative Fields. (85%)\n",
      "There are 5432 papers left of 1078817 which is 0.50%\n"
     ]
    }
   ],
   "source": [
    "# Drop papers based on authors \n",
    "df_papers = generate_CCS_papers_2(df_papers, social_science_fields, quantitative_fields, verbose=True)\n",
    "\n",
    "print(f\"There are {len(df_papers)} papers left of {n_papers_before} which is {100*len(df_papers)/n_papers_before:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 403 that were duplicates. (92.58%)\n",
      "There are 5029 left\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates \n",
    "n_papers = len(df_papers)\n",
    "df_papers.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"], inplace=True) # weird columns that were somehow created? \n",
    "df_papers.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "print(f\"Removed {n_papers - len(df_papers)} that were duplicates. ({100*(n_papers - len(df_papers)) / n_papers :.2f}%)\")\n",
    "print(f\"There are {len(df_papers)} left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agumentet dataframe \n",
    "pd.DataFrame.to_csv(df_papers, \"df_CSS_paper.csv\")\n",
    "\n",
    "# TODO Check how many unique authors have written these papers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe (don't wanna run everything above)\n",
    "df_CSS_papers = pd.read_csv(\"df_CSS_paper.csv\")\n",
    "df_authors = pd.read_csv(\"df_author.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Linked Data - The Story So Far' has 5527 citations.\n",
      "'The future of employment: How susceptible are jobs to computerisation?' has 5025 citations.\n",
      "'CRITICAL QUESTIONS FOR BIG DATA' has 3422 citations.\n",
      "'Network Analysis in the Social Sciences' has 3363 citations.\n",
      "'I tweet honestly, I tweet passionately: Twitter users, context collapse, and the imagined audience' has 3042 citations.\n",
      "'MatchIt: Nonparametric Preprocessing for Parametric Causal Inference' has 2816 citations.\n",
      "'Fixed Effects Regression Models' has 2739 citations.\n",
      "'Causal Inference without Balance Checking: Coarsened Exact Matching' has 2428 citations.\n",
      "'Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems' has 2355 citations.\n",
      "'Wikidata: a free collaborative knowledgebase' has 2133 citations.\n"
     ]
    }
   ],
   "source": [
    "# Print 10 top papers \n",
    "df_CSS_papers.sort_values(by=[\"citationCount\"], ascending=False, inplace=True)\n",
    "for i, paper in df_CSS_papers.head(10).iterrows():\n",
    "    print(f\"'{paper['title']}' has {int(paper['citationCount'])} citations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 23441 papers there are 49989 unique authors, but only 10727 are from our original data frame.\n"
     ]
    }
   ],
   "source": [
    "# Get unique authors \n",
    "authors = set() \n",
    "\n",
    "for i, paper in df_CSS_papers.iterrows(): \n",
    "    paper_authors = set(ast.literal_eval(paper[\"authors\"]))\n",
    "    authors.update(paper_authors)\n",
    "    \n",
    "if None in authors: # If no is in the author list \n",
    "    authors.remove(None)\n",
    "\n",
    "authors = set([int(id) for id in authors]) # convert strings to ints to compare them\n",
    "total_CSS_authors = set([int(author) for author in df_authors[\"id\"]])\n",
    "\n",
    "CSS_authors = authors & total_CSS_authors\n",
    "    \n",
    "print(f\"From the {len(df_CSS_papers)} papers there are {len(authors)} unique authors, but only {len(CSS_authors)} are from our original data frame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function \n",
    "def fix_authors(authors, CSS_authors):\n",
    "    #Fix string, none type and type errors\n",
    "    authors = ast.literal_eval(authors)\n",
    "    authors = [int(id) for id in authors if id != None]\n",
    "    authors = set(authors) & CSS_authors\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23441/23441 [00:00<00:00, 67039.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Got through all CSS papers and initialize a dictionary with collabaration \n",
    "author_dict = {}\n",
    "\n",
    "for authors in tqdm(df_CSS_papers['authors']):\n",
    "    authors = fix_authors(authors, CSS_authors=CSS_authors) # Get author in correct shape \n",
    "\n",
    "    sorted_authors = sorted(authors) # Sort authors to consistantly index dictionary \n",
    "    for i, author in enumerate(sorted_authors):\n",
    "            for coauthor in sorted_authors[i+1:]:\n",
    "                    author, coauthor = int(author), int(coauthor)\n",
    "                    try: \n",
    "                        author_dict[author, coauthor] += 1\n",
    "                    except:\n",
    "                        author_dict[author, coauthor] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15992/15992 [00:00<00:00, 1453136.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating weighted edges to use in network\n",
    "weighted_edges = []\n",
    "for author, coauthor in tqdm(author_dict.keys()):\n",
    "    weighted_edges.append((author, coauthor, author_dict[author, coauthor]))\n",
    "\n",
    "CSS_graph = nx.Graph()\n",
    "CSS_graph.add_nodes_from(CSS_authors)\n",
    "CSS_graph.add_weighted_edges_from(weighted_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10727/10727 [00:06<00:00, 1683.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting node atributes\n",
    "# First 2 atts\n",
    "author_atts = dict()\n",
    "\n",
    "for author in tqdm(list(CSS_authors)):\n",
    "    author_atts[author] = dict()\n",
    "\n",
    "    #Getting the name of each author\n",
    "    aliases = df_authors.loc[df_authors['id'] == author, 'aliases'].values[0]\n",
    "    if type(aliases) != list():\n",
    "        name = df_authors.loc[df_authors['id'] == author, 'name'].values[0]\n",
    "    else:\n",
    "        aliases.append(df_authors.loc[df_authors['id'] == author, 'name'].values[0])\n",
    "        name = max(aliases, key=len) #Asume the true name is the longest alias\n",
    "    author_atts[author]['att1'] = name\n",
    "\n",
    "\n",
    "    field = df_authors.loc[df_authors['id'] == author, 'field'].values[0]\n",
    "    author_atts[author]['att2'] = field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last three atts\n",
    "\n",
    "citation_dict = {}\n",
    "amount_of_css_papers = {}\n",
    "first_paper_year = {}\n",
    "\n",
    "# Setup dicts\n",
    "for author in CSS_authors:\n",
    "    amount_of_css_papers[author] = 0\n",
    "    citation_dict[author] = []\n",
    "    first_paper_year[author] = np.inf # going for the first paper created from the author which much be eariler than inf\n",
    "\n",
    "# Counting the papers each author as contributed to\n",
    "for authors in df_CSS_papers['authors']:\n",
    "    authors = fix_authors(authors, CSS_authors)\n",
    "\n",
    "    for author in authors:\n",
    "        amount_of_css_papers[author] += 1\n",
    "\n",
    "\n",
    "\n",
    "for i, paper in df_CSS_papers.iterrows():\n",
    "\n",
    "    authors = fix_authors(paper['authors'], CSS_authors)\n",
    "\n",
    "    for author in authors:\n",
    "        # Citation count\n",
    "        citation_dict[author].append(paper['citationCount'])\n",
    "\n",
    "        #First paper\n",
    "        if paper['year'] < first_paper_year[author]:\n",
    "            first_paper_year[author] = paper['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attribute\n",
    "for author in CSS_authors:\n",
    "\n",
    "    author_atts[author]['att3'] = np.median(citation_dict[author])\n",
    "    author_atts[author]['att4'] = amount_of_css_papers[author]\n",
    "    author_atts[author]['att5'] = first_paper_year[author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set node attributes\n",
    "nx.set_node_attributes(CSS_graph, author_atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes (authors) are 10727\n",
      "The number of edges is 15992\n",
      "The total number of possible edges are 57528901.0\n",
      "And the density is 0.00027798201811642467\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of nodes (authors) are {CSS_graph.number_of_nodes()}\")\n",
    "print(f\"The number of edges is {CSS_graph.number_of_edges()}\")\n",
    "print(f\"The total number of possible edges are {(CSS_graph.number_of_nodes()**2 - CSS_graph.number_of_nodes())/2}\")\n",
    "print(f\"And the density is {nx.density(CSS_graph)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network \n",
    "nx.write_graphml(CSS_graph, \"CSS_graph.graphml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eedd9fc418f36e2614580cbf31de2386521dabe9ec8a80375aa0fadc40a33bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

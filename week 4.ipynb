{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 6 nodes and 8 edges\n",
      "Graph with 6 nodes and 7 edges\n"
     ]
    }
   ],
   "source": [
    "# Create\n",
    "edges = [(1,2), (2,4), (2,3), (3,2), (3,1), (4,1), (6,1), (6,3)]\n",
    "nodes = [1,2,3,4,5,6]\n",
    "G = nx.DiGraph(edges)\n",
    "G.add_node(5)\n",
    "print(G)\n",
    "\n",
    "H = G.to_undirected()\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjecentcy matrix \n",
    "adjaceny = nx.adjacency_matrix(G)\n",
    "\n",
    "# Edge list \n",
    "edge_list = np.array([list(i) for i in list(G.edges)]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute clustering coefficients \n",
    "nx.clustering(G)\n",
    "nx.clustering(H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you switch the labels of nodes 5 and 6 in Image 2.20a, how does that move change the adjacency matrix? And the link list?\n",
    "\n",
    "Adjacency matrix \n",
    "swap row 4 and 5 and column 4 and 5.\n",
    "\n",
    "Edge list \n",
    "Then we just change all instances of 6 to five and 5 to 6...? \n",
    "\n",
    "Node 5 is not in the adjacency list \n",
    "\n",
    "There are respectivly 5 and 0 paths. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 11 nodes and 10 edges\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.5 \n",
    "edges = [(1,7), (2,9), (3,7), (3,8), (3,9), (4,9), (4,10), (5,9), (5,11), (6,11)]\n",
    "G = nx.Graph(edges)\n",
    "print(G)\n",
    "# Max number of edges : size of first cluster times size of second. \n",
    "# (N_1^2 + N_2^2 + 2*N_1*N_2  - N_1 - N_2 ) / 2\n",
    "# Difference : (N_1^2 + N_2^2 - N_1 - N_2 ) / 2  \n",
    "\n",
    "# Because n_2 is large actual number of links divided by n_2 is large and goes towards 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77349\n",
      "[{'category': 'Physics', 'source': 's2-fos-model'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(df_papers))\n",
    "\n",
    "# Testing things \n",
    "for index, paper in df_papers.iterrows():\n",
    "    print(ast.literal_eval(paper[\"field\"]))\n",
    "    break\n",
    "    # ast.literal_eval(paper[\"field\"])[0][\"category\"]\n",
    "    # break\n",
    "\n",
    "# for index, paper in df_papers.iterrows():\n",
    "#     print(len(ast.literal_eval(paper[\"doi\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in list of authorIds and outputs the corresponding top fields in the same order \n",
    "def author_field_v1(ids, df_author):\n",
    "    \"\"\"\n",
    "    Compute the top field of the given authors \n",
    "\n",
    "    Args:\n",
    "        ids (list): containing the ids of the authors in question\n",
    "        df_author (pandas df): data frame of type author that contains the given ids \n",
    "    \n",
    "    return: \n",
    "        fields (list): list of top fields of the given authors \n",
    "        n_CSS_authors (int): number of social science authors \n",
    "    \"\"\"\n",
    "    # Get boolean array that indicates where the authors are \n",
    "    mask = df_author[\"id\"].isin(ids)\n",
    "    \n",
    "    # Get the fields \n",
    "    fields = list(df_author[\"field\"][mask])\n",
    "    \n",
    "    # No. social science authors \n",
    "    n_CSS_authors = sum(mask)\n",
    "\n",
    "    return fields, n_CSS_authors\n",
    "\n",
    "# Not used because the len operator is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in list of authorIds and outputs the corresponding top fields in the same order \n",
    "def author_field(ids, df_author):\n",
    "    \"\"\"\n",
    "    Compute the top field of the given authors \n",
    "\n",
    "    Args:\n",
    "        ids (list): containing the ids of the authors in question\n",
    "        df_author (pandas df): data frame of type author that contains the given ids \n",
    "    \n",
    "    return: \n",
    "        fields (list): list of top fields of the given authors \n",
    "\n",
    "    \"\"\"\n",
    "    # Get boolean array that indicates where the authors are \n",
    "    mask = df_author[\"id\"].isin(ids)\n",
    "    \n",
    "    # Get the fields \n",
    "    fields = list(df_author[\"field\"][mask]) \n",
    "    \n",
    "    return fields \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argument_paper_dataframe_v1(df_papers, df_author):\n",
    "    \"\"\"\n",
    "    Takes in a paper data frame, arguments it with 2 new columns and puts the fields of the author in the first column and the number of social scientists in the second.\n",
    "\n",
    "    Args:\n",
    "        df_papers (pandas dataframe): paper dataframe (like before)\n",
    "        df_author (pandas dataframe): author dataframe (like before)\n",
    "\n",
    "    Returns:\n",
    "        df_papers (pandas dataframe): the dataframe from before, argumentet with two new columns\n",
    "    \"\"\"\n",
    "    df_papers[\"author_field\"] = None \n",
    "    df_papers[\"n_CSS_authors\"] = None\n",
    "\n",
    "    for index, row in tqdm(df_papers.iterrows()):\n",
    "        # Currently authors are stored as a string representation of the list so we make it a list again \n",
    "        authors = ast.literal_eval(row[\"authors\"]) \n",
    "        # Now we need to turn the authors into a list of integers, because the df_paper dataframe stores them as such \n",
    "        authors = [eval(id) for id in authors if id is not None]\n",
    "        #authors = [eval(id) for id in authors]\n",
    "        # Find the fields of the authors \n",
    "        authors_fields, n_CSS_authors = author_field(authors, df_author=df_author)\n",
    "        df_papers[\"author_field\"][index] = authors_fields\n",
    "        df_papers[\"n_CSS_authors\"][index] = authors_fields\n",
    "        # print(authors_fields) # debugging \n",
    "    # print(df_papers.head()) # debugging \n",
    "        \n",
    "    return df_papers \n",
    "\n",
    "# Not used because the len operator is good enough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argument_paper_dataframe(df_papers, df_author):\n",
    "    \"\"\"\n",
    "    Takes in a paper data frame, arguments it with a new column and puts the fields of the author in that column.\n",
    "\n",
    "    Args:\n",
    "        df_papers (pandas dataframe): paper dataframe (like before)\n",
    "        df_author (pandas dataframe): author dataframe (like before)\n",
    "\n",
    "    Returns:\n",
    "        df_papers (pandas dataframe): the dataframe from before, argumentet with the new column\n",
    "    \"\"\"\n",
    "    df_papers[\"author_field\"] = None \n",
    "\n",
    "    for index, row in tqdm(df_papers.iterrows()):\n",
    "        # Currently authors are stored as a string representation of the list so we make it a list again \n",
    "        authors = ast.literal_eval(row[\"authors\"]) \n",
    "        # Now we need to turn the authors into a list of integers, because the df_paper dataframe stores them as such \n",
    "        authors = [eval(id) for id in authors if id is not None]\n",
    "        #authors = [eval(id) for id in authors]\n",
    "        # Find the fields of the authors \n",
    "        authors_fields = author_field(authors, df_author=df_author)\n",
    "        df_papers[\"author_field\"][index] = authors_fields\n",
    "        # print(authors_fields) # debugging \n",
    "    # print(df_papers.head()) # debugging \n",
    "        \n",
    "    return df_papers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the paper datafram to a social science dataframe\n",
    "def generate_CCS_papers_1(df_papers, social_science_fields, quantitative_fields, verbose=False):\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop rows that are before 2008\n",
    "    for index, row in df_papers.iterrows():\n",
    "        if row[\"year\"] < 2008: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because they were to old. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "        \n",
    "    # Drop rows that do not contain a DOI\n",
    "    for index, row in df_papers.iterrows():\n",
    "        if row[\"doi\"] == None: \n",
    "            rows_to_drop.append(index)\n",
    "        else: \n",
    "            try:\n",
    "                ast.literal_eval(row[\"doi\"])[0][\"DOI\"]\n",
    "            except: \n",
    "                rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because they did not have a DOI. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if it includes biology\n",
    "    for index, row in df_papers.iterrows():\n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] == \"Biology\":\n",
    "                rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because biology was in the field. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if the fields are not not included in social science fields\n",
    "    for index, row in df_papers.iterrows():\n",
    "        is_in_SCF = False \n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] in social_science_fields:\n",
    "                is_in_SCF = True\n",
    "        \n",
    "        # if the paper was not in social science fields drop it \n",
    "        if not is_in_SCF: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because thier fields not in Social Science Fields. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    return df_papers\n",
    "\n",
    "\n",
    "# It is time consuming to add a row in the dataframe, hence it helps that the dataframe is 100 times smaller \n",
    "def generate_CCS_papers_2(df_papers, social_science_fields, quantitative_fields, verbose=False):\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop the papers with more than 9 Computational Social Science authors? TODO what does she mean! \n",
    "    for index, row in df_papers.iterrows():\n",
    "        if len(row[\"author_field\"]) > 9: # Count authors, should I check if they are in the author data frame? That's gonna take a while\n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because there are more than 9 CSS authors. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Drop paper if the fields are not not included in quantitative data and authors aren't either\n",
    "    for index, row in df_papers.iterrows():\n",
    "        is_in_SCF = False \n",
    "        # Go through the entire list of fields for each paper \n",
    "        for field in ast.literal_eval(row[\"field\"]):\n",
    "            if field[\"category\"] in quantitative_fields:\n",
    "                is_in_SCF = True\n",
    "        \n",
    "        # Check if the authors are in the quantitative_fields\n",
    "        for author_field in row[\"author_field\"]: \n",
    "            if author_field in quantitative_fields: \n",
    "                is_in_SCF = True\n",
    "       \n",
    "        # if the paper was not in social science fields drop it \n",
    "        if not is_in_SCF: \n",
    "            rows_to_drop.append(index)\n",
    "    if verbose: print(f\"{len(rows_to_drop)} papers removed because thier fields and authors are not in Quantitative Fields. ({len(rows_to_drop)/len(df_papers)*100:.0f}%)\")\n",
    "    df_papers.drop(index=rows_to_drop, inplace=True) \n",
    "    rows_to_drop = []\n",
    "    \n",
    "    return df_papers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially we have 1078817 papers.\n",
      "302900 papers removed because they were to old. (28%)\n",
      "148196 papers removed because they did not have a DOI. (19%)\n",
      "122867 papers removed because biology was in the field. (20%)\n",
      "466782 papers removed because thier fields not in Social Science Fields. (92%)\n",
      "There are 38072 papers left of 1078817 which is 3.53%\n"
     ]
    }
   ],
   "source": [
    "# Heuristic \n",
    "social_science_fields = {\"Political Science\", \"Sociology\", \"Economics\"}\n",
    "quantitative_fields = {\"Mathematics\", \"Physics\", \"Computer Science\"}\n",
    "\n",
    "# Load dataframes\n",
    "df_author = pd.read_csv(\"df_author.csv\")\n",
    "df_papers = pd.read_csv(\"df_paper.csv\")\n",
    "\n",
    "# Drop papers, but not based on the authors\n",
    "n_papers_before = len(df_papers)\n",
    "\n",
    "print(f\"Initially we have {n_papers_before} papers.\")\n",
    "df_papers = generate_CCS_papers_1(df_papers, social_science_fields, quantitative_fields, verbose=True)\n",
    "\n",
    "# Papers removed so far \n",
    "print(f\"There are {len(df_papers)} papers left of {n_papers_before} which is {100*len(df_papers)/n_papers_before:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_11632/3790806285.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_papers[\"author_field\"][index] = authors_fields\n",
      "38072it [00:41, 913.57it/s] \n"
     ]
    }
   ],
   "source": [
    "# Now the time consuming part \n",
    "# Argment paper dataframe \n",
    "df_papers = argument_paper_dataframe(df_papers=df_papers, df_author=df_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 papers removed because there are more than 9 CSS authors. (0%)\n",
      "32257 papers removed because thier fields and authors are not in Quantitative Fields. (85%)\n",
      "There are 5650 papers left of 1078817 which is 0.52%\n"
     ]
    }
   ],
   "source": [
    "# Drop papers based on authors \n",
    "df_papers = generate_CCS_papers_2(df_papers, social_science_fields, quantitative_fields, verbose=True)\n",
    "\n",
    "print(f\"There are {len(df_papers)} papers left of {n_papers_before} which is {100*len(df_papers)/n_papers_before:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(df_papers, \"df_CSS_paper.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eedd9fc418f36e2614580cbf31de2386521dabe9ec8a80375aa0fadc40a33bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

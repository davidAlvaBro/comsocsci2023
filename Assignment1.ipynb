{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "Collaborators (Name, study id, github handle): <br />\n",
    "August Hertz Bugge - s194350 - libze<br />\n",
    "David Bro Ludvigsen - s204102 - davidAlvaBro<br />\n",
    "Sebastian Nicolai Fabricius Grut  - s204150 - Sebastiannfg\n",
    "\n",
    "Github : https://github.com/davidAlvaBro/comsocsci2023.git \n",
    "\n",
    "#### Contributions \n",
    "We collaborated as a group. We have had weekly meetings where we completed the weekly assignments together - everything has been discussed and made together. \n",
    "\n",
    "\n",
    "#### Note to reader \n",
    "We found it helpfull to make the exercises as scripts for the different weeks, so that it is easier to run on a new device. The scripts used can be found in this github aswell and are the reason that much of our code is seperated into functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - all imports \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping (week 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code - Webscraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was needed to run on certain devices / disables certain warnings that will stop the script\n",
    "def disable_warnings():\n",
    "    requests.packages.urllib3.disable_warnings()\n",
    "    requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n",
    "    try:\n",
    "        requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n",
    "    except AttributeError:\n",
    "    # no pyopenssl support used / needed / available\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 posters \n",
    "def get_2019_posters(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : https://2019.ic2s2.org/posters/\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get the webpage data\n",
    "    LINK = \"https://2019.ic2s2.org/posters/\"\n",
    "    r = requests.get(LINK)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "\n",
    "    # Get each bullet point under class \"col-md-8\" (all the names are here)\n",
    "    text = soup.find(\"div\", {\"class\": \"col-md-8\"})\n",
    "    items = text.find_all(\"li\")\n",
    "    item_list = [str(item) for item in items]\n",
    "\n",
    "    # Get content between > and <, and seperate at , \n",
    "    regex_compiler = re.compile(\"(?<=\\>)(.*?)(?=\\<)\")\n",
    "\n",
    "    names = [regex_compiler.findall(item)[0] for item in item_list]\n",
    "    ind_names = [re.split(', | and', name) for name in names]\n",
    "    persons =  []\n",
    "    \n",
    "    # Collect to one list\n",
    "    for list in ind_names:\n",
    "        for name in list:\n",
    "            persons.append(name)\n",
    "\n",
    "    # Verbose \n",
    "    people_set = set(persons)\n",
    "    if verbose: print(f\"There are {len(people_set)} different people and {len(persons)} name occurences in {LINK}\")\n",
    "    return people_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 oral presentations \n",
    "def get_2019_oral(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : https://2019.ic2s2.org/oral-presentations/\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    def find_between( string, first, last ):\n",
    "        \"\"\"\n",
    "        Returns the part of a string that is in the middel of first and last (substrings)\n",
    "\n",
    "        Args:\n",
    "            string (String): The string\n",
    "            first (String): the start \"token\"\n",
    "            last (String): the end \"token\"\n",
    "\n",
    "        Returns:\n",
    "            _type_: Substring between \"first\" and \"last\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start = string.index( first ) + len( first )\n",
    "            end = string.index( last, start )\n",
    "            return string[start:end]\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    \n",
    "    # Get the webpage data\n",
    "    LINK = \"https://2019.ic2s2.org/oral-presentations/\"\n",
    "    r = requests.get(LINK)\n",
    "    soup = BeautifulSoup(r.content) \n",
    "    \n",
    "    # All names are between these two titles as a string \n",
    "    soup_string = find_between(str(soup),\"1A Misinformation\",\"Evidence of Influence Hierarchies in GitHub’s Cryptocurrency Community\" ).split(\"<p>\")# each <p> has it's own section with a chairname and a list of presenters\n",
    "    \n",
    "    # form a new list of the unfiltered remaining names \n",
    "    new_list = []\n",
    "    for d in soup_string:\n",
    "        new_list.append(find_between(d,\"</em><br/>\",\"</p>\"))\n",
    "    \n",
    "    newer_list = []\n",
    "    #Remove known non-name including files\n",
    "    for i in range(20): \n",
    "        current_line = new_list[i].split(\"<br/>\")\n",
    "        for x in current_line:\n",
    "            x = x[16:]\n",
    "            if str(x) == \"No Presentation\":\n",
    "                pass\n",
    "            else:\n",
    "                newer_list.append(x)\n",
    "    \n",
    "    # Seperate into two schools - the ones that end with .  and the ones with - \n",
    "    # Seperate names at , and remove empty names. \n",
    "    # If there is a : then it is not a name, only take what is after. Exceptions like these are due to the format of the html soup\n",
    "    names_list = []\n",
    "    for i in newer_list:\n",
    "        if str(i[-1]) == \"–\":\n",
    "            x = str(i[:-2]).split(\",\")\n",
    "            for m in x:\n",
    "                if m != \"\":\n",
    "                    if \":\" in m:\n",
    "                        m = m[m.find(\":\")+1:]\n",
    "                    names_list.append(m)\n",
    "        else:\n",
    "            imp_ful = 0\n",
    "            for E in range(len(i)):\n",
    "                if i[E] == \".\" and i[E-2] != \" \":\n",
    "                    imp_ful = E\n",
    "                    break\n",
    "            namees = i[:imp_ful].split(\",\")\n",
    "            for O in namees:\n",
    "                if O != \"\":\n",
    "                    if \":\" in O:\n",
    "                        O = O[O.find(\":\")+2:]\n",
    "                    names_list.append(O)\n",
    "    \n",
    "    # Delete \"No presentation (cancelled)\" entries \n",
    "    for i in range(len(names_list)):\n",
    "        if names_list[i-1] == \"No presentation (cancelled)\":\n",
    "            names_list.pop(i-1)\n",
    "    \n",
    "    \n",
    "    ## Get the chair names \n",
    "    # These are uniquely seperated, as they are always in the same place, seperate of the other author names.\n",
    "    chair_names = []\n",
    "    for i in soup_string:\n",
    "        chair_names.append(find_between(i, \"Chair:\", \"</em>\"))\n",
    "    \n",
    "    ### Merge the two lists \n",
    "    final_list = []\n",
    "    for i in names_list+chair_names:\n",
    "        if i[0] == \" \":\n",
    "            final_list.append(i[1:])\n",
    "        else:\n",
    "            final_list.append(i)\n",
    "        \n",
    "    return set(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020 \n",
    "def get_2020_all(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : \"https://ic2s2.mit.edu/program\"\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get page content \n",
    "    LINK = \"https://ic2s2.mit.edu/program\"\n",
    "    req = requests.get(LINK, verify= False)\n",
    "    soup = BeautifulSoup(req.content, features=\"html.parser\")\n",
    "    \n",
    "    # Scrape link to the page with the actuel content\n",
    "    text = soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "    str_text = str(text).split(\"src=\")\n",
    "    docs_link = str_text[-1].split(\" \")[0][1:-1]\n",
    "    \n",
    "    # All names are stored in the table in the class \"waffle\" \n",
    "    r = requests.get(docs_link)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\": \"waffle\"})\n",
    "    table_rows = table.find_all(\"tr\") # Get all rows \n",
    "    \n",
    "    # Go through each row and put the data into a list (row of dataframe)\n",
    "    rows = []\n",
    "    for tr in table_rows[1:]:\n",
    "        tds = tr.find_all('td')\n",
    "        row = [td.text.replace(\"\\n\",\"\") for td in tds]\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Make the dataframe \n",
    "    df = pd.DataFrame(rows)#, columns=header[0:5])\n",
    "    # Extract names from column 2 (zero indexed) starting from the 1 (zero indexed) element\n",
    "    names_plus = [name.split(', ') for name in list(df.iloc[:,2][1:])]\n",
    "    names = []\n",
    "    for name in names_plus:\n",
    "        for n in name:\n",
    "            if len(n) :\n",
    "                names.append(n)\n",
    "    if verbose: f\"There are {len(set(names))} different people and {len(names)} name occurences in {LINK}\"\n",
    "    return list(set(names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021 \n",
    "def get_2021_all(verbose=False): \n",
    "    \"\"\"\n",
    "    Get all names from the webpage : \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get page content \n",
    "    LINK = \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "    request = requests.get(LINK)\n",
    "    soup = BeautifulSoup(request.content)\n",
    "    \n",
    "    # All names are in the table \"index\" and we split at \"tr\"; each containing one name\n",
    "    contents = soup.find(\"table\", \"index\")\n",
    "    contents = contents.find_all(\"tr\")\n",
    "    \n",
    "    # Regex compiler that finds elements between > < (not including)\n",
    "    regex_compiler = re.compile(\"\\>(.*?)\\<\")\n",
    "    names = set()\n",
    "    counter = 0\n",
    "    \n",
    "    # Go through each tr (statement) and split at each td find the first name and surname\n",
    "    for content in contents: \n",
    "        person = str(content.find_all(\"td\")[0])\n",
    "        titles = regex_compiler.findall(person)[2:-1]\n",
    "        if len(titles) == 2: # There is not exactly two elements it is not a name (but a Alphabetic code)\n",
    "            name = titles[1] + \" \" + titles[0]\n",
    "            name = name.replace(\",\", \"\")\n",
    "            name = name.strip()\n",
    "            \n",
    "            names.add(name) \n",
    "            counter +=1\n",
    "\n",
    "    if verbose: f\"There are {len(names)} different people and {counter} name occurences in {LINK}\"\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions, as the datasets will need to be stored underway as they are very large\n",
    "def save_data(data, file_name): \n",
    "    \"\"\"\n",
    "    A function to save a dictionary (or set) \n",
    "\n",
    "    Args:\n",
    "        ids (data): data/set that needs to be stored\n",
    "        file_name (str): file name \n",
    "    \"\"\"\n",
    "    np.save(f\"{file_name}.npy\", data)\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads a data object (dict or set) \n",
    "\n",
    "    Args:\n",
    "        file_name (str): file_name (without prefix)\n",
    "    Returns:\n",
    "        set or dict: data in the file\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.load(f\"{file_name}.npy\", allow_pickle=True).item()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total we have 2133 unique computational social scientists across the three years\n"
     ]
    }
   ],
   "source": [
    "# Scrape data from all three years \n",
    "def get_all_names(verbose=False, Body = False):\n",
    "    \"\"\"\n",
    "    Collects all four datasets's names into one set \n",
    "\n",
    "    Args:\n",
    "        verbose (bool, optional): True -> prints comments about how many times names appear and how many there are\n",
    "    \"\"\"\n",
    "    if Body: disable_warnings()\n",
    "\n",
    "    # Run the previous four methods\n",
    "    names_2019_poster = get_2019_posters(verbose)\n",
    "    names_2019_oral = get_2019_oral(verbose)\n",
    "    names_2020_all = get_2020_all(verbose)\n",
    "    names_2021_all = get_2021_all(verbose)\n",
    "    \n",
    "    # Collect into one set\n",
    "    names_all = set()\n",
    "    names_all.update(names_2019_poster, names_2019_oral, names_2020_all, names_2021_all)\n",
    "    \n",
    "    if verbose: print(f\"There are {len(get_all_names())} unique names in total\")\n",
    "    return names_all\n",
    "\n",
    "# To not repeat this time consuming process we save the results and load them if possible\n",
    "science_people_file_name = \"names_week_1\"\n",
    "if os.path.isfile(science_people_file_name + \".npy\"):\n",
    "    science_people = load_data(science_people_file_name)\n",
    "else: \n",
    "    science_people = get_all_names()\n",
    "    save_data(science_people, science_people_file_name)\n",
    "\n",
    "print(f\"In total we have {len(science_people)} unique computational social scientists across the three years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 355 unique authors in the oral presentation for 2019\n",
      "There are 471 unique authors in the poster presentation for 2019\n",
      "There are 774 unique authors in total\n"
     ]
    }
   ],
   "source": [
    "# How many unique authors are there in 2019 split across oral and poster? \n",
    "authors_oral_2019 = get_2019_oral(verbose=False)\n",
    "authors_poster_2019 = get_2019_posters(verbose=False)\n",
    "\n",
    "print(f'There are {len(authors_oral_2019)} unique authors in the oral presentation for 2019')\n",
    "print(f'There are {len(authors_poster_2019)} unique authors in the poster presentation for 2019')\n",
    "print(f'There are {len(set(authors_oral_2019).union(authors_poster_2019))} unique authors in total')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion from week 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2) \n",
    "How many unique researchers you got in 2019?\n",
    "\n",
    "We found 774 unique authors in the year 2019 with webscraping, and across all three years we found 2133 authors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3) \n",
    "Explain one or two decisions you took during the web-scraping exercise, for 2019 or any other year. Why did you take this choice? How might your decision impact the final number of authors?\n",
    "\n",
    "2019 - oral </br>\n",
    "This webpage was the hardest to scrape because the layout had many different types of diviants. In each paragraph with names there were a time stamp followed by a list of names \",\" seperated and then a \".\" to end the listing. \n",
    "However, some names contained \".\" such as \"Harvey G. Jensen\". Also some name listnings ended with \"-\" instead. </br>\n",
    "To solve this we split these pagraphs into two branches, one that ends with \"-\" and one that ends with \".\". The second problem was solved by checking if the second letter before the \".\" was a space - if that is the case it is a part of the name. </br>\n",
    "At last when the list of candidate names was found we iterated through it and got rid of faulty entries such as \"No Presentation\". </br>\n",
    "We have included all names between \"\\</em>\\<br/>\" and \"\\</p>\" to find the list of representers and all names between \"Chair:\", \"\\</em>\" to find the chair names (usually repeated later). Only names in these listings are included. Also if a \"-\" is in the name we exclude it. \n",
    "\n",
    "2019 - poster </br>\n",
    "The webscraping execise was a bit simpler. There is one name in each row (\"\\<li>\" to \"\\</li>\") and the name is the part that is the first element in the list when we look at the seperation \">\" to \"<\". \n",
    "In this setting all the names are included as the page layout do not have any diviants. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic scholar (week 2) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code - Find coauthors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting all coauthors to a set of names  \n",
    "def get_ids_and_coauthors(names, file_name, load_previous=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in a set of names and returns a set of ids of the author and the coauthors to the authors papers\n",
    "\n",
    "    Args:\n",
    "        names (set): names of the authors\n",
    "        file_name (str): file name of where to save progress\n",
    "        verbose (Boolean): whether the function should speak or not\n",
    "        \n",
    "    Returns: \n",
    "        ids (set): Set of author ids for all \"names\" and coauthors on all papers of \"names\"\n",
    "        nin_names (set): Set of names that was not in the sematic scholar database \n",
    "    \"\"\" \n",
    "    \n",
    "    # Base address for requests\n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "    VERSION  = \"v1/\"\n",
    "    RESOURCE = \"author/search?query=\"\n",
    "    ADDITION = \"&fields=papers.authors\"\n",
    "    complete_url = BASE_URL + VERSION + RESOURCE\n",
    "    \n",
    "    # The set of ids \n",
    "    ids = set()\n",
    "    evaluated_names = set() \n",
    "    nin_names = set()\n",
    "    \n",
    "    # Check if the problem has been worked on previously \n",
    "    if load_previous:\n",
    "        try:    \n",
    "            ids = load_data(file_name=file_name)\n",
    "            evaluated_names = load_data(file_name=file_name + \"_evaluated_names\")\n",
    "            nin_names = load_data(file_name=file_name + \"_nin_names\")\n",
    "            if verbose: print(f\"{len(evaluated_names)} already evaluated of {len(names)}\") \n",
    "            names = names - evaluated_names\n",
    "            if verbose:  print(f\"Hence there are {len(names)} left \")\n",
    "        except:\n",
    "            if verbose : print(f\"There are not any progress previously achieved\")\n",
    "        \n",
    "    \n",
    "    # Loop over authors \n",
    "    for i, name in enumerate(names):\n",
    "        # We can only do 150 request each five minuts, use the down time to save progress \n",
    "        if i % 150 == 149: \n",
    "            start_time = time.time()\n",
    "            # printing \n",
    "            if verbose: \n",
    "                print(f\"Completed searches for {i} out of {len(names)}, but reached limit\")\n",
    "            # Save prograss \n",
    "            save_data(ids, file_name=file_name)\n",
    "            save_data(evaluated_names, file_name=file_name + \"_evaluated_names\")\n",
    "            save_data(nin_names, file_name=file_name + \"_nin_names\")\n",
    "            time.sleep(60*5+10 + start_time - time.time()) # the +10 is a buffer \n",
    "        \n",
    "        # Make request \n",
    "        # print(complete_url + name + ADDITION) # Debugging\n",
    "        response = requests.get(complete_url + name + ADDITION).json()\n",
    "        \n",
    "        # If something goes wrong, it will be reported here \n",
    "        try: \n",
    "            for paper in response[\"data\"][0][\"papers\"]: \n",
    "                for author in paper[\"authors\"]: \n",
    "                    ids.add(author[\"authorId\"]) \n",
    "        except: # Usually only occurs if the author has not realeased any papers or is not found \n",
    "            print(f\"The error occured at search number {i}, the name {name} and the response is: \\n {response}\")\n",
    "            nin_names.add(name)\n",
    "        # In either case the name has been evaluated\n",
    "        evaluated_names.add(name) \n",
    "               \n",
    "    # Just to not mess whith the other parts of the code (amount of requests)\n",
    "    if len(names) % 150 > 50: \n",
    "        time.sleep(60*5) \n",
    "    \n",
    "    # Save progress for next time \n",
    "    save_data(ids, file_name=file_name)\n",
    "    save_data(evaluated_names, file_name=file_name + \"_evaluated_names\")\n",
    "    save_data(nin_names, file_name=file_name + \"_nin_names\")\n",
    "            \n",
    "    return ids, nin_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2133 already evaluated of 2133\n",
      "Hence there are 0 left \n",
      "We have found 118159 coauthors of the 2133 from week 1\n",
      "Of the 2133 authors, 181 where not found\n"
     ]
    }
   ],
   "source": [
    "# Get all coauthors to the 2133 authors found in week 1 \n",
    "ids, nin_names = get_ids_and_coauthors(science_people, file_name=\"ids_dict\", load_previous=True, verbose=True) \n",
    "\n",
    "print(f\"We have found {len(ids)} coauthors of the {len(science_people)} from week 1\")\n",
    "print(f\"Of the {len(science_people)} authors, {len(nin_names)} where not found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions and thoughts (these are mostly notes to ourselves)\n",
    "\n",
    "We have now gathered a dataset of more than 100 000 authors, which is quite a lot. The issue that we can only make about 150 requests to semantic scholar every five minutes arrises and hence we need to batch our id searches, as the process will otherwise take to long (suprise it takes very long anyway). \n",
    "\n",
    "This is however not easy as semantic scholar can only handle batches of size 100 when we are also asking for the papers written by each author. \n",
    "\n",
    "The amount of data that we want to gather from sematic scholar is quite large, as a dictionary containing the first 10000 author id's as keys takes up 1.7 GB of data. \n",
    "Furthermore, semantic scholar can often not handle batches of size 100 if there is two much data in the batch, hence the error handeling in the next couple of methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting authors into a dataframe\n",
    "def format_authors(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors with the data; id, name, alias, citationCount, field\n",
    "    \"\"\"\n",
    "    # Create people dataframe \n",
    "    zero_data = np.zeros((len(ids_dict.keys()), 5))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"name\", \"aliases\", \"citationCount\", \"field\"])\n",
    "\n",
    "    for i, id in enumerate(ids_dict.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id \n",
    "        information = ids_dict[str(id)]\n",
    "        # Name\n",
    "        df[\"name\"][i] = information[\"name\"]\n",
    "        # try: # Debugging \n",
    "        #     df[\"name\"][i] = information[\"name\"]\n",
    "        # except:\n",
    "        #     print(information)\n",
    "        #     print(df[\"name\"])\n",
    "        # Aliases\n",
    "        df[\"aliases\"][i] = information[\"aliases\"] \n",
    "        # try:\n",
    "        #     df[\"aliases\"][i] = information[\"aliases\"] \n",
    "        # except:\n",
    "        #     print(f\"{id} has no aliases\")\n",
    "        #     print(information)\n",
    "        # citation count \n",
    "        citation_count = 0\n",
    "        for paper in information[\"papers\"]: \n",
    "            citation_count += paper[\"citationCount\"]\n",
    "        df[\"citationCount\"][i] = citation_count\n",
    "        # field - count each occurence and take the maximum \n",
    "        potential_fields = {}\n",
    "        for paper in information[\"papers\"]: \n",
    "            for fields in paper[\"s2FieldsOfStudy\"]:\n",
    "                field = fields[\"category\"]\n",
    "                try:\n",
    "                    potential_fields[field] += 1\n",
    "                except: \n",
    "                    potential_fields[field] = 1\n",
    "        if potential_fields == {}: \n",
    "            pass\n",
    "        else: \n",
    "            # This is a spicy way to do this o.0\n",
    "            df[\"field\"][i] = max(potential_fields, key=potential_fields.get)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting papers into a dataframe\n",
    "def format_papers(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors's papers with the data; id, title, year, DOI, citationCount, field, authors\n",
    "    \"\"\"\n",
    "    # Start by making a dictionary of papers instead of authors\n",
    "    papers = {}\n",
    "    for id in ids_dict.keys():\n",
    "        # get the papers \n",
    "        information = ids_dict[str(id)]\n",
    "        for paper in information[\"papers\"]:\n",
    "            if paper[\"paperId\"] in papers:\n",
    "                pass\n",
    "            else: \n",
    "                # Find the author id's\n",
    "                authors = set()\n",
    "                for author in paper[\"authors\"]:\n",
    "                    authors.add(author[\"authorId\"])\n",
    "                # Make new elements in the dictionary \n",
    "                papers[paper[\"paperId\"]] = {\"id\": paper[\"paperId\"],\n",
    "                                            \"title\": paper[\"title\"], \n",
    "                                            \"year\": paper[\"year\"], \n",
    "                                            \"doi\": paper[\"externalIds\"],\n",
    "                                            \"citationCount\": paper[\"citationCount\"], \n",
    "                                            \"field\": paper[\"s2FieldsOfStudy\"], \n",
    "                                            \"authors\": authors}\n",
    "    # Create the paper dataframe \n",
    "    zero_data = np.zeros((len(papers.keys()), 7))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"title\", \"year\", \"doi\", \"citationCount\", \"field\", \"authors\"])\n",
    "\n",
    "    for i, id in enumerate(papers.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id\n",
    "        information = papers[str(id)]\n",
    "        # title\n",
    "        df[\"title\"][i] = information[\"title\"]\n",
    "        # Aliases\n",
    "        df[\"year\"][i] = information[\"year\"] \n",
    "        # DOI \n",
    "        df[\"doi\"][i] = [information[\"doi\"]] # Can't have a dict, but it is okay to wrap it with list\n",
    "        # citation count \n",
    "        df[\"citationCount\"][i] = information[\"citationCount\"]\n",
    "        # field \n",
    "        try:\n",
    "            df[\"field\"][i] = information[\"field\"] # Sadly at least one paper does not have a field...\n",
    "        except:\n",
    "            print(f\"Paper {id} does not have a field and it crashes everything!?\")\n",
    "        # authors \n",
    "        df[\"authors\"][i] = list(information[\"authors\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting paper abstracts into a dataframe\n",
    "def format_paper_abstracts(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors's papers with the data; id, abstract\n",
    "    \"\"\"\n",
    "    # Start by making a dictionary of papers instead of authors\n",
    "    papers = {}\n",
    "    for id in ids_dict.keys():\n",
    "        # get the papers \n",
    "        information = ids_dict[str(id)]\n",
    "        for paper in information[\"papers\"]:\n",
    "            if paper[\"paperId\"] in papers:\n",
    "                pass\n",
    "            else:\n",
    "                # Make new elements in the dictionary \n",
    "                papers[paper[\"paperId\"]] = {\"id\": paper[\"paperId\"],\n",
    "                                            \"abstract\": paper[\"abstract\"]}\n",
    "    # Create the paper dataframe \n",
    "    zero_data = np.zeros((len(papers.keys()), 2))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"abstract\"])\n",
    "\n",
    "    for i, id in enumerate(papers.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id\n",
    "        information = papers[str(id)]\n",
    "        # abstract\n",
    "        df[\"abstract\"][i] = information[\"abstract\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that formats a dictionary into dataframes and stores them\n",
    "def create_dataframes(ids_dict, prefix=\"\", verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary of authors and their papers and generates two datasets and stores these\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): The key is an auther id, the contents is a dictionary with three atributes, \"name\", \"aliases\" and \"papers\".\n",
    "        prefix (str): If the function will be called multiple times, this is to not overwrite previous stored files\n",
    "    \n",
    "    Return: \n",
    "        df_author: The above specified dataframe for authors\n",
    "        df_paper: The above specified dataframe for papers\n",
    "    \"\"\"\n",
    "    # Create dataframes\n",
    "    # Authors \n",
    "    df_author = format_authors(ids_dict)\n",
    "    if verbose: print(\"Formatted author dataframe\")\n",
    "    pd.DataFrame.to_csv(df_author, f\"df_author{prefix}.csv\")\n",
    "    if verbose: print(\"Saved author dataframe\")\n",
    "    \n",
    "    # Papers\n",
    "    df_paper = format_papers(ids_dict)\n",
    "    if verbose: print(\"Formatted paper dataframe\")\n",
    "    pd.DataFrame.to_csv(df_paper, f\"df_paper{prefix}.csv\")\n",
    "    if verbose: print(\"Saved paper dataframe\")\n",
    "    \n",
    "    # Paper abstracts \n",
    "    df_paper_abstract = format_paper_abstracts(ids_dict)\n",
    "    if verbose: print(\"Formatted paper dataframe\")\n",
    "    pd.DataFrame.to_csv(df_paper_abstract, f\"df_paper_abstract{prefix}.csv\")\n",
    "    if verbose: print(\"Saved paper dataframe\")\n",
    "    \n",
    "    return df_author, df_paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that finds all papers from each author\n",
    "def get_data_from_ids(ids, verbose=False, load_previous=True, file_name=\"ids_enumerated_dict\"): \n",
    "    \"\"\"\n",
    "    Returns a complete data frame of \n",
    "    authors (id, name, alias, citationCount, field) and\n",
    "    papers (id, title, year, DOI, citationCount, field, authors)\n",
    "\n",
    "    Args:\n",
    "        ids (set): ids of the authors in question\n",
    "\n",
    "    Returns:\n",
    "        file_extension (int): The number of dataframes created\n",
    "        nin_ids (set): The set of ids that could not be processed  \n",
    "    \"\"\"\n",
    "    ids_dict = {} # This will continually be reset, otherwise it would take up to much space\n",
    "    evaluated_ids = set()\n",
    "    nin_ids = set()\n",
    "    ids_dict[\"file_extension\"] = 0\n",
    "    \n",
    "    # Check if the problem has been worked on previously \n",
    "    if load_previous:\n",
    "        try: \n",
    "            ids_dict = load_data(file_name=file_name)\n",
    "            evaluated_ids = load_data(file_name=file_name + \"_evaluated_ids\")\n",
    "            nin_ids = load_data(file_name=file_name + \"_nin_ids\")\n",
    "            if verbose: print(f\"{len(evaluated_ids)} already evaluated of {len(ids)}\") \n",
    "            ids = ids - evaluated_ids\n",
    "            if verbose: print(f\"Hence there are {len(ids)} left \")\n",
    "        except:\n",
    "            print(f\"There are no previous progress made\")\n",
    "    \n",
    "    # Partition the ids into batches of 20 ids, because semantic scholar can only take that many \n",
    "    ids = list(ids) # temporary to get results\n",
    "    default_batch_size = 64\n",
    "    batch_size = default_batch_size\n",
    "    n_batches = len(ids) // batch_size + 1\n",
    "    batches_left = True \n",
    "    sent_requests = 0\n",
    "    index = 0 \n",
    "    if verbose: print(f\"Total number of batches are {n_batches}\")\n",
    "        \n",
    "    #  Current file name extension\n",
    "    file_extension = ids_dict[\"file_extension\"]\n",
    "    file_start = -len(ids_dict)\n",
    "    file_size = 6000 # Hope this is small enough \n",
    "    \n",
    "    # Use a while loop to go through each batch of ids so that we can change sizes dynamically\n",
    "    # (This stems from the fact that semantic scholar will return errors if we ask for too much data)\n",
    "    while(batches_left):\n",
    "        # To avoid memory overflow convert to pandas \n",
    "        if index > file_start + file_size: \n",
    "            # Note that the return dataframes from create_dataframes are not used, as we do not have memory enough to keep them\n",
    "            # (they are only stored to physical memory)\n",
    "            del(ids_dict[\"file_extension\"])\n",
    "            create_dataframes(ids_dict=ids_dict, prefix=file_extension, verbose=verbose)\n",
    "            file_start += file_size\n",
    "            file_extension += 1\n",
    "            ids_dict = {}\n",
    "            ids_dict[\"file_extension\"] = file_extension\n",
    "        \n",
    "        # Make request for batch \n",
    "        batch_url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "        data = {\"ids\": ids[index:min(index + batch_size, len(ids))]}\n",
    "        params = {\"fields\": \"aliases,papers.title,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount,papers.abstract,name,papers.authors\"}\n",
    "        response = requests.post(batch_url, json=data, params=params).json()\n",
    "        sent_requests += 1\n",
    "        \n",
    "        # Assert the response\n",
    "        if response == {'message': 'Internal server error'}:\n",
    "            if verbose: print(f\"Server error for index {index} with batch size {batch_size}\")\n",
    "            batch_size = batch_size // 2 # Half batch size and try again \n",
    "            \n",
    "            if batch_size == 0: \n",
    "                # Save the faulty element that makes semantic scholar give internal errors \n",
    "                # and proceed to the next one\n",
    "                if verbose: print(f\"Had to remove {index} which is {ids[index]}\")\n",
    "                batch_size = 1 \n",
    "                evaluated_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "                nin_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "                index += batch_size \n",
    "        else: \n",
    "            # If there is something wrong with the request\n",
    "            try: \n",
    "                for person in response: \n",
    "                    # Update dictionary \n",
    "                    # If something goes wrong, it will be reported here \n",
    "                    try: \n",
    "                        ids_dict[person[\"authorId\"]] = {\"name\": person[\"name\"], \n",
    "                                \"aliases\": person[\"aliases\"],\n",
    "                                \"papers\": person[\"papers\"]}\n",
    "                    except: # Usually only occurs if the author has not realeased any papers or is not found \n",
    "                        print(\"Something is wrong with this person (usually it is a None value somehow?)\")\n",
    "                        print(f\"The index is {index} with batch size {batch_size}\")\n",
    "                        # nin_ids.add(person[\"authorId\"]) # Semantic scholar returns None - hence we can't do this TODO: enumerate people and store the index\n",
    "            except: \n",
    "                # If it messes up print the request and put the ids in nin \n",
    "                if verbose:  \n",
    "                    print(response)\n",
    "                    print(f\"The index is {index} with batch size {batch_size}\")\n",
    "                nin_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "            \n",
    "            # Update processed ids \n",
    "            evaluated_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "            index += batch_size\n",
    "            batch_size = default_batch_size\n",
    "        \n",
    "        # If there has been too many request we need to break - we can probably skip this now because requests take so long \n",
    "        if sent_requests % 150 == 149: \n",
    "            start_time = time.time()\n",
    "            # printing \n",
    "            if verbose: \n",
    "                print(f\"Completed searches for {index} out of {len(ids)}, but reached limit\")\n",
    "            # Save prograss \n",
    "            save_data(ids_dict, file_name=file_name)\n",
    "            save_data(evaluated_ids, file_name=file_name + \"_evaluated_ids\")\n",
    "            save_data(nin_ids, file_name=file_name + \"_nin_ids\")\n",
    "            time.sleep(max(60*5+10 + start_time - time.time(), 0)) # the +10 is a buffer\n",
    "        \n",
    "        # Check if the loop is complete \n",
    "        if index >= len(ids): \n",
    "            batches_left = False \n",
    "    \n",
    "    # Create and save the final dataframes \n",
    "    if len(ids_dict) > 1: \n",
    "        del(ids_dict[\"file_extension\"])\n",
    "        create_dataframes(ids_dict=ids_dict, prefix=file_extension, verbose=verbose)\n",
    "        save_data(evaluated_ids, file_name=file_name + \"_evaluated_ids\")\n",
    "        save_data(nin_ids, file_name=file_name + \"_nin_ids\")\n",
    "    \n",
    "    return file_extension, nin_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117414 already evaluated of 117414\n",
      "Hence there are 0 left \n",
      "Total number of batches are 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/2599186691.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"aliases\"][i] = information[\"aliases\"]\n",
      "c:\\Users\\david\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/2599186691.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"citationCount\"][i] = citation_count\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/2599186691.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"field\"][i] = max(potential_fields, key=potential_fields.get)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/2599186691.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"id\"][i] = id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted author dataframe\n",
      "Saved author dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"title\"][i] = information[\"title\"]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"year\"][i] = information[\"year\"]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"doi\"][i] = [information[\"doi\"]] # Can't have a dict, but it is okay to wrap it with list\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"citationCount\"][i] = information[\"citationCount\"]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"field\"][i] = information[\"field\"] # Sadly at least one paper does not have a field...\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"authors\"][i] = list(information[\"authors\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted paper dataframe\n",
      "Saved paper dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/2284731925.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"abstract\"][i] = information[\"abstract\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted paper dataframe\n",
      "Saved paper dataframe\n",
      "Of the 117414 authors, 179 could not be found in the dataset\n",
      "There are 19 dataframes of authors, papers and paper abstracts that needs to be merged\n"
     ]
    }
   ],
   "source": [
    "# Generate all dataframes \n",
    "n_files, nin_ids = get_data_from_ids(ids, load_previous=True, verbose=True)\n",
    "\n",
    "print(f\"Of the {len(ids)} authors, {len(nin_ids)} semantic scholar could not return\")\n",
    "print(f\"There are {n_files} dataframes of authors, papers and paper abstracts that needs to be merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to merge dataframes \n",
    "def load_and_merge(id, number, front=\"df_\", end=\".csv\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        id (str): auther, paper, paper_abstract\n",
    "        number (int): number of files \n",
    "        front (str, optional):  Defaults to \"df_\".\n",
    "        end (str, optional):  Defaults to \".csv\".\n",
    "    \"\"\"\n",
    "    # Load the first data file\n",
    "    final_dataframe = pd.read_csv(front + id + str(0) + end)\n",
    "    # Merge with the rest \n",
    "    for i in range(1,number):\n",
    "        new_df = pd.read_csv(front + id + str(i) + end)\n",
    "        final_dataframe = pd.concat([final_dataframe, new_df])\n",
    "    \n",
    "    final_dataframe.drop_duplicates()\n",
    "    \n",
    "    # Save dataframe \n",
    "    pd.DataFrame.to_csv(final_dataframe, f\"{front}{id}{end}\")\n",
    "    return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (15003, 6) elements in author\n",
      "There are (1078817, 3) elements in paper_abstract\n",
      "There are (1078817, 8) elements in paper\n"
     ]
    }
   ],
   "source": [
    "# Merge dataframes \n",
    "types = [\"author\", \"paper_abstract\", \"paper\"]\n",
    "\n",
    "# We do not have enough memory to test all 20 csv's for each of the three. Therefore we only do a subset \n",
    "# n_files = 3 \n",
    "\n",
    "for id in types:\n",
    "    df = load_and_merge(id=id, number=n_files)\n",
    "    print(f\"There are {df.shape} elements in {id}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from week 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1) \n",
    "Share the number of authors you will use as starting point in this exercises. Add a comment clarifying how many IC2S2 editions you included and if the collaborators were included or not.\n",
    "\n",
    "We used all 2133 authors form week 1 as a starting point, but 181 of these could not be found in the semantic scholar database. We found 118 951 coauthors (or collaborators) to these 1952 (2133 - 181) with themselves included.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3) \n",
    "How long is your final Author dataframe? How long is your final Paper dataframe?\n",
    "\n",
    "About 118951 -1500320/3 = 18931 of the authors returned errors when we asked semantic scholar for their papers and they have been removed. This leaves us with a dataframe of authers with about 100020 entries. </br>\n",
    "Together these 100020 authors have written about 107881720/3 = 7 192 113 unique papers. \n",
    "\n",
    "This is an estimate as we lack the RAM to properly combine the csv files, and cannot properly check how many unique papers there are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions (week 3)  \n",
    "\n",
    "We have made two plots for each distribution because the 10 000 sample plots had to large values to see the fine details. \n",
    "\n",
    "Further more, we have added histogram plots to better view the distributions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for generating distributions and plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot function that makes histograms and cumulative mean and median plots. \n",
    "def create_plot(data, mean, median, show_hist=False, plot=True, safeFig = False, n = 10000, name = \"Distribution name\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (list): List of datapoints sampled from a distribution\n",
    "        mean (int): The mean value of the distribution \n",
    "        median (int): The median value of the distribution \n",
    "        show_hist (bool): If True create a histogram showing the data given\n",
    "        plot (bool): If True create a two plots one showing the cummulative mean and with error bars and the distribution mean\n",
    "        safeFig (bool): If True safe the figure locally. Only works if show_hist or plot is True\n",
    "        n (int): Number of sampled from the distribution\n",
    "        name (str): Name of the distribution\n",
    "    \"\"\"\n",
    "    # Data saved as dataframe\n",
    "    Norm_p = pd.DataFrame(data)\n",
    "\n",
    "    if show_hist:\n",
    "        # Create histogram\n",
    "        plt.hist(data, histtype='step')\n",
    "        plt.title(\"Histogram\")\n",
    "        plt.show()\n",
    "\n",
    "    if plot:\n",
    "        # Setup for the plots, calculation of key numbers\n",
    "        Cum_Mean = [Norm_p[0][:i].mean() for i in range(len(data))]\n",
    "        Cum_std_err = [Norm_p[0][:i].std() / np.sqrt(i) for i in range(1, len(data) + 1)]\n",
    "        \n",
    "        # starts at one as when viewed it makes sense to have the first element be 1\n",
    "        x_vals = list(range(1, len(Cum_Mean) + 1))\n",
    "\n",
    "        # Setup for the plots\n",
    "        fig, axs = plt.subplots(2, figsize = (12,8))\n",
    "        fig.tight_layout(pad= 3)\n",
    "\n",
    "        # First value is excluded as standard error of a single point is Nan\n",
    "        ye = np.array(Cum_Mean)[1:]\n",
    "        yerr = np.array(Cum_std_err[1:])\n",
    "\n",
    "        # Creating plot for mean\n",
    "        axs[0].set_title(\"Mean for \" + name +\" distribution\")\n",
    "        axs[0].errorbar(x_vals[1:], ye, yerr=yerr, label='Error bars', linewidth=0.5, ecolor = \"blue\", fmt='none')\n",
    "        axs[0].plot(x_vals, Cum_Mean, '-', label = \"Cumulative mean\", linewidth=1, color = \"red\")\n",
    "        axs[0].plot([1, n], [mean, mean], label = \"Distribution mean\", linewidth=2, color=\"orange\")\n",
    "        axs[0].legend(loc = \"upper left\", ncol = 3)\n",
    "\n",
    "        Cum_Median = Norm_p.rolling(len(Norm_p), min_periods=2).median()\n",
    "        Cum_Median[0][0] = Norm_p[0][0]\n",
    "        ye = np.array(Cum_Median)\n",
    "\n",
    "        # Creating plot for median\n",
    "        axs[1].set_title(\"Median for \" + name +\" distribution\")\n",
    "        axs[1].plot(x_vals, ye, label = \"Cumulative median\", color=\"red\")\n",
    "        axs[1].plot([1, n], [median, median], label = \"Distribution mean\", color=\"orange\")\n",
    "        axs[1].legend(ncol = 2)\n",
    "\n",
    "        # Give plots more information\n",
    "        fig.supxlabel(\"Number of elements used\")\n",
    "        plt.setp(axs[0], ylabel = \"Mean\")\n",
    "        plt.setp(axs[1], ylabel = \"Median\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if safeFig:\n",
    "            # Save image of the plot locally\n",
    "            plt.savefig(\"my_data/\" + name +\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data, name, log = False, remove_empty = False, n_bins = 50):\n",
    "    \"\"\"\n",
    "    Creates a histogram from the data given\n",
    "    \n",
    "    Args:\n",
    "        data(List): List with datapoints\n",
    "        log(Bool): If True the plots will be in log-log scale\n",
    "        remove_empty(Bool): Removes empty bins from the plot\n",
    "        n_bins(Int): Number of bins for the hist\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup. Creating bins according to the given data\n",
    "    if log:\n",
    "        bins = np.logspace(np.log10(max(1, int(min(data)))), max(1, np.log10(np.ceil(max(data)) + int(max(data)/20))), n_bins)\n",
    "        hist, edges = np.histogram(data, bins = bins, density = True) # Probability density to not penalise the first bins for being smaller.\n",
    "    else:\n",
    "        bins = np.linspace(int(min(data)), np.ceil(max(data)) + int(max(data)/20), n_bins)\n",
    "        hist, edges = np.histogram(data, bins = bins)\n",
    "\n",
    "    x = (edges[:-1] + edges[1:])/2\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Remove bins without any points in\n",
    "    if remove_empty:\n",
    "        xx,yy = zip(*[(i,j) for (i,j) in zip(x, hist) if j > 0])\n",
    "        # ax.bar(xx, yy, width = width)\n",
    "        ax.plot(xx,yy, marker = '.')\n",
    "    else:\n",
    "        #ax.bar(x, hist, width = width * 0.98)\n",
    "        ax.plot(x, hist, marker = '.')\n",
    "\n",
    "    ax.set_xlabel(name)\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.title(f'Probability density function for {name}')\n",
    "    \n",
    "    #Sets the histogram to log-log scale\n",
    "    if log:\n",
    "        ax.set_ylabel('Probability density')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data distributions \n",
    "# Generate distributions \n",
    "np.random.seed(10)\n",
    "n = 10000\n",
    "n2 = 500\n",
    "\n",
    "# Normal distribution \n",
    "normal_mu = 0 \n",
    "normal_std = 4\n",
    "normal = np.random.normal(normal_mu, normal_std, size=(n))\n",
    "\n",
    "# pareto \n",
    "pareto_alpha = 0.5 \n",
    "pareto = np.random.pareto(pareto_alpha, size=n)\n",
    "pareto_mean = None \n",
    "pareto_median = 1*(2)**(1/pareto_alpha)\n",
    "\n",
    "# lognormal \n",
    "lognormal_mu = 0 \n",
    "lognormal_std = 4\n",
    "lognormal = np.random.lognormal(lognormal_mu, lognormal_std, size= n)\n",
    "lognormal_mean = np.exp(lognormal_mu + lognormal_std**2 / 2)\n",
    "lognormal_median = np.exp(lognormal_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5680/3989592368.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcreate_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormal_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormal_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"normal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcreate_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormal_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormal_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"normal (500)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5680/2194616678.py\u001b[0m in \u001b[0;36mcreate_plot\u001b[1;34m(data, mean, median, show_hist, plot, safeFig, n, name)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Setup for the plots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot normal\n",
    "create_plot(normal, mean=normal_mu, median=normal_mu, n=n, name=\"normal\")\n",
    "create_plot(normal[:n2], mean=normal_mu, median=normal_mu, n=n2, name=\"normal (500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pareto \n",
    "create_plot(pareto, mean=pareto_mean, median=pareto_median, n=n, name=\"pareto\")\n",
    "create_plot(pareto[:n2], mean=pareto_mean, median=pareto_median, n=n2, name=\"pareto (500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lognormal\n",
    "create_plot(lognormal, mean=lognormal_mean, median=lognormal_median, n=n, name=\"lognormal\")\n",
    "create_plot(lognormal[:n2], mean=lognormal_mean, median=lognormal_median, n=n2, name=\"logmormal (500)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample citation counts from the 2009 computational social science papers\n",
    "df_papers = pd.read_csv(\"DataFoundPreviously/df_paper.csv\")\n",
    "mask_2009 = df_papers['year'] == 2009\n",
    "citationCount_all = np.array(df_papers['citationCount'][mask_2009])\n",
    "citations_mean = np.mean(citationCount_all)\n",
    "citations_median = np.median(citationCount_all)\n",
    "sampledCitationCount = np.random.choice(citationCount_all, n)\n",
    "\n",
    "create_plot(sampledCitationCount, mean=citations_mean, median=citations_median, n=n, name=\"citations\")\n",
    "create_plot(sampledCitationCount[:n2], mean=citations_mean, median=citations_median, n=n2, name=\"citations (500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of all 4 distributions \n",
    "\n",
    "# Plot normal\n",
    "plot_hist(normal, name=\"Normal Distribution\", remove_empty = True)\n",
    "plot_hist(normal, name=\"Normal Distribution\", remove_empty = True, log = True)\n",
    "\n",
    "# pareto\n",
    "plot_hist(pareto, name=\"Pareto Distribution\", remove_empty = True)\n",
    "plot_hist(pareto, name=\"Pareto Distribution\", remove_empty = True, log = True)\n",
    "\n",
    "# Lognormal\n",
    "plot_hist(lognormal, name=\"Lognormal Distribution\", remove_empty = True)\n",
    "plot_hist(lognormal, name=\"Lognormal Distribution\", remove_empty = True, log = True)\n",
    "\n",
    "# Citation counts \n",
    "plot_hist(sampledCitationCount, name=\"Citation Count Distribution\", remove_empty = True)\n",
    "plot_hist(sampledCitationCount, name=\"Citation Count Distribution\", remove_empty = True, log = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) \n",
    "Compare the evolution of the cumulative average for the Gaussian, Pareto and LogNormal distribution. What do you observe? Would you expect these results? Why?\n",
    "\n",
    "From this experimental setup we see that the Guassian distributions empirical average converges to the true mean value with less values, while the convergence for the LogNormal and Pareto distributions seem to require more values, and is disrupted by sudden realisations which push the empirical average towards much larger values. This happens recurringly.\n",
    "This is more obvious for the Pareto distribution, which is also what we learned in class - the Pareto distributions mean value is defined as infinity for $\\alpha < 1$, since no single value can be infinite we should expect that the empirical average will never converge to anything because we will always get a realisation from the heavy tail that is much bigger than all previous values combined that it can inflate the mean arbitrarily much.\n",
    "The Pareto distribution is extreme compared to the LogNormal that actually has a defined mean value, but both shows how the heavy tails dominate the empirical average value. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) \n",
    "Compare the cumulative median vs the cumulative average for the three distributions. What do you observe? Can you draw any conclusions regarding which statistics (the mean or the median) is more usfeul in the different cases?\n",
    "\n",
    "In this experiment it is clear that even though the mean value takes a very long time to converge, the median converges quite quickly for both of the heavy tailed distributions. This is expected when there is at least a moderate amount of probability mass around the true distribution median, because statistically half of the realisations are larger and half are lower. In other words the tail/outliers of the distribution do not significantly affect the empirical median. \n",
    "\n",
    "The empirical mean and median are of similar note for a Guassian distribution because they converge to the same - they are both central estimators for the Guassian distribution. \n",
    "However, for the skewed/heavy tailed distributions the mean takes quite a while to converge (and in the Pareto distributions case it does not), which makes the median a more informative statistic. This is especially true if we only have few samples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3) \n",
    "Consider the plots you made using the citation count data in point 14. What do you observe? What are the implications?\n",
    "\n",
    "In our case we see that the empirical mean of the citation counts does converge, but has some jumps like the heavy tailed distributions. To investigate further if it indeed follows a heavy tailed distribution we plot the histogram of all 4 distributions. Here we see that it does not follow a something very close to a straight line as the Pareto and LogNormal distribution, but it is definitly more heavy tailed than the Guassian distribution. \n",
    "\n",
    "For this reason the median is likly a more informative statistic than the mean value. Because there are not many jumps in the empirical average plot, we believe that this empirical average is a fairly good estimator of the mean. (Note: we don't actually know the distribution mean, the closest we can get is taking the average of ALL samples from 2009)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4) \n",
    "What do you think are the main take-home message of this exercise?\n",
    "\n",
    "We believe that the lesson to learn is that statistics do not uniquely define all distributions. Statistics are usefull tools to help describe and compare distributions, but if the distribution is unknown it is a good idea to examine it further than just looking at a few statistics.   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eedd9fc418f36e2614580cbf31de2386521dabe9ec8a80375aa0fadc40a33bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to reader \n",
    "We found it helpfull to make the exercises as scripts for the different weeks, so that it is easier to run on a new device. The scripts used can be found in this github aswell and are the reason that much of our code is seperated into functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - all imports \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was needed to run on certain devices / disables certain warnings that will stop the script\n",
    "def disable_warnings():\n",
    "    requests.packages.urllib3.disable_warnings()\n",
    "    requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n",
    "    try:\n",
    "        requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n",
    "    except AttributeError:\n",
    "    # no pyopenssl support used / needed / available\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 posters \n",
    "def get_2019_posters(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : https://2019.ic2s2.org/posters/\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get the webpage data\n",
    "    LINK = \"https://2019.ic2s2.org/posters/\"\n",
    "    r = requests.get(LINK)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "\n",
    "    # Get each bullet point under class \"col-md-8\" (all the names are here)\n",
    "    text = soup.find(\"div\", {\"class\": \"col-md-8\"})\n",
    "    items = text.find_all(\"li\")\n",
    "    item_list = [str(item) for item in items]\n",
    "\n",
    "    # Get content between > and <, and seperate at , \n",
    "    regex_compiler = re.compile(\"(?<=\\>)(.*?)(?=\\<)\")\n",
    "\n",
    "    names = [regex_compiler.findall(item)[0] for item in item_list]\n",
    "    ind_names = [re.split(', | and', name) for name in names]\n",
    "    persons =  []\n",
    "    \n",
    "    # Collect to one list\n",
    "    for list in ind_names:\n",
    "        for name in list:\n",
    "            persons.append(name)\n",
    "\n",
    "    # Verbose \n",
    "    people_set = set(persons)\n",
    "    if verbose: print(f\"There are {len(people_set)} different people and {len(persons)} name occurences in {LINK}\")\n",
    "    return people_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 oral presentations \n",
    "def get_2019_oral(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : https://2019.ic2s2.org/oral-presentations/\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    def find_between( s, first, last ):\n",
    "        \"\"\"\n",
    "        Returns the part of a string that is in the middel of first and last (substrings)\n",
    "\n",
    "        Args:\n",
    "            s (String): The string\n",
    "            first (String): the start \"token\"\n",
    "            last (String): the end \"token\"\n",
    "\n",
    "        Returns:\n",
    "            _type_: Substring between \"first\" and \"last\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start = s.index( first ) + len( first )\n",
    "            end = s.index( last, start )\n",
    "            return s[start:end]\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    \n",
    "    # Get the webpage data\n",
    "    LINK = \"https://2019.ic2s2.org/oral-presentations/\"\n",
    "    r = requests.get(LINK)\n",
    "    soup = BeautifulSoup(r.content) \n",
    "    \n",
    "    # All names are between these two titles \n",
    "    bet = find_between(str(soup),\"1A Misinformation\",\"Evidence of Influence Hierarchies in GitHub’s Cryptocurrency Community\" )\n",
    "    spli = bet.split(\"<p>\") # each <p> has it's own section with a chairname and a list of presenters\n",
    "    \n",
    "    # Get the chairname \n",
    "    chair_names = []\n",
    "    for i in spli:\n",
    "        e = find_between(i, \"Chair:\", \"</em>\")\n",
    "        chair_names.append(e)\n",
    "    \n",
    "    # Get the other occupants \n",
    "    new_list = []\n",
    "    for d in spli:\n",
    "        new_list.append(find_between(d,\"</em><br/>\",\"</p>\"))\n",
    "    \n",
    "    newer_list = []\n",
    "    #Remove known non-name including files\n",
    "    for i in range(20): \n",
    "        abe = new_list[i].split(\"<br/>\")\n",
    "        for x in abe:\n",
    "            x = x[16:]\n",
    "            if str(x) == \"No Presentation\":\n",
    "                pass\n",
    "            else:\n",
    "                newer_list.append(x)\n",
    "    \n",
    "    # Seperate into two schools - the ones that end with .  and the ones with - \n",
    "    # Seperate names at , and remove empty names. \n",
    "    # If there is a : then it is not a name, only take what is after. \n",
    "    names_list = []\n",
    "    for i in newer_list:\n",
    "        if str(i[-1]) == \"–\":\n",
    "            lol = str(i[:-2]).split(\",\")\n",
    "            for m in lol:\n",
    "                if m != \"\":\n",
    "                    if \":\" in m:\n",
    "                        m = m[m.find(\":\")+1:]\n",
    "                    names_list.append(m)\n",
    "        else:\n",
    "            imp_ful = 0\n",
    "            for E in range(len(i)):\n",
    "                if i[E] == \".\" and i[E-2] != \" \":\n",
    "                    imp_ful = E\n",
    "                    break\n",
    "            namees = i[:imp_ful].split(\",\")\n",
    "            for O in namees:\n",
    "                if O != \"\":\n",
    "                    if \":\" in O:\n",
    "                        O = O[O.find(\":\")+2:]\n",
    "                    names_list.append(O)\n",
    "    \n",
    "    # Delete \"No presentation (cancelled)\" entries \n",
    "    for i in range(len(names_list)):\n",
    "        if names_list[i-1] == \"No presentation (cancelled)\":\n",
    "            names_list.pop(i-1)\n",
    "    \n",
    "    # Merge the two lists \n",
    "    final_list = []\n",
    "    for i in names_list+chair_names:\n",
    "        if i[0] == \" \":\n",
    "            final_list.append(i[1:])\n",
    "        else:\n",
    "            final_list.append(i)\n",
    "        \n",
    "    return set(final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020 \n",
    "def get_2020_all(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : \"https://ic2s2.mit.edu/program\"\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get page content \n",
    "    LINK = \"https://ic2s2.mit.edu/program\"\n",
    "    req = requests.get(LINK, verify= False)\n",
    "    soup = BeautifulSoup(req.content, features=\"html.parser\")\n",
    "    \n",
    "    # Scrape link to the page with the actuel content\n",
    "    text = soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "    str_text = str(text).split(\"src=\")\n",
    "    docs_link = str_text[-1].split(\" \")[0][1:-1]\n",
    "    \n",
    "    # All names are stored in the table in the class \"waffle\" \n",
    "    r = requests.get(docs_link)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\": \"waffle\"})\n",
    "    table_rows = table.find_all(\"tr\") # Get all rows \n",
    "    \n",
    "    # Go through each row and put the data into a list (row of dataframe)\n",
    "    rows = []\n",
    "    for tr in table_rows[1:]:\n",
    "        tds = tr.find_all('td')\n",
    "        row = [td.text.replace(\"\\n\",\"\") for td in tds]\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Make the dataframe \n",
    "    df = pd.DataFrame(rows)#, columns=header[0:5])\n",
    "    # Extract names from column 2 (zero indexed) starting from the 1 (zero indexed) element\n",
    "    names_plus = [name.split(', ') for name in list(df.iloc[:,2][1:])]\n",
    "    names = []\n",
    "    for name in names_plus:\n",
    "        for n in name:\n",
    "            if len(n) :\n",
    "                names.append(n)\n",
    "    if verbose: f\"There are {len(set(names))} different people and {len(names)} name occurences in {LINK}\"\n",
    "    return list(set(names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021 \n",
    "def get_2021_all(verbose=False): \n",
    "    \"\"\"\n",
    "    Get all names from the webpage : \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get page content \n",
    "    LINK = \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "    request = requests.get(LINK)\n",
    "    soup = BeautifulSoup(request.content)\n",
    "    \n",
    "    # All names are in the table \"index\" and we split at \"tr\"; each containing one name\n",
    "    contents = soup.find(\"table\", \"index\")\n",
    "    contents = contents.find_all(\"tr\")\n",
    "    \n",
    "    # Regex compiler that finds elements between > < (not including)\n",
    "    regex_compiler = re.compile(\"\\>(.*?)\\<\")\n",
    "    names = set()\n",
    "    counter = 0\n",
    "    \n",
    "    # Go through each tr (statement) and split at each td find the first name and surname\n",
    "    for content in contents: \n",
    "        person = str(content.find_all(\"td\")[0])\n",
    "        titles = regex_compiler.findall(person)[2:-1]\n",
    "        if len(titles) == 2: # There is not exactly two elements it is not a name (but a Alphabetic code)\n",
    "            name = titles[1] + \" \" + titles[0]\n",
    "            name = name.replace(\",\", \"\")\n",
    "            name = name.strip()\n",
    "            \n",
    "            names.add(name) \n",
    "            counter +=1\n",
    "\n",
    "    if verbose: f\"There are {len(names)} different people and {counter} name occurences in {LINK}\"\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions, as the datasets will need to be stored underway as they are very large\n",
    "def save_data(data, file_name): \n",
    "    \"\"\"\n",
    "    A function to save a dictionary (or set) \n",
    "\n",
    "    Args:\n",
    "        ids (data): data/set that needs to be stored\n",
    "        file_name (str): file name \n",
    "    \"\"\"\n",
    "    np.save(f\"{file_name}.npy\", data)\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads a data object (dict or set) \n",
    "\n",
    "    Args:\n",
    "        file_name (str): file_name (without prefix)\n",
    "    Returns:\n",
    "        set or dict: data in the file\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.load(f\"{file_name}.npy\", allow_pickle=True).item()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ic2s2.mit.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total we have 2133 unique computational social scientists across the three years\n"
     ]
    }
   ],
   "source": [
    "# Scrape data from all three years \n",
    "def get_all_names(verbose=False, Body = False):\n",
    "    \"\"\"\n",
    "    Collects all four datasets's names into one set \n",
    "\n",
    "    Args:\n",
    "        verbose (bool, optional): True -> prints comments about how many times names appear and how many there are\n",
    "    \"\"\"\n",
    "    if Body: disable_warnings()\n",
    "\n",
    "    # Run the previous four methods\n",
    "    names_2019_poster = get_2019_posters(verbose)\n",
    "    names_2019_oral = get_2019_oral(verbose)\n",
    "    names_2020_all = get_2020_all(verbose)\n",
    "    names_2021_all = get_2021_all(verbose)\n",
    "    \n",
    "    # Collect into one set\n",
    "    names_all = set()\n",
    "    names_all.update(names_2019_poster, names_2019_oral, names_2020_all, names_2021_all)\n",
    "    \n",
    "    if verbose: print(f\"There are {len(get_all_names())} unique names in total\")\n",
    "    return names_all\n",
    "\n",
    "# To not repeat this time consuming process we save the results and load them if possible\n",
    "science_people_file_name = \"names_week_1\"\n",
    "if os.path.isfile(science_people_file_name + \".npy\"):\n",
    "    science_people = load_data(science_people_file_name)\n",
    "else: \n",
    "    science_people = get_all_names()\n",
    "    save_data(science_people, science_people_file_name)\n",
    "\n",
    "print(f\"In total we have {len(science_people)} unique computational social scientists across the three years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 355 unique authors in the oral presentation for 2019\n",
      "There are 471 unique authors in the poster presentation for 2019\n",
      "There are 774 unique authors in total\n"
     ]
    }
   ],
   "source": [
    "# How many unique authors are there in 2019 split across oral and poster? \n",
    "authors_oral_2019 = get_2019_oral(verbose=False)\n",
    "authors_poster_2019 = get_2019_posters(verbose=False)\n",
    "\n",
    "print(f'There are {len(authors_oral_2019)} unique authors in the oral presentation for 2019')\n",
    "print(f'There are {len(authors_poster_2019)} unique authors in the poster presentation for 2019')\n",
    "print(f'There are {len(set(authors_oral_2019).union(authors_poster_2019))} unique authors in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion from week 1\n",
    "We have found 774 unique authors in the year 2019 with webscraping, and across all three years we have 2133 authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find coauthors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting all coauthors to a set of names  \n",
    "def get_ids_and_coauthors(names, file_name, load_previous=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in a set of names and returns a set of ids of the author and the coauthors to the authors papers\n",
    "\n",
    "    Args:\n",
    "        names (set): names of the authors\n",
    "        file_name (str): file name of where to save progress\n",
    "        verbose (Boolean): whether the function should speak or not\n",
    "        \n",
    "    Returns: \n",
    "        ids (set): Set of author ids for all \"names\" and coauthors on all papers of \"names\"\n",
    "        nin_names (set): Set of names that was not in the sematic scholar database \n",
    "    \"\"\" \n",
    "    \n",
    "    # Base address for requests\n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "    VERSION  = \"v1/\"\n",
    "    RESOURCE = \"author/search?query=\"\n",
    "    ADDITION = \"&fields=papers.authors\"\n",
    "    complete_url = BASE_URL + VERSION + RESOURCE\n",
    "    \n",
    "    # The set of ids \n",
    "    ids = set()\n",
    "    evaluated_names = set() \n",
    "    nin_names = set()\n",
    "    \n",
    "    # Check if the problem has been worked on previously \n",
    "    if load_previous:\n",
    "        try:    \n",
    "            ids = load_data(file_name=file_name)\n",
    "            evaluated_names = load_data(file_name=file_name + \"_evaluated_names\")\n",
    "            nin_names = load_data(file_name=file_name + \"_nin_names\")\n",
    "            if verbose: print(f\"{len(evaluated_names)} already evaluated of {len(names)}\") \n",
    "            names = names - evaluated_names\n",
    "            if verbose:  print(f\"Hence there are {len(names)} left \")\n",
    "        except:\n",
    "            if verbose : print(f\"There are not any progress previously achieved\")\n",
    "        \n",
    "    \n",
    "    # Loop over authors \n",
    "    for i, name in enumerate(names):\n",
    "        # We can only do 150 request each five minuts, use the down time to save progress \n",
    "        if i % 150 == 149: \n",
    "            start_time = time.time()\n",
    "            # printing \n",
    "            if verbose: \n",
    "                print(f\"Completed searches for {i} out of {len(names)}, but reached limit\")\n",
    "            # Save prograss \n",
    "            save_data(ids, file_name=file_name)\n",
    "            save_data(evaluated_names, file_name=file_name + \"_evaluated_names\")\n",
    "            save_data(nin_names, file_name=file_name + \"_nin_names\")\n",
    "            time.sleep(60*5+10 + start_time - time.time()) # the +10 is a buffer \n",
    "        \n",
    "        # Make request \n",
    "        # print(complete_url + name + ADDITION) # Debugging\n",
    "        response = requests.get(complete_url + name + ADDITION).json()\n",
    "        \n",
    "        # If something goes wrong, it will be reported here \n",
    "        try: \n",
    "            for paper in response[\"data\"][0][\"papers\"]: \n",
    "                for author in paper[\"authors\"]: \n",
    "                    ids.add(author[\"authorId\"]) \n",
    "        except: # Usually only occurs if the author has not realeased any papers or is not found \n",
    "            print(f\"The error occured at search number {i}, the name {name} and the response is: \\n {response}\")\n",
    "            nin_names.add(name)\n",
    "        # In either case the name has been evaluated\n",
    "        evaluated_names.add(name) \n",
    "               \n",
    "    # Just to not mess whith the other parts of the code (amount of requests)\n",
    "    if len(names) % 150 > 50: \n",
    "        time.sleep(60*5) \n",
    "    \n",
    "    # Save progress for next time \n",
    "    save_data(ids, file_name=file_name)\n",
    "    save_data(evaluated_names, file_name=file_name + \"_evaluated_names\")\n",
    "    save_data(nin_names, file_name=file_name + \"_nin_names\")\n",
    "            \n",
    "    return ids, nin_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are not any progress previously achieved\n",
      "The error occured at search number 0, the name  and the response is: \n",
      " {'error': \"Missing required parameter: 'query'\"}\n",
      "The error occured at search number 7, the name Mu-jung Cho and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 50, the name Ho-Chun Herbert Chang and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 53, the name Steve R. Scheinert and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 82, the name Deniz Gezerli and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 99, the name Mohammed Aleinzi and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 127, the name Sandor Lera and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 137, the name Aaron Cluaset and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 138, the name Adam Pah and Brian Uzzi and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 141, the name Huei-Yen Chen and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 145, the name Benjamin Ooghe-tabanou and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 149 out of 2133, but reached limit\n",
      "The error occured at search number 149, the name Ricarda Schafer and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 153, the name Arseny Rasov and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 170, the name Phoebe Yao and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 207, the name Justin Chun-Ting Ho and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 211, the name Georg-christoph Haas and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 236, the name Ho-chun Chang and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 251, the name Joseph D. O Brien and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 253, the name Je Hoon Chae and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 255, the name Niklas Dorner and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 259, the name Yi-nung Huang and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 299 out of 2133, but reached limit\n",
      "The error occured at search number 303, the name Javier Garcia-Bernardo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 318, the name Tung-Duong Mai and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 320, the name Manuel Garcia-Herranz and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 350, the name Abhi Dubey and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 381, the name Abiram Gangavaram and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 394, the name Chung-Hong Chan and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 428, the name Viacheslav A. Shibaev and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 448, the name Pik-Mai Hui and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 449 out of 2133, but reached limit\n",
      "The error occured at search number 480, the name Reid Mcilroy-young and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 490, the name Sandra Gonzalez-Bailon and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 494, the name Mikhail Sirkenko and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 532, the name Julia Atienza-barthelemy and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 546, the name R. Rio-chanona and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 566, the name Kartiki Maheshwari and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 576, the name Casandra Ciuchina-Szabo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 585, the name Deok-sun Lee and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 599 out of 2133, but reached limit\n",
      "The error occured at search number 606, the name Lluc Font-Pomarol and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 611, the name Dominic Saad and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 616, the name Kyriacos Vitalis and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 620, the name Paul Guille-escuret and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 626, the name Philip Borgmann and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 645, the name Flora Samu and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 654, the name Diego Saez-Trumper and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 659, the name Mariano Beguerisse-diaz and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 663, the name Samuel Martin-gutierrez and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 665, the name Sahiti Sarva and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 672, the name Utkarshani Ja and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 678, the name José-Ignacio Alvarez-Hamelin and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 689, the name Petter Tornberg and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 707, the name Giuseppe Joe Labiancaba and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 709, the name Jakob Palinger and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 727, the name Mario Gutierrez-roig and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 743, the name Hang-hyun Jo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 747, the name Zachary Steinert-Threlkeld and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 748, the name Matthewhew Roughan and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 749 out of 2133, but reached limit\n",
      "The error occured at search number 763, the name Matthewhieu Nadini and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 783, the name Apratim Mishra and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 795, the name Philipp Kling and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 796, the name Miriam Koschate-reis and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 803, the name Marco Fiore and the response is: \n",
      " {'message': 'Internal server error'}\n",
      "The error occured at search number 816, the name Matthewhew Baum and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 821, the name Matthewhew Jones and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 833, the name Eszter Bokanyi and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 851, the name Javier Borge-holthoefer and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 858, the name Cristian Candia Vallejos and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 861, the name Diego Fregolent Mendes de Oliveira and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 865, the name King-wa Fu and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 866, the name Pietro Rampazzo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 870, the name Lana Bilalova and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 871, the name Luisa Fernanda Chaparro and the response is: \n",
      " {'message': 'Internal server error'}\n",
      "The error occured at search number 876, the name Tulin Inkaya and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 881, the name Anamika Shreeva and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 884, the name Nicholas Hagar and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 887, the name Jeffery Schank and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 889, the name Gloria Wai Shan Ma and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 895, the name Hannah Weinberg-wolf and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 898, the name Simon Kuhne and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 899 out of 2133, but reached limit\n",
      "The error occured at search number 932, the name Gulcan Petricli and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 933, the name Hsien-Te Kao and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 957, the name Bruno Toshio Sugano and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 980, the name Irma Varela-Lasheras and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 986, the name Markus Reiter-Haas and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 993, the name Yy Ahn and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 995, the name Christopher Torres-lugo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1012, the name Sergio Nasarre-Aznar and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1033, the name Mariano Gastón Beiró and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1045, the name Pik-mai Hui and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1046, the name Anne-Marie Nussberger and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 1049 out of 2133, but reached limit\n",
      "The error occured at search number 1069, the name Nicholas Johnson-restrepo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1083, the name Alexandru-Ionut Babeanu and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1106, the name Nicolas Velazquez-hernandez and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1107, the name Javier Alvarez-Galvez and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1112, the name Allison Koencke and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1113, the name Matthewo Mildenberger and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1115, the name Jacob Staerk-ostergaard and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1128, the name Henry Smart III and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1138, the name Deok-Sun Lee and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1149, the name Daniel Bilsen and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1154, the name Ian Anderson and the response is: \n",
      " {'message': 'Endpoint request timed out'}\n",
      "The error occured at search number 1157, the name Yong-Yeol Ahn and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1164, the name Lewis Mitchell and the response is: \n",
      " {'message': 'Endpoint request timed out'}\n",
      "Completed searches for 1199 out of 2133, but reached limit\n",
      "The error occured at search number 1201, the name Marta Sales-Pardo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1211, the name Julia Galantai and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1216, the name Jian Zhu and the response is: \n",
      " {'message': 'Endpoint request timed out'}\n",
      "The error occured at search number 1240, the name  Philip Howard and the response is: \n",
      " {'message': 'Internal server error'}\n",
      "The error occured at search number 1245, the name Reid McIlroy-Young and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1252, the name Johanna Temps and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1255, the name Sho Izumo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1264, the name Christos Nicolaides,Luis Cueto-Felgueroso and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1265, the name Sebastian Munoz-najar and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1279, the name Prof Levene and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1284, the name Peter Ejbye-Ernst and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1292, the name Isabella Czedik-eysenberg and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1293, the name Ryan Gallagheer and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1297, the name Xi Chen and the response is: \n",
      " {'message': 'Internal server error'}\n",
      "The error occured at search number 1301, the name Ana-andreea Stoica and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1322, the name Jean-philippe Cointet and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1337, the name Juan Mateos-Garcia and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 1349 out of 2133, but reached limit\n",
      "The error occured at search number 1354, the name and Rens Wilderom and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1361, the name Viktor Aigenseer and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1367, the name Felix Munch and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1382, the name Peter Barbrook-Johnson and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1393, the name Tim Schatto-Eckrodt and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1415, the name  Donna Spruijt-Metz and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1417, the name Lisette Espín-Noboa and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1437, the name Hossein Rahmana and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1446, the name Alberto Sánchez-Carralero and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1452, the name Tasi-ching Lu and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1459, the name Polina Revina and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1466, the name Sjoerd Hooijmaaijers and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1477, the name Hang-Hyun Jo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 1499 out of 2133, but reached limit\n",
      "The error occured at search number 1513, the name Marina Mancoridis and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1520, the name  Roy Lay-Yee and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1534, the name Matthewhew Turner and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1535, the name Christopher Torres-Lugo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1537, the name Anne Toonders and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1550, the name  Joana Gonçalves-Sá and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1568, the name Emőke-Ágnes Horvát and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1581, the name Marc-Etienne Brunet and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1596, the name Maya Josifovska and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1597, the name Krisztián Boros and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1598, the name Fosca Gianotti and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1602, the name Jennifer Stromer-galley and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1605, the name Yaneer Bar-yam and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1611, the name Jose Carlock and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1637, the name Dhruv Sapra and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 1649 out of 2133, but reached limit\n",
      "The error occured at search number 1652, the name Ramon Villa-Cox and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1686, the name Chrysan Angela Piarso and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1690, the name  Victor Suarez-Lledo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1692, the name Forough Poursabzi-sangdeh and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1695, the name Sunje Paasch-colberg and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1705, the name Jean-francois Bonnefon and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1737, the name Micheal Szell and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1744, the name Zach Fulker and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1764, the name Benjamin Ooghe-Tabanou and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 1799 out of 2133, but reached limit\n",
      "The error occured at search number 1808, the name Marta Rivera-Alba and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1815, the name Karunakar Reddy Mannem and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1822, the name Sebastian Munoz-Najar Galvez and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1837, the name Kira von Kleist and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1839, the name Naseem Makiya and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1845, the name Joana Gonçalves-Sá and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1858, the name Chia-Jung Lee and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1867, the name Anna Croley and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1894, the name Emily Bello-Pardo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1911, the name Carlos X. Lastra-Anadon and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1921, the name Nathante Teblunthuis and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1922, the name Gord Pennycook and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1927, the name Polina Mosolova and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1936, the name Jean Garcia-gathright and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 1949 out of 2133, but reached limit\n",
      "The error occured at search number 1962, the name Lara Lofdahl and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1973, the name Ana Alonso-Curbelo and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1981, the name Issa Luna-pla and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 1983, the name Jean-Philippe Cointet and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 2049, the name Lorena Cano-Orón and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 2062, the name Juliano Cavalli and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 2078, the name Philipp Lorenz-Spreen and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 2086, the name Ben Rachunok and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "Completed searches for 2099 out of 2133, but reached limit\n",
      "The error occured at search number 2106, the name Jianyin Roachell and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 2108, the name Jay Kachhadia and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "The error occured at search number 2116, the name Forough Poursabzi-Sangdeh and the response is: \n",
      " {'total': 0, 'offset': 0, 'data': []}\n",
      "We have found 118159 coauthors of the 2133 from week 1\n",
      "Of the 2133 authors, 181 where not found\n"
     ]
    }
   ],
   "source": [
    "# Get all coauthors to the 2133 authors found in week 1 \n",
    "ids, nin_names = get_ids_and_coauthors(science_people, file_name=\"ids_dict\", load_previous=True, verbose=True) \n",
    "\n",
    "print(f\"We have found {len(ids)} coauthors of the {len(science_people)} from week 1\")\n",
    "print(f\"Of the {len(science_people)} authors, {len(nin_names)} where not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion and thoughts \n",
    "We see that we have now gathered a dataset of more than 100000 authors, which is quite a lot. We have the issue that we can only make about 150 requests to semantic scholar every five minutes and hence we need to batch our id searches, as the process will otherwise take to long. \n",
    "\n",
    "This is however not easy as semantic scholar can only handle batches of size 100 when we are also asking for the papers written by each author. \n",
    "\n",
    "##### Thoughts after initial struggles \n",
    "The amount of data that we want to gather from sematic scholar is quite large, as a dictionary containing the first 10000 author id's as keys takes up 1.7 GB of data. \n",
    "Furthermore, semantic scholar can often not handle batches of size 100 if there is two much data in the batch, hence the error handeling in the next couple of methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting authors into a dataframe\n",
    "def format_authors(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors with the data; id, name, alias, citationCount, field\n",
    "    \"\"\"\n",
    "    # Create people dataframe \n",
    "    zero_data = np.zeros((len(ids_dict.keys()), 5))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"name\", \"aliases\", \"citationCount\", \"field\"])\n",
    "\n",
    "    for i, id in enumerate(ids_dict.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id \n",
    "        information = ids_dict[str(id)]\n",
    "        # Name\n",
    "        df[\"name\"][i] = information[\"name\"]\n",
    "        # Aliases\n",
    "        df[\"aliases\"][i] = information[\"aliases\"] \n",
    "        # citation count \n",
    "        citation_count = 0\n",
    "        for paper in information[\"papers\"]: \n",
    "            citation_count += paper[\"citationCount\"]\n",
    "        df[\"citationCount\"][i] = citation_count\n",
    "        # field - count each occurence and take the maximum \n",
    "        potential_fields = {}\n",
    "        for paper in information[\"papers\"]: \n",
    "            for fields in paper[\"s2FieldsOfStudy\"]:\n",
    "                field = fields[\"category\"]\n",
    "                try:\n",
    "                    potential_fields[field] += 1\n",
    "                except: \n",
    "                    potential_fields[field] = 1\n",
    "        if potential_fields == {}: \n",
    "            pass\n",
    "        else: \n",
    "            # This is a spicy way to do this o.0\n",
    "            df[\"field\"][i] = max(potential_fields, key=potential_fields.get)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting papers into a dataframe\n",
    "def format_papers(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors's papers with the data; id, title, year, DOI, citationCount, field, authors\n",
    "    \"\"\"\n",
    "    # Start by making a dictionary of papers instead of authors\n",
    "    papers = {}\n",
    "    for id in ids_dict.keys():\n",
    "        # get the papers \n",
    "        information = ids_dict[str(id)]\n",
    "        for paper in information[\"papers\"]:\n",
    "            if paper[\"paperId\"] in papers:\n",
    "                pass\n",
    "            else: \n",
    "                # Find the author id's\n",
    "                authors = set()\n",
    "                for author in paper[\"authors\"]:\n",
    "                    authors.add(author[\"authorId\"])\n",
    "                # Make new elements in the dictionary \n",
    "                papers[paper[\"paperId\"]] = {\"id\": paper[\"paperId\"],\n",
    "                                            \"title\": paper[\"title\"], \n",
    "                                            \"year\": paper[\"year\"], \n",
    "                                            \"doi\": paper[\"externalIds\"],\n",
    "                                            \"citationCount\": paper[\"citationCount\"], \n",
    "                                            \"field\": paper[\"s2FieldsOfStudy\"], \n",
    "                                            \"authors\": authors}\n",
    "    # Create the paper dataframe \n",
    "    zero_data = np.zeros((len(papers.keys()), 7))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"title\", \"year\", \"doi\", \"citationCount\", \"field\", \"authors\"])\n",
    "\n",
    "    for i, id in enumerate(papers.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id\n",
    "        information = papers[str(id)]\n",
    "        # title\n",
    "        df[\"title\"][i] = information[\"title\"]\n",
    "        # Aliases\n",
    "        df[\"year\"][i] = information[\"year\"] \n",
    "        # DOI \n",
    "        df[\"doi\"][i] = [information[\"doi\"]] # Can't have a dict, but it is okay to wrap it with list\n",
    "        # citation count \n",
    "        df[\"citationCount\"][i] = information[\"citationCount\"]\n",
    "        # field \n",
    "        df[\"field\"][i] = information[\"field\"] # Does this work? Yes somehow \n",
    "        # authors \n",
    "        df[\"authors\"][i] = list(information[\"authors\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting paper abstracts into a dataframe\n",
    "def format_paper_abstracts(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors's papers with the data; id, abstract\n",
    "    \"\"\"\n",
    "    # Start by making a dictionary of papers instead of authors\n",
    "    papers = {}\n",
    "    for id in ids_dict.keys():\n",
    "        # get the papers \n",
    "        information = ids_dict[str(id)]\n",
    "        for paper in information[\"papers\"]:\n",
    "            if paper[\"paperId\"] in papers:\n",
    "                pass\n",
    "            else:\n",
    "                # Make new elements in the dictionary \n",
    "                papers[paper[\"paperId\"]] = {\"id\": paper[\"paperId\"],\n",
    "                                            \"title\": paper[\"abstract\"]}\n",
    "    # Create the paper dataframe \n",
    "    zero_data = np.zeros((len(papers.keys()), 2))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"abstract\"])\n",
    "\n",
    "    for i, id in enumerate(papers.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id\n",
    "        information = papers[str(id)]\n",
    "        # abstract\n",
    "        df[\"abstract\"][i] = information[\"abstract\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that formats a dictionary into dataframes and stores them\n",
    "def create_dataframes(ids_dict, prefix=\"\", verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary of authors and their papers and generates two datasets and stores these\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): The key is an auther id, the contents is a dictionary with three atributes, \"name\", \"aliases\" and \"papers\".\n",
    "        prefix (str): If the function will be called multiple times, this is to not overwrite previous stored files\n",
    "    \n",
    "    Return: \n",
    "        df_author: The above specified dataframe for authors\n",
    "        df_paper: The above specified dataframe for papers\n",
    "    \"\"\"\n",
    "    # Create dataframes\n",
    "    # Authors \n",
    "    df_author = format_authors(ids_dict)\n",
    "    if verbose: print(\"Formatted author dataframe\")\n",
    "    pd.DataFrame.to_csv(df_author, f\"df_author{prefix}.csv\")\n",
    "    if verbose: print(\"Saved author dataframe\")\n",
    "    \n",
    "    # Papers\n",
    "    df_paper = format_papers(ids_dict)\n",
    "    if verbose: print(\"Formatted paper dataframe\")\n",
    "    pd.DataFrame.to_csv(df_author, f\"df_paper{prefix}.csv\")\n",
    "    if verbose: print(\"Saved paper dataframe\")\n",
    "    \n",
    "    # Paper abstracts \n",
    "    df_paper = format_paper_abstracts(ids_dict)\n",
    "    if verbose: print(\"Formatted paper dataframe\")\n",
    "    pd.DataFrame.to_csv(df_author, f\"df_paper{prefix}.csv\")\n",
    "    if verbose: print(\"Saved paper dataframe\")\n",
    "    \n",
    "    return df_author, df_paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that finds all papers from each author\n",
    "def get_data_from_ids(ids, verbose=False, load_previous=True, file_name=\"ids_enumerated_dict\"): \n",
    "    \"\"\"\n",
    "    Returns a complete data frame of \n",
    "    authors (id, name, alias, citationCount, field) and\n",
    "    papers (id, title, year, DOI, citationCount, field, authors)\n",
    "\n",
    "    Args:\n",
    "        ids (set): ids of the authors in question\n",
    "\n",
    "    Returns:\n",
    "        file_extension (int): The number of dataframes created\n",
    "        nin_ids (set): The set of ids that could not be processed  \n",
    "    \"\"\"\n",
    "    ids_dict = {} # This will continually be reset, otherwise it would take up to much space\n",
    "    evaluated_ids = set()\n",
    "    nin_ids = set()\n",
    "    ids_dict[\"file_extension\"] = 0\n",
    "    \n",
    "    # Check if the problem has been worked on previously \n",
    "    if load_previous:\n",
    "        try: \n",
    "            ids_dict = load_data(file_name=file_name)\n",
    "            evaluated_ids = load_data(file_name=file_name + \"_evaluated_ids\")\n",
    "            nin_ids = load_data(file_name=file_name + \"_nin_ids\")\n",
    "            if verbose: print(f\"{len(evaluated_ids)} already evaluated of {len(ids)}\") \n",
    "            ids = ids - evaluated_ids\n",
    "            if verbose: print(f\"Hence there are {len(ids)} left \")\n",
    "        except:\n",
    "            print(f\"There are no previous progress made\")\n",
    "    \n",
    "    # Partition the ids into batches of 20 ids, because semantic scholar can only take that many \n",
    "    ids = list(ids) # temporary to get results\n",
    "    default_batch_size = 64\n",
    "    batch_size = default_batch_size\n",
    "    n_batches = len(ids) // batch_size + 1\n",
    "    batches_left = True \n",
    "    sent_requests = 0\n",
    "    index = 0 \n",
    "    if verbose: print(f\"Total number of batches are {n_batches}\")\n",
    "        \n",
    "    #  Current file name extension\n",
    "    file_extension = ids_dict[\"file_extension\"]\n",
    "    file_start = 0 \n",
    "    file_size = 8000 # Hope this is small enough \n",
    "    \n",
    "    # Use a while loop to go through each batch of ids so that we can change sizes dynamically\n",
    "    # (This stems from the fact that semantic scholar will return errors if we ask for too much data)\n",
    "    while(batches_left):\n",
    "        # To avoid memory overflow convert to pandas \n",
    "        if index > file_start + file_size: \n",
    "            # Note that the return dataframes from create_dataframes are not used, as we do not have memory enough to keep them\n",
    "            # (they are only stored to physical memory)\n",
    "            create_dataframes(ids_dict=ids_dict, prefix=file_extension, verbose=verbose)\n",
    "            file_start += file_size\n",
    "            file_extension += 1\n",
    "            ids_dict = {}\n",
    "            ids_dict[\"file_extension\"] = file_extension\n",
    "        \n",
    "        # Make request for batch \n",
    "        batch_url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "        data = {\"ids\": ids[index:min(index + batch_size, len(ids))]}\n",
    "        params = {\"fields\": \"aliases,papers.title,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount,papers.abstract,name,papers.authors\"}\n",
    "        response = requests.post(batch_url, json=data, params=params).json()\n",
    "        sent_requests += 1\n",
    "        \n",
    "        # Assert the response\n",
    "        if response == {'message': 'Internal server error'}:\n",
    "            if verbose: print(f\"Server error for index {index} with batch size {batch_size}\")\n",
    "            batch_size = batch_size // 2 # Half batch size and try again \n",
    "            \n",
    "            if batch_size == 0: \n",
    "                # Save the faulty element that makes semantic scholar give internal errors \n",
    "                # and proceed to the next one\n",
    "                if verbose: print(f\"Had to remove {index} which is {ids[index]}\")\n",
    "                batch_size = 1 \n",
    "                evaluated_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "                nin_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "                index += batch_size \n",
    "        else: \n",
    "            # If there is something wrong with the request\n",
    "            try: \n",
    "                for person in response: \n",
    "                    # Update dictionary \n",
    "                    # If something goes wrong, it will be reported here \n",
    "                    try: \n",
    "                        ids_dict[person[\"authorId\"]] = {\"name\": person[\"name\"], \n",
    "                                \"aliases\": person[\"aliases\"],\n",
    "                                \"papers\": person[\"papers\"]}\n",
    "                    except: # Usually only occurs if the author has not realeased any papers or is not found \n",
    "                        print(\"Something is wrong with this person (usually it is a None value somehow?)\")\n",
    "                        print(f\"The index is {index} with batch size {batch_size}\")\n",
    "            except: \n",
    "                # If it messes up print the request and put the ids in nin \n",
    "                if verbose:  \n",
    "                    print(response)\n",
    "                    print(f\"The index is {index} with batch size {batch_size}\")\n",
    "                nin_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "            \n",
    "            # Update processed ids \n",
    "            evaluated_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "            index += batch_size\n",
    "            batch_size = default_batch_size\n",
    "        \n",
    "        # If there has been too many request we need to break - we can probably skip this now because requests take so long \n",
    "        if sent_requests % 150 == 149: \n",
    "            start_time = time.time()\n",
    "            # printing \n",
    "            if verbose: \n",
    "                print(f\"Completed searches for {index} out of {len(ids)}, but reached limit\")\n",
    "            # Save prograss \n",
    "            save_data(ids_dict, file_name=file_name)\n",
    "            save_data(evaluated_ids, file_name=file_name + \"_evaluated_ids\")\n",
    "            save_data(nin_ids, file_name=file_name + \"_nin_ids\")\n",
    "            time.sleep(max(60*5+10 + start_time - time.time(), 0)) # the +10 is a buffer\n",
    "        \n",
    "    # Create and save the final dataframes \n",
    "    create_dataframes(ids_dict=ids_dict, prefix=file_extension, verbose=verbose)\n",
    "    \n",
    "    return file_extension, nin_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all dataframes \n",
    "n_files, nin_ids = get_data_from_ids(ids, load_previous=True, verbose=True)\n",
    "\n",
    "print(f\"Of the {len(ids)} authors, {len(nin_ids)} could not be found in the dataset\")\n",
    "print(f\"There are {n_files} dataframes of authors, papers and paper abstracts that needs to be merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes \n",
    "# USE drop_duplicates after merging all the n_files dataframes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eedd9fc418f36e2614580cbf31de2386521dabe9ec8a80375aa0fadc40a33bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

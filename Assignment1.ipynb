{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "Collaborators (Name, study id, github handle): <br />\n",
    "August Hertz Bugge - s194350 - libze<br />\n",
    "David Bro Ludvigsen - s204102 - davidAlvaBro<br />\n",
    "Sebastian Nicolai Fabricius Grut  - s204150 - Sebastiannfg\n",
    "\n",
    "Github : https://github.com/davidAlvaBro/comsocsci2023.git \n",
    "\n",
    "#### Contributions \n",
    "We collaborated as a group. We have had weekly meetings where we completed the weekly assignments together - everything has been discussed and made together. \n",
    "\n",
    "\n",
    "#### Note to reader \n",
    "We found it helpfull to make the exercises as scripts for the different weeks, so that it is easier to run on a new device. The scripts used can be found in this github aswell and are the reason that much of our code is seperated into functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - all imports \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import time\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping (week 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code - Webscraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was needed to run on certain devices / disables certain warnings that will stop the script\n",
    "def disable_warnings():\n",
    "    requests.packages.urllib3.disable_warnings()\n",
    "    requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n",
    "    try:\n",
    "        requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n",
    "    except AttributeError:\n",
    "    # no pyopenssl support used / needed / available\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 posters \n",
    "def get_2019_posters(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : https://2019.ic2s2.org/posters/\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get the webpage data\n",
    "    LINK = \"https://2019.ic2s2.org/posters/\"\n",
    "    r = requests.get(LINK)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "\n",
    "    # Get each bullet point under class \"col-md-8\" (all the names are here)\n",
    "    text = soup.find(\"div\", {\"class\": \"col-md-8\"})\n",
    "    items = text.find_all(\"li\")\n",
    "    item_list = [str(item) for item in items]\n",
    "\n",
    "    # Get content between > and <, and seperate at , \n",
    "    regex_compiler = re.compile(\"(?<=\\>)(.*?)(?=\\<)\")\n",
    "\n",
    "    names = [regex_compiler.findall(item)[0] for item in item_list]\n",
    "    ind_names = [re.split(', | and', name) for name in names]\n",
    "    persons =  []\n",
    "    \n",
    "    # Collect to one list\n",
    "    for list in ind_names:\n",
    "        for name in list:\n",
    "            persons.append(name)\n",
    "\n",
    "    # Verbose \n",
    "    people_set = set(persons)\n",
    "    if verbose: print(f\"There are {len(people_set)} different people and {len(persons)} name occurences in {LINK}\")\n",
    "    return people_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 oral presentations \n",
    "def get_2019_oral(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : https://2019.ic2s2.org/oral-presentations/\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    def find_between( s, first, last ):\n",
    "        \"\"\"\n",
    "        Returns the part of a string that is in the middel of first and last (substrings)\n",
    "\n",
    "        Args:\n",
    "            s (String): The string\n",
    "            first (String): the start \"token\"\n",
    "            last (String): the end \"token\"\n",
    "\n",
    "        Returns:\n",
    "            _type_: Substring between \"first\" and \"last\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start = s.index( first ) + len( first )\n",
    "            end = s.index( last, start )\n",
    "            return s[start:end]\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    \n",
    "    # Get the webpage data\n",
    "    LINK = \"https://2019.ic2s2.org/oral-presentations/\"\n",
    "    r = requests.get(LINK)\n",
    "    soup = BeautifulSoup(r.content) \n",
    "    \n",
    "    # All names are between these two titles \n",
    "    bet = find_between(str(soup),\"1A Misinformation\",\"Evidence of Influence Hierarchies in GitHub’s Cryptocurrency Community\" )\n",
    "    spli = bet.split(\"<p>\") # each <p> has it's own section with a chairname and a list of presenters\n",
    "    \n",
    "    # Get the chairname \n",
    "    chair_names = []\n",
    "    for i in spli:\n",
    "        e = find_between(i, \"Chair:\", \"</em>\")\n",
    "        chair_names.append(e)\n",
    "    \n",
    "    # Get the other occupants \n",
    "    new_list = []\n",
    "    for d in spli:\n",
    "        new_list.append(find_between(d,\"</em><br/>\",\"</p>\"))\n",
    "    \n",
    "    newer_list = []\n",
    "    #Remove known non-name including files\n",
    "    for i in range(20): \n",
    "        abe = new_list[i].split(\"<br/>\")\n",
    "        for x in abe:\n",
    "            x = x[16:]\n",
    "            if str(x) == \"No Presentation\":\n",
    "                pass\n",
    "            else:\n",
    "                newer_list.append(x)\n",
    "    \n",
    "    # Seperate into two schools - the ones that end with .  and the ones with - \n",
    "    # Seperate names at , and remove empty names. \n",
    "    # If there is a : then it is not a name, only take what is after. \n",
    "    names_list = []\n",
    "    for i in newer_list:\n",
    "        if str(i[-1]) == \"–\":\n",
    "            lol = str(i[:-2]).split(\",\")\n",
    "            for m in lol:\n",
    "                if m != \"\":\n",
    "                    if \":\" in m:\n",
    "                        m = m[m.find(\":\")+1:]\n",
    "                    names_list.append(m)\n",
    "        else:\n",
    "            imp_ful = 0\n",
    "            for E in range(len(i)):\n",
    "                if i[E] == \".\" and i[E-2] != \" \":\n",
    "                    imp_ful = E\n",
    "                    break\n",
    "            namees = i[:imp_ful].split(\",\")\n",
    "            for O in namees:\n",
    "                if O != \"\":\n",
    "                    if \":\" in O:\n",
    "                        O = O[O.find(\":\")+2:]\n",
    "                    names_list.append(O)\n",
    "    \n",
    "    # Delete \"No presentation (cancelled)\" entries \n",
    "    for i in range(len(names_list)):\n",
    "        if names_list[i-1] == \"No presentation (cancelled)\":\n",
    "            names_list.pop(i-1)\n",
    "    \n",
    "    # Merge the two lists \n",
    "    final_list = []\n",
    "    for i in names_list+chair_names:\n",
    "        if i[0] == \" \":\n",
    "            final_list.append(i[1:])\n",
    "        else:\n",
    "            final_list.append(i)\n",
    "        \n",
    "    return set(final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020 \n",
    "def get_2020_all(verbose=False):\n",
    "    \"\"\"\n",
    "    Get all names from the webpage : \"https://ic2s2.mit.edu/program\"\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get page content \n",
    "    LINK = \"https://ic2s2.mit.edu/program\"\n",
    "    req = requests.get(LINK, verify= False)\n",
    "    soup = BeautifulSoup(req.content, features=\"html.parser\")\n",
    "    \n",
    "    # Scrape link to the page with the actuel content\n",
    "    text = soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "    str_text = str(text).split(\"src=\")\n",
    "    docs_link = str_text[-1].split(\" \")[0][1:-1]\n",
    "    \n",
    "    # All names are stored in the table in the class \"waffle\" \n",
    "    r = requests.get(docs_link)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\": \"waffle\"})\n",
    "    table_rows = table.find_all(\"tr\") # Get all rows \n",
    "    \n",
    "    # Go through each row and put the data into a list (row of dataframe)\n",
    "    rows = []\n",
    "    for tr in table_rows[1:]:\n",
    "        tds = tr.find_all('td')\n",
    "        row = [td.text.replace(\"\\n\",\"\") for td in tds]\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Make the dataframe \n",
    "    df = pd.DataFrame(rows)#, columns=header[0:5])\n",
    "    # Extract names from column 2 (zero indexed) starting from the 1 (zero indexed) element\n",
    "    names_plus = [name.split(', ') for name in list(df.iloc[:,2][1:])]\n",
    "    names = []\n",
    "    for name in names_plus:\n",
    "        for n in name:\n",
    "            if len(n) :\n",
    "                names.append(n)\n",
    "    if verbose: f\"There are {len(set(names))} different people and {len(names)} name occurences in {LINK}\"\n",
    "    return list(set(names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021 \n",
    "def get_2021_all(verbose=False): \n",
    "    \"\"\"\n",
    "    Get all names from the webpage : \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all unique names occuring in the page\n",
    "    \"\"\"\n",
    "    # Get page content \n",
    "    LINK = \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "    request = requests.get(LINK)\n",
    "    soup = BeautifulSoup(request.content)\n",
    "    \n",
    "    # All names are in the table \"index\" and we split at \"tr\"; each containing one name\n",
    "    contents = soup.find(\"table\", \"index\")\n",
    "    contents = contents.find_all(\"tr\")\n",
    "    \n",
    "    # Regex compiler that finds elements between > < (not including)\n",
    "    regex_compiler = re.compile(\"\\>(.*?)\\<\")\n",
    "    names = set()\n",
    "    counter = 0\n",
    "    \n",
    "    # Go through each tr (statement) and split at each td find the first name and surname\n",
    "    for content in contents: \n",
    "        person = str(content.find_all(\"td\")[0])\n",
    "        titles = regex_compiler.findall(person)[2:-1]\n",
    "        if len(titles) == 2: # There is not exactly two elements it is not a name (but a Alphabetic code)\n",
    "            name = titles[1] + \" \" + titles[0]\n",
    "            name = name.replace(\",\", \"\")\n",
    "            name = name.strip()\n",
    "            \n",
    "            names.add(name) \n",
    "            counter +=1\n",
    "\n",
    "    if verbose: f\"There are {len(names)} different people and {counter} name occurences in {LINK}\"\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions, as the datasets will need to be stored underway as they are very large\n",
    "def save_data(data, file_name): \n",
    "    \"\"\"\n",
    "    A function to save a dictionary (or set) \n",
    "\n",
    "    Args:\n",
    "        ids (data): data/set that needs to be stored\n",
    "        file_name (str): file name \n",
    "    \"\"\"\n",
    "    np.save(f\"{file_name}.npy\", data)\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads a data object (dict or set) \n",
    "\n",
    "    Args:\n",
    "        file_name (str): file_name (without prefix)\n",
    "    Returns:\n",
    "        set or dict: data in the file\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.load(f\"{file_name}.npy\", allow_pickle=True).item()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total we have 2133 unique computational social scientists across the three years\n"
     ]
    }
   ],
   "source": [
    "# Scrape data from all three years \n",
    "def get_all_names(verbose=False, Body = False):\n",
    "    \"\"\"\n",
    "    Collects all four datasets's names into one set \n",
    "\n",
    "    Args:\n",
    "        verbose (bool, optional): True -> prints comments about how many times names appear and how many there are\n",
    "    \"\"\"\n",
    "    if Body: disable_warnings()\n",
    "\n",
    "    # Run the previous four methods\n",
    "    names_2019_poster = get_2019_posters(verbose)\n",
    "    names_2019_oral = get_2019_oral(verbose)\n",
    "    names_2020_all = get_2020_all(verbose)\n",
    "    names_2021_all = get_2021_all(verbose)\n",
    "    \n",
    "    # Collect into one set\n",
    "    names_all = set()\n",
    "    names_all.update(names_2019_poster, names_2019_oral, names_2020_all, names_2021_all)\n",
    "    \n",
    "    if verbose: print(f\"There are {len(get_all_names())} unique names in total\")\n",
    "    return names_all\n",
    "\n",
    "# To not repeat this time consuming process we save the results and load them if possible\n",
    "science_people_file_name = \"names_week_1\"\n",
    "if os.path.isfile(science_people_file_name + \".npy\"):\n",
    "    science_people = load_data(science_people_file_name)\n",
    "else: \n",
    "    science_people = get_all_names()\n",
    "    save_data(science_people, science_people_file_name)\n",
    "\n",
    "print(f\"In total we have {len(science_people)} unique computational social scientists across the three years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 355 unique authors in the oral presentation for 2019\n",
      "There are 471 unique authors in the poster presentation for 2019\n",
      "There are 774 unique authors in total\n"
     ]
    }
   ],
   "source": [
    "# How many unique authors are there in 2019 split across oral and poster? \n",
    "authors_oral_2019 = get_2019_oral(verbose=False)\n",
    "authors_poster_2019 = get_2019_posters(verbose=False)\n",
    "\n",
    "print(f'There are {len(authors_oral_2019)} unique authors in the oral presentation for 2019')\n",
    "print(f'There are {len(authors_poster_2019)} unique authors in the poster presentation for 2019')\n",
    "print(f'There are {len(set(authors_oral_2019).union(authors_poster_2019))} unique authors in total')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion from week 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2) \n",
    "How many unique researchers you got in 2019?\n",
    "\n",
    "We found 774 unique authors in the year 2019 with webscraping, and across all three years we found 2133 authors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3) \n",
    "Explain one or two decisions you took during the web-scraping exercise, for 2019 or any other year. Why did you take this choice? How might your decision impact the final number of authors?\n",
    "\n",
    "2019 - oral </br>\n",
    "This webpage was the hardest to scrape because the layout had many different types of diviants. In each paragraph with names there were a time stamp followed by a list of names \",\" seperated and then a \".\" to end the listing. \n",
    "However, some names contained \".\" such as \"Harvey G. Jensen\". Also some name listnings ended with \"-\" instead. </br>\n",
    "To solve this we split these pagraphs into two branches, one that ends with \"-\" and one that ends with \".\". The second problem was solved by checking if the second letter before the \".\" was a space - if that is the case it is a part of the name. </br>\n",
    "At last when the list of candidate names was found we iterated through it and got rid of faulty entries such as \"No Presentation\". </br>\n",
    "We have included all names between \"\\</em>\\<br/>\" and \"\\</p>\" to find the list of representers and all names between \"Chair:\", \"\\</em>\" to find the chair names (usually repeated later). Only names in these listings are included. Also if a \"-\" is in the name we exclude it. \n",
    "\n",
    "2019 - poster </br>\n",
    "The webscraping execise was a bit simpler. There is one name in each row (\"\\<li>\" to \"\\</li>\") and the name is the part that is the first element in the list when we look at the seperation \">\" to \"<\". \n",
    "In this setting all the names are included as the page layout do not have any diviants. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic scholar (week 2) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code - Find coauthors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting all coauthors to a set of names  \n",
    "def get_ids_and_coauthors(names, file_name, load_previous=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in a set of names and returns a set of ids of the author and the coauthors to the authors papers\n",
    "\n",
    "    Args:\n",
    "        names (set): names of the authors\n",
    "        file_name (str): file name of where to save progress\n",
    "        verbose (Boolean): whether the function should speak or not\n",
    "        \n",
    "    Returns: \n",
    "        ids (set): Set of author ids for all \"names\" and coauthors on all papers of \"names\"\n",
    "        nin_names (set): Set of names that was not in the sematic scholar database \n",
    "    \"\"\" \n",
    "    \n",
    "    # Base address for requests\n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "    VERSION  = \"v1/\"\n",
    "    RESOURCE = \"author/search?query=\"\n",
    "    ADDITION = \"&fields=papers.authors\"\n",
    "    complete_url = BASE_URL + VERSION + RESOURCE\n",
    "    \n",
    "    # The set of ids \n",
    "    ids = set()\n",
    "    evaluated_names = set() \n",
    "    nin_names = set()\n",
    "    \n",
    "    # Check if the problem has been worked on previously \n",
    "    if load_previous:\n",
    "        try:    \n",
    "            ids = load_data(file_name=file_name)\n",
    "            evaluated_names = load_data(file_name=file_name + \"_evaluated_names\")\n",
    "            nin_names = load_data(file_name=file_name + \"_nin_names\")\n",
    "            if verbose: print(f\"{len(evaluated_names)} already evaluated of {len(names)}\") \n",
    "            names = names - evaluated_names\n",
    "            if verbose:  print(f\"Hence there are {len(names)} left \")\n",
    "        except:\n",
    "            if verbose : print(f\"There are not any progress previously achieved\")\n",
    "        \n",
    "    \n",
    "    # Loop over authors \n",
    "    for i, name in enumerate(names):\n",
    "        # We can only do 150 request each five minuts, use the down time to save progress \n",
    "        if i % 150 == 149: \n",
    "            start_time = time.time()\n",
    "            # printing \n",
    "            if verbose: \n",
    "                print(f\"Completed searches for {i} out of {len(names)}, but reached limit\")\n",
    "            # Save prograss \n",
    "            save_data(ids, file_name=file_name)\n",
    "            save_data(evaluated_names, file_name=file_name + \"_evaluated_names\")\n",
    "            save_data(nin_names, file_name=file_name + \"_nin_names\")\n",
    "            time.sleep(60*5+10 + start_time - time.time()) # the +10 is a buffer \n",
    "        \n",
    "        # Make request \n",
    "        # print(complete_url + name + ADDITION) # Debugging\n",
    "        response = requests.get(complete_url + name + ADDITION).json()\n",
    "        \n",
    "        # If something goes wrong, it will be reported here \n",
    "        try: \n",
    "            for paper in response[\"data\"][0][\"papers\"]: \n",
    "                for author in paper[\"authors\"]: \n",
    "                    ids.add(author[\"authorId\"]) \n",
    "        except: # Usually only occurs if the author has not realeased any papers or is not found \n",
    "            print(f\"The error occured at search number {i}, the name {name} and the response is: \\n {response}\")\n",
    "            nin_names.add(name)\n",
    "        # In either case the name has been evaluated\n",
    "        evaluated_names.add(name) \n",
    "               \n",
    "    # Just to not mess whith the other parts of the code (amount of requests)\n",
    "    if len(names) % 150 > 50: \n",
    "        time.sleep(60*5) \n",
    "    \n",
    "    # Save progress for next time \n",
    "    save_data(ids, file_name=file_name)\n",
    "    save_data(evaluated_names, file_name=file_name + \"_evaluated_names\")\n",
    "    save_data(nin_names, file_name=file_name + \"_nin_names\")\n",
    "            \n",
    "    return ids, nin_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2133 already evaluated of 2133\n",
      "Hence there are 0 left \n",
      "We have found 118159 coauthors of the 2133 from week 1\n",
      "Of the 2133 authors, 181 where not found\n"
     ]
    }
   ],
   "source": [
    "# Get all coauthors to the 2133 authors found in week 1 \n",
    "ids, nin_names = get_ids_and_coauthors(science_people, file_name=\"ids_dict\", load_previous=True, verbose=True) \n",
    "\n",
    "print(f\"We have found {len(ids)} coauthors of the {len(science_people)} from week 1\")\n",
    "print(f\"Of the {len(science_people)} authors, {len(nin_names)} where not found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions and thoughts (these are mostly notes to ourselves)\n",
    "\n",
    "We have now gathered a dataset of more than 100 000 authors, which is quite a lot. The issue that we can only make about 150 requests to semantic scholar every five minutes arrises and hence we need to batch our id searches, as the process will otherwise take to long (suprise it takes very long anyway). \n",
    "\n",
    "This is however not easy as semantic scholar can only handle batches of size 100 when we are also asking for the papers written by each author. \n",
    "\n",
    "The amount of data that we want to gather from sematic scholar is quite large, as a dictionary containing the first 10000 author id's as keys takes up 1.7 GB of data. \n",
    "Furthermore, semantic scholar can often not handle batches of size 100 if there is two much data in the batch, hence the error handeling in the next couple of methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting authors into a dataframe\n",
    "def format_authors(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors with the data; id, name, alias, citationCount, field\n",
    "    \"\"\"\n",
    "    # Create people dataframe \n",
    "    zero_data = np.zeros((len(ids_dict.keys()), 5))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"name\", \"aliases\", \"citationCount\", \"field\"])\n",
    "\n",
    "    for i, id in enumerate(ids_dict.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id \n",
    "        information = ids_dict[str(id)]\n",
    "        # Name\n",
    "        try: # Debugging \n",
    "            df[\"name\"][i] = information[\"name\"]\n",
    "        except:\n",
    "            print(information)\n",
    "            print(df[\"name\"])\n",
    "        # Aliases\n",
    "        df[\"aliases\"][i] = information[\"aliases\"] \n",
    "        # citation count \n",
    "        citation_count = 0\n",
    "        for paper in information[\"papers\"]: \n",
    "            citation_count += paper[\"citationCount\"]\n",
    "        df[\"citationCount\"][i] = citation_count\n",
    "        # field - count each occurence and take the maximum \n",
    "        potential_fields = {}\n",
    "        for paper in information[\"papers\"]: \n",
    "            for fields in paper[\"s2FieldsOfStudy\"]:\n",
    "                field = fields[\"category\"]\n",
    "                try:\n",
    "                    potential_fields[field] += 1\n",
    "                except: \n",
    "                    potential_fields[field] = 1\n",
    "        if potential_fields == {}: \n",
    "            pass\n",
    "        else: \n",
    "            # This is a spicy way to do this o.0\n",
    "            df[\"field\"][i] = max(potential_fields, key=potential_fields.get)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting papers into a dataframe\n",
    "def format_papers(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors's papers with the data; id, title, year, DOI, citationCount, field, authors\n",
    "    \"\"\"\n",
    "    # Start by making a dictionary of papers instead of authors\n",
    "    papers = {}\n",
    "    for id in ids_dict.keys():\n",
    "        # get the papers \n",
    "        information = ids_dict[str(id)]\n",
    "        for paper in information[\"papers\"]:\n",
    "            if paper[\"paperId\"] in papers:\n",
    "                pass\n",
    "            else: \n",
    "                # Find the author id's\n",
    "                authors = set()\n",
    "                for author in paper[\"authors\"]:\n",
    "                    authors.add(author[\"authorId\"])\n",
    "                # Make new elements in the dictionary \n",
    "                papers[paper[\"paperId\"]] = {\"id\": paper[\"paperId\"],\n",
    "                                            \"title\": paper[\"title\"], \n",
    "                                            \"year\": paper[\"year\"], \n",
    "                                            \"doi\": paper[\"externalIds\"],\n",
    "                                            \"citationCount\": paper[\"citationCount\"], \n",
    "                                            \"field\": paper[\"s2FieldsOfStudy\"], \n",
    "                                            \"authors\": authors}\n",
    "    # Create the paper dataframe \n",
    "    zero_data = np.zeros((len(papers.keys()), 7))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"title\", \"year\", \"doi\", \"citationCount\", \"field\", \"authors\"])\n",
    "\n",
    "    for i, id in enumerate(papers.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id\n",
    "        information = papers[str(id)]\n",
    "        # title\n",
    "        df[\"title\"][i] = information[\"title\"]\n",
    "        # Aliases\n",
    "        df[\"year\"][i] = information[\"year\"] \n",
    "        # DOI \n",
    "        df[\"doi\"][i] = [information[\"doi\"]] # Can't have a dict, but it is okay to wrap it with list\n",
    "        # citation count \n",
    "        df[\"citationCount\"][i] = information[\"citationCount\"]\n",
    "        # field \n",
    "        try:\n",
    "            df[\"field\"][i] = information[\"field\"] # Sadly at least one paper does not have a field...\n",
    "        except:\n",
    "            print(f\"Paper {id} does not have a field and it crashes everything!?\")\n",
    "        # authors \n",
    "        df[\"authors\"][i] = list(information[\"authors\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting paper abstracts into a dataframe\n",
    "def format_paper_abstracts(ids_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary with all data of the authors into a simple dataframe\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): a dictionary with author ids and their papers\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of the given authors's papers with the data; id, abstract\n",
    "    \"\"\"\n",
    "    # Start by making a dictionary of papers instead of authors\n",
    "    papers = {}\n",
    "    for id in ids_dict.keys():\n",
    "        # get the papers \n",
    "        information = ids_dict[str(id)]\n",
    "        for paper in information[\"papers\"]:\n",
    "            if paper[\"paperId\"] in papers:\n",
    "                pass\n",
    "            else:\n",
    "                # Make new elements in the dictionary \n",
    "                papers[paper[\"paperId\"]] = {\"id\": paper[\"paperId\"],\n",
    "                                            \"abstract\": paper[\"abstract\"]}\n",
    "    # Create the paper dataframe \n",
    "    zero_data = np.zeros((len(papers.keys()), 2))\n",
    "    zero_data[:] = np.nan\n",
    "    df = pd.DataFrame(zero_data, columns=[\"id\", \"abstract\"])\n",
    "\n",
    "    for i, id in enumerate(papers.keys()):\n",
    "        # ID\n",
    "        df[\"id\"][i] = id\n",
    "        information = papers[str(id)]\n",
    "        # abstract\n",
    "        df[\"abstract\"][i] = information[\"abstract\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that formats a dictionary into dataframes and stores them\n",
    "def create_dataframes(ids_dict, prefix=\"\", verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary of authors and their papers and generates two datasets and stores these\n",
    "\n",
    "    Args:\n",
    "        ids_dict (dict): The key is an auther id, the contents is a dictionary with three atributes, \"name\", \"aliases\" and \"papers\".\n",
    "        prefix (str): If the function will be called multiple times, this is to not overwrite previous stored files\n",
    "    \n",
    "    Return: \n",
    "        df_author: The above specified dataframe for authors\n",
    "        df_paper: The above specified dataframe for papers\n",
    "    \"\"\"\n",
    "    # Create dataframes\n",
    "    # Authors \n",
    "    df_author = format_authors(ids_dict)\n",
    "    if verbose: print(\"Formatted author dataframe\")\n",
    "    pd.DataFrame.to_csv(df_author, f\"df_author{prefix}.csv\")\n",
    "    if verbose: print(\"Saved author dataframe\")\n",
    "    \n",
    "    # Papers\n",
    "    df_paper = format_papers(ids_dict)\n",
    "    if verbose: print(\"Formatted paper dataframe\")\n",
    "    pd.DataFrame.to_csv(df_paper, f\"df_paper{prefix}.csv\")\n",
    "    if verbose: print(\"Saved paper dataframe\")\n",
    "    \n",
    "    # Paper abstracts \n",
    "    df_paper_abstract = format_paper_abstracts(ids_dict)\n",
    "    if verbose: print(\"Formatted paper dataframe\")\n",
    "    pd.DataFrame.to_csv(df_paper_abstract, f\"df_paper_abstract{prefix}.csv\")\n",
    "    if verbose: print(\"Saved paper dataframe\")\n",
    "    \n",
    "    return df_author, df_paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that finds all papers from each author\n",
    "def get_data_from_ids(ids, verbose=False, load_previous=True, file_name=\"ids_enumerated_dict\"): \n",
    "    \"\"\"\n",
    "    Returns a complete data frame of \n",
    "    authors (id, name, alias, citationCount, field) and\n",
    "    papers (id, title, year, DOI, citationCount, field, authors)\n",
    "\n",
    "    Args:\n",
    "        ids (set): ids of the authors in question\n",
    "\n",
    "    Returns:\n",
    "        file_extension (int): The number of dataframes created\n",
    "        nin_ids (set): The set of ids that could not be processed  \n",
    "    \"\"\"\n",
    "    ids_dict = {} # This will continually be reset, otherwise it would take up to much space\n",
    "    evaluated_ids = set()\n",
    "    nin_ids = set()\n",
    "    ids_dict[\"file_extension\"] = 0\n",
    "    \n",
    "    # Check if the problem has been worked on previously \n",
    "    if load_previous:\n",
    "        try: \n",
    "            ids_dict = load_data(file_name=file_name)\n",
    "            evaluated_ids = load_data(file_name=file_name + \"_evaluated_ids\")\n",
    "            nin_ids = load_data(file_name=file_name + \"_nin_ids\")\n",
    "            if verbose: print(f\"{len(evaluated_ids)} already evaluated of {len(ids)}\") \n",
    "            ids = ids - evaluated_ids\n",
    "            if verbose: print(f\"Hence there are {len(ids)} left \")\n",
    "        except:\n",
    "            print(f\"There are no previous progress made\")\n",
    "    \n",
    "    # Partition the ids into batches of 20 ids, because semantic scholar can only take that many \n",
    "    ids = list(ids) # temporary to get results\n",
    "    default_batch_size = 64\n",
    "    batch_size = default_batch_size\n",
    "    n_batches = len(ids) // batch_size + 1\n",
    "    batches_left = True \n",
    "    sent_requests = 0\n",
    "    index = 0 \n",
    "    if verbose: print(f\"Total number of batches are {n_batches}\")\n",
    "        \n",
    "    #  Current file name extension\n",
    "    file_extension = ids_dict[\"file_extension\"]\n",
    "    file_start = -len(ids_dict)\n",
    "    file_size = 6000 # Hope this is small enough \n",
    "    \n",
    "    # Use a while loop to go through each batch of ids so that we can change sizes dynamically\n",
    "    # (This stems from the fact that semantic scholar will return errors if we ask for too much data)\n",
    "    while(batches_left):\n",
    "        # To avoid memory overflow convert to pandas \n",
    "        if index > file_start + file_size: \n",
    "            # Note that the return dataframes from create_dataframes are not used, as we do not have memory enough to keep them\n",
    "            # (they are only stored to physical memory)\n",
    "            del(ids_dict[\"file_extension\"])\n",
    "            create_dataframes(ids_dict=ids_dict, prefix=file_extension, verbose=verbose)\n",
    "            file_start += file_size\n",
    "            file_extension += 1\n",
    "            ids_dict = {}\n",
    "            ids_dict[\"file_extension\"] = file_extension\n",
    "        \n",
    "        # Make request for batch \n",
    "        batch_url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "        data = {\"ids\": ids[index:min(index + batch_size, len(ids))]}\n",
    "        params = {\"fields\": \"aliases,papers.title,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount,papers.abstract,name,papers.authors\"}\n",
    "        response = requests.post(batch_url, json=data, params=params).json()\n",
    "        sent_requests += 1\n",
    "        \n",
    "        # Assert the response\n",
    "        if response == {'message': 'Internal server error'}:\n",
    "            if verbose: print(f\"Server error for index {index} with batch size {batch_size}\")\n",
    "            batch_size = batch_size // 2 # Half batch size and try again \n",
    "            \n",
    "            if batch_size == 0: \n",
    "                # Save the faulty element that makes semantic scholar give internal errors \n",
    "                # and proceed to the next one\n",
    "                if verbose: print(f\"Had to remove {index} which is {ids[index]}\")\n",
    "                batch_size = 1 \n",
    "                evaluated_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "                nin_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "                index += batch_size \n",
    "        else: \n",
    "            # If there is something wrong with the request\n",
    "            try: \n",
    "                for person in response: \n",
    "                    # Update dictionary \n",
    "                    # If something goes wrong, it will be reported here \n",
    "                    try: \n",
    "                        ids_dict[person[\"authorId\"]] = {\"name\": person[\"name\"], \n",
    "                                \"aliases\": person[\"aliases\"],\n",
    "                                \"papers\": person[\"papers\"]}\n",
    "                    except: # Usually only occurs if the author has not realeased any papers or is not found \n",
    "                        print(\"Something is wrong with this person (usually it is a None value somehow?)\")\n",
    "                        print(f\"The index is {index} with batch size {batch_size}\")\n",
    "            except: \n",
    "                # If it messes up print the request and put the ids in nin \n",
    "                if verbose:  \n",
    "                    print(response)\n",
    "                    print(f\"The index is {index} with batch size {batch_size}\")\n",
    "                nin_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "            \n",
    "            # Update processed ids \n",
    "            evaluated_ids.update(set(ids[index:min(index + batch_size, len(ids))]))\n",
    "            index += batch_size\n",
    "            batch_size = default_batch_size\n",
    "        \n",
    "        # If there has been too many request we need to break - we can probably skip this now because requests take so long \n",
    "        if sent_requests % 150 == 149: \n",
    "            start_time = time.time()\n",
    "            # printing \n",
    "            if verbose: \n",
    "                print(f\"Completed searches for {index} out of {len(ids)}, but reached limit\")\n",
    "            # Save prograss \n",
    "            save_data(ids_dict, file_name=file_name)\n",
    "            save_data(evaluated_ids, file_name=file_name + \"_evaluated_ids\")\n",
    "            save_data(nin_ids, file_name=file_name + \"_nin_ids\")\n",
    "            time.sleep(max(60*5+10 + start_time - time.time(), 0)) # the +10 is a buffer\n",
    "        \n",
    "        # Check if the loop is complete \n",
    "        if index >= len(ids): \n",
    "            batches_left = False \n",
    "    \n",
    "    # Create and save the final dataframes \n",
    "    if len(ids_dict) > 1: \n",
    "        create_dataframes(ids_dict=ids_dict, prefix=file_extension, verbose=verbose)\n",
    "    \n",
    "    return file_extension, nin_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764 already evaluated of 118159\n",
      "Hence there are 116395 left \n",
      "Total number of batches are 1819\n",
      "Server error for index 0 with batch size 64\n",
      "Server error for index 0 with batch size 32\n",
      "Server error for index 0 with batch size 16\n",
      "Server error for index 8 with batch size 64\n",
      "Server error for index 40 with batch size 64\n",
      "Server error for index 72 with batch size 64\n",
      "Server error for index 168 with batch size 64\n",
      "Server error for index 168 with batch size 32\n",
      "Server error for index 184 with batch size 64\n",
      "Server error for index 184 with batch size 32\n",
      "Server error for index 184 with batch size 16\n",
      "Server error for index 192 with batch size 64\n",
      "Server error for index 192 with batch size 32\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 192 with batch size 16\n",
      "Server error for index 208 with batch size 64\n",
      "Server error for index 240 with batch size 64\n",
      "Server error for index 240 with batch size 32\n",
      "Server error for index 256 with batch size 64\n",
      "Server error for index 256 with batch size 32\n",
      "Server error for index 272 with batch size 64\n",
      "Server error for index 272 with batch size 32\n",
      "Server error for index 288 with batch size 64\n",
      "Server error for index 320 with batch size 64\n",
      "Server error for index 320 with batch size 32\n",
      "Server error for index 336 with batch size 64\n",
      "Server error for index 336 with batch size 32\n",
      "Server error for index 336 with batch size 16\n",
      "Server error for index 336 with batch size 8\n",
      "Server error for index 340 with batch size 64\n",
      "Server error for index 340 with batch size 32\n",
      "Server error for index 340 with batch size 16\n",
      "Server error for index 340 with batch size 8\n",
      "Server error for index 340 with batch size 4\n",
      "Server error for index 342 with batch size 64\n",
      "Server error for index 342 with batch size 32\n",
      "Server error for index 342 with batch size 16\n",
      "Server error for index 342 with batch size 8\n",
      "Server error for index 342 with batch size 4\n",
      "Server error for index 342 with batch size 2\n",
      "Server error for index 342 with batch size 1\n",
      "Had to remove 342 which is 9945617\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 344 with batch size 64\n",
      "Server error for index 408 with batch size 64\n",
      "Server error for index 440 with batch size 64\n",
      "Server error for index 440 with batch size 32\n",
      "Server error for index 456 with batch size 64\n",
      "Server error for index 456 with batch size 32\n",
      "Server error for index 456 with batch size 16\n",
      "Server error for index 464 with batch size 64\n",
      "Server error for index 464 with batch size 32\n",
      "Server error for index 464 with batch size 16\n",
      "Server error for index 464 with batch size 8\n",
      "Server error for index 468 with batch size 64\n",
      "Server error for index 468 with batch size 32\n",
      "Server error for index 468 with batch size 16\n",
      "Server error for index 468 with batch size 8\n",
      "Server error for index 468 with batch size 4\n",
      "Server error for index 470 with batch size 64\n",
      "Server error for index 470 with batch size 32\n",
      "Server error for index 470 with batch size 16\n",
      "Server error for index 470 with batch size 8\n",
      "Server error for index 470 with batch size 4\n",
      "Server error for index 470 with batch size 2\n",
      "Server error for index 470 with batch size 1\n",
      "Had to remove 470 which is 3571219\n",
      "Server error for index 472 with batch size 64\n",
      "Server error for index 472 with batch size 32\n",
      "Server error for index 472 with batch size 16\n",
      "Server error for index 480 with batch size 64\n",
      "Server error for index 480 with batch size 32\n",
      "Server error for index 480 with batch size 16\n",
      "Server error for index 480 with batch size 8\n",
      "Server error for index 480 with batch size 4\n",
      "Server error for index 480 with batch size 2\n",
      "Server error for index 481 with batch size 64\n",
      "Server error for index 481 with batch size 32\n",
      "Server error for index 481 with batch size 16\n",
      "Server error for index 481 with batch size 8\n",
      "Server error for index 481 with batch size 4\n",
      "Server error for index 481 with batch size 2\n",
      "Server error for index 481 with batch size 1\n",
      "Had to remove 481 which is 47955120\n",
      "Server error for index 547 with batch size 64\n",
      "Server error for index 547 with batch size 32\n",
      "Server error for index 547 with batch size 16\n",
      "Server error for index 547 with batch size 8\n",
      "Server error for index 547 with batch size 4\n",
      "Server error for index 547 with batch size 2\n",
      "Server error for index 547 with batch size 1\n",
      "Had to remove 547 which is 50666209\n",
      "Server error for index 613 with batch size 64\n",
      "Server error for index 645 with batch size 64\n",
      "Server error for index 645 with batch size 32\n",
      "Server error for index 661 with batch size 64\n",
      "Server error for index 693 with batch size 64\n",
      "Server error for index 725 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 757 with batch size 64\n",
      "Server error for index 821 with batch size 64\n",
      "Server error for index 853 with batch size 64\n",
      "Server error for index 885 with batch size 64\n",
      "Server error for index 917 with batch size 64\n",
      "Server error for index 949 with batch size 64\n",
      "Server error for index 981 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 1013 with batch size 64\n",
      "Server error for index 1077 with batch size 64\n",
      "Server error for index 1109 with batch size 64\n",
      "Server error for index 1109 with batch size 32\n",
      "Server error for index 1125 with batch size 64\n",
      "Server error for index 1125 with batch size 32\n",
      "Server error for index 1125 with batch size 16\n",
      "Server error for index 1133 with batch size 64\n",
      "Completed searches for 1133 out of 116395, but reached limit\n",
      "Server error for index 1133 with batch size 32\n",
      "Server error for index 1133 with batch size 16\n",
      "Server error for index 1133 with batch size 8\n",
      "Server error for index 1137 with batch size 64\n",
      "Server error for index 1137 with batch size 32\n",
      "Server error for index 1137 with batch size 16\n",
      "Server error for index 1137 with batch size 8\n",
      "Server error for index 1137 with batch size 4\n",
      "Server error for index 1139 with batch size 64\n",
      "Server error for index 1139 with batch size 32\n",
      "Server error for index 1139 with batch size 16\n",
      "Server error for index 1139 with batch size 8\n",
      "Server error for index 1139 with batch size 4\n",
      "Server error for index 1139 with batch size 2\n",
      "Server error for index 1139 with batch size 1\n",
      "Had to remove 1139 which is 1390520869\n",
      "Server error for index 1141 with batch size 64\n",
      "Server error for index 1173 with batch size 64\n",
      "Server error for index 1205 with batch size 64\n",
      "Server error for index 1237 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 1269 with batch size 64\n",
      "Server error for index 1333 with batch size 64\n",
      "Server error for index 1333 with batch size 32\n",
      "Server error for index 1349 with batch size 64\n",
      "Server error for index 1349 with batch size 32\n",
      "Server error for index 1349 with batch size 16\n",
      "Server error for index 1357 with batch size 64\n",
      "Server error for index 1357 with batch size 32\n",
      "Server error for index 1357 with batch size 16\n",
      "Server error for index 1357 with batch size 8\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 1361 with batch size 64\n",
      "Server error for index 1425 with batch size 64\n",
      "Server error for index 1425 with batch size 32\n",
      "Server error for index 1441 with batch size 64\n",
      "Server error for index 1441 with batch size 32\n",
      "Server error for index 1441 with batch size 16\n",
      "Server error for index 1449 with batch size 64\n",
      "Server error for index 1449 with batch size 32\n",
      "Server error for index 1449 with batch size 16\n",
      "Server error for index 1449 with batch size 8\n",
      "Server error for index 1449 with batch size 4\n",
      "Server error for index 1449 with batch size 2\n",
      "Server error for index 1450 with batch size 64\n",
      "Server error for index 1450 with batch size 32\n",
      "Server error for index 1450 with batch size 16\n",
      "Server error for index 1450 with batch size 8\n",
      "Server error for index 1450 with batch size 4\n",
      "Server error for index 1450 with batch size 2\n",
      "Server error for index 1450 with batch size 1\n",
      "Had to remove 1450 which is 120117447\n",
      "Server error for index 1644 with batch size 64\n",
      "Server error for index 1740 with batch size 64\n",
      "Server error for index 1772 with batch size 64\n",
      "Server error for index 1772 with batch size 32\n",
      "Server error for index 1788 with batch size 64\n",
      "Server error for index 1788 with batch size 32\n",
      "Server error for index 1788 with batch size 16\n",
      "Server error for index 1796 with batch size 64\n",
      "Server error for index 1796 with batch size 32\n",
      "Server error for index 1796 with batch size 16\n",
      "Server error for index 1796 with batch size 8\n",
      "Server error for index 1796 with batch size 4\n",
      "Server error for index 1798 with batch size 64\n",
      "Server error for index 1798 with batch size 32\n",
      "Server error for index 1798 with batch size 16\n",
      "Server error for index 1798 with batch size 8\n",
      "Server error for index 1798 with batch size 4\n",
      "Server error for index 1798 with batch size 2\n",
      "Server error for index 1799 with batch size 64\n",
      "Server error for index 1799 with batch size 32\n",
      "Server error for index 1799 with batch size 16\n",
      "Server error for index 1799 with batch size 8\n",
      "Server error for index 1799 with batch size 4\n",
      "Server error for index 1799 with batch size 2\n",
      "Server error for index 1799 with batch size 1\n",
      "Had to remove 1799 which is 95231958\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 1865 with batch size 64\n",
      "Server error for index 1929 with batch size 64\n",
      "Server error for index 1961 with batch size 64\n",
      "Server error for index 1961 with batch size 32\n",
      "Server error for index 1961 with batch size 16\n",
      "Server error for index 1961 with batch size 8\n",
      "Server error for index 1961 with batch size 4\n",
      "Server error for index 1961 with batch size 2\n",
      "Server error for index 1962 with batch size 64\n",
      "Server error for index 1962 with batch size 32\n",
      "Server error for index 1962 with batch size 16\n",
      "Server error for index 1962 with batch size 8\n",
      "Server error for index 1962 with batch size 4\n",
      "Server error for index 1962 with batch size 2\n",
      "Server error for index 1962 with batch size 1\n",
      "Had to remove 1962 which is 144438751\n",
      "Server error for index 2092 with batch size 64\n",
      "Server error for index 2124 with batch size 64\n",
      "Server error for index 2124 with batch size 32\n",
      "Server error for index 2124 with batch size 16\n",
      "Server error for index 2132 with batch size 64\n",
      "Server error for index 2132 with batch size 32\n",
      "Server error for index 2148 with batch size 64\n",
      "Server error for index 2148 with batch size 32\n",
      "Server error for index 2148 with batch size 16\n",
      "Server error for index 2156 with batch size 64\n",
      "Server error for index 2156 with batch size 32\n",
      "Server error for index 2156 with batch size 16\n",
      "Server error for index 2156 with batch size 8\n",
      "Server error for index 2160 with batch size 64\n",
      "Server error for index 2160 with batch size 32\n",
      "Server error for index 2160 with batch size 16\n",
      "Server error for index 2160 with batch size 8\n",
      "Server error for index 2160 with batch size 4\n",
      "Server error for index 2162 with batch size 64\n",
      "Server error for index 2162 with batch size 32\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 2162 with batch size 16\n",
      "Server error for index 2178 with batch size 64\n",
      "Server error for index 2178 with batch size 32\n",
      "Server error for index 2178 with batch size 16\n",
      "Completed searches for 2186 out of 116395, but reached limit\n",
      "Server error for index 2186 with batch size 64\n",
      "Server error for index 2186 with batch size 32\n",
      "Server error for index 2186 with batch size 16\n",
      "Server error for index 2186 with batch size 8\n",
      "Server error for index 2190 with batch size 64\n",
      "Server error for index 2190 with batch size 32\n",
      "Server error for index 2190 with batch size 16\n",
      "Server error for index 2198 with batch size 64\n",
      "Server error for index 2230 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 2262 with batch size 64\n",
      "Server error for index 2390 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 2422 with batch size 64\n",
      "Server error for index 2550 with batch size 64\n",
      "Server error for index 2582 with batch size 64\n",
      "Server error for index 2582 with batch size 32\n",
      "Server error for index 2662 with batch size 64\n",
      "Server error for index 2694 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 2694 with batch size 32\n",
      "Server error for index 2854 with batch size 64\n",
      "Server error for index 2886 with batch size 64\n",
      "Server error for index 2918 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 2950 with batch size 64\n",
      "Server error for index 3014 with batch size 64\n",
      "Server error for index 3046 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 3078 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 3142 with batch size 64\n",
      "Server error for index 3206 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 3238 with batch size 64\n",
      "Server error for index 3686 with batch size 64\n",
      "Server error for index 3718 with batch size 64\n",
      "Server error for index 3750 with batch size 64\n",
      "Server error for index 3750 with batch size 32\n",
      "Server error for index 3750 with batch size 16\n",
      "Server error for index 3750 with batch size 8\n",
      "Server error for index 3754 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 3850 with batch size 64\n",
      "Server error for index 3978 with batch size 64\n",
      "Server error for index 4074 with batch size 64\n",
      "Server error for index 4106 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 4138 with batch size 64\n",
      "Server error for index 4202 with batch size 64\n",
      "Server error for index 4202 with batch size 32\n",
      "Server error for index 4202 with batch size 16\n",
      "Server error for index 4210 with batch size 64\n",
      "Server error for index 4210 with batch size 32\n",
      "Server error for index 4210 with batch size 16\n",
      "Server error for index 4282 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 4314 with batch size 64\n",
      "Server error for index 4378 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 4410 with batch size 64\n",
      "Server error for index 4538 with batch size 64\n",
      "Server error for index 4538 with batch size 32\n",
      "Server error for index 4554 with batch size 64\n",
      "Server error for index 4586 with batch size 64\n",
      "Server error for index 4586 with batch size 32\n",
      "Server error for index 4586 with batch size 16\n",
      "Server error for index 4594 with batch size 64\n",
      "Server error for index 4594 with batch size 32\n",
      "Server error for index 4594 with batch size 16\n",
      "Server error for index 4594 with batch size 8\n",
      "Server error for index 4594 with batch size 4\n",
      "Server error for index 4596 with batch size 64\n",
      "Server error for index 4596 with batch size 32\n",
      "Server error for index 4596 with batch size 16\n",
      "Server error for index 4596 with batch size 8\n",
      "Server error for index 4596 with batch size 4\n",
      "Server error for index 4596 with batch size 2\n",
      "Server error for index 4596 with batch size 1\n",
      "Had to remove 4596 which is 1381445236\n",
      "Server error for index 4598 with batch size 64\n",
      "Server error for index 4630 with batch size 64\n",
      "Server error for index 4630 with batch size 32\n",
      "Server error for index 4646 with batch size 64\n",
      "Server error for index 4646 with batch size 32\n",
      "Server error for index 4646 with batch size 16\n",
      "Server error for index 4654 with batch size 64\n",
      "Server error for index 4654 with batch size 32\n",
      "Server error for index 4654 with batch size 16\n",
      "Server error for index 4654 with batch size 8\n",
      "Server error for index 4658 with batch size 64\n",
      "Server error for index 4658 with batch size 32\n",
      "Server error for index 4658 with batch size 16\n",
      "Server error for index 4658 with batch size 8\n",
      "Server error for index 4658 with batch size 4\n",
      "Server error for index 4658 with batch size 2\n",
      "Server error for index 4658 with batch size 1\n",
      "Had to remove 4658 which is 3091212\n",
      "Server error for index 4724 with batch size 64\n",
      "Server error for index 4756 with batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/3264884921.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"aliases\"][i] = information[\"aliases\"]\n",
      "c:\\Users\\david\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/3264884921.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"citationCount\"][i] = citation_count\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/3264884921.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"field\"][i] = max(potential_fields, key=potential_fields.get)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/3264884921.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"id\"][i] = id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted author dataframe\n",
      "Saved author dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"title\"][i] = information[\"title\"]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"year\"][i] = information[\"year\"]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"citationCount\"][i] = information[\"citationCount\"]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"field\"][i] = information[\"field\"] # Sadly at least one paper does not have a field...\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/47228800.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"authors\"][i] = list(information[\"authors\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted paper dataframe\n",
      "Saved paper dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_5680/2284731925.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"abstract\"][i] = information[\"abstract\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted paper dataframe\n",
      "Saved paper dataframe\n",
      "Server error for index 4788 with batch size 64\n",
      "Server error for index 4820 with batch size 64\n",
      "Server error for index 4852 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 4948 with batch size 64\n",
      "Server error for index 5012 with batch size 64\n",
      "Completed searches for 5012 out of 116395, but reached limit\n",
      "Server error for index 5044 with batch size 64\n",
      "Server error for index 5140 with batch size 64\n",
      "Server error for index 5140 with batch size 32\n",
      "Server error for index 5156 with batch size 64\n",
      "Server error for index 5156 with batch size 32\n",
      "Server error for index 5236 with batch size 64\n",
      "Server error for index 5236 with batch size 32\n",
      "Server error for index 5252 with batch size 64\n",
      "Server error for index 5252 with batch size 32\n",
      "Server error for index 5332 with batch size 64\n",
      "Server error for index 5364 with batch size 64\n",
      "Server error for index 5396 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 5428 with batch size 64\n",
      "Server error for index 5492 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 5524 with batch size 64\n",
      "Server error for index 5716 with batch size 64\n",
      "Server error for index 5716 with batch size 32\n",
      "Server error for index 5732 with batch size 64\n",
      "Server error for index 5892 with batch size 64\n",
      "Server error for index 5924 with batch size 64\n",
      "Server error for index 5956 with batch size 64\n",
      "Server error for index 5988 with batch size 64\n",
      "Server error for index 6020 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 6052 with batch size 64\n",
      "Server error for index 6180 with batch size 64\n",
      "Server error for index 6212 with batch size 64\n",
      "Server error for index 6212 with batch size 32\n",
      "Server error for index 6292 with batch size 64\n",
      "Server error for index 6324 with batch size 64\n",
      "Server error for index 6356 with batch size 64\n",
      "Server error for index 6388 with batch size 64\n",
      "Server error for index 6548 with batch size 64\n",
      "Server error for index 6548 with batch size 32\n",
      "Server error for index 6564 with batch size 64\n",
      "Server error for index 6564 with batch size 32\n",
      "Server error for index 6564 with batch size 16\n",
      "Server error for index 6564 with batch size 8\n",
      "Server error for index 6564 with batch size 4\n",
      "Server error for index 6564 with batch size 2\n",
      "Server error for index 6565 with batch size 64\n",
      "Server error for index 6565 with batch size 32\n",
      "Server error for index 6565 with batch size 16\n",
      "Server error for index 6565 with batch size 8\n",
      "Server error for index 6565 with batch size 4\n",
      "Server error for index 6565 with batch size 2\n",
      "Server error for index 6565 with batch size 1\n",
      "Had to remove 6565 which is 10188077\n",
      "Server error for index 6567 with batch size 64\n",
      "Server error for index 6599 with batch size 64\n",
      "Server error for index 6631 with batch size 64\n",
      "Server error for index 6631 with batch size 32\n",
      "Server error for index 6631 with batch size 16\n",
      "Server error for index 6631 with batch size 8\n",
      "Server error for index 6635 with batch size 64\n",
      "Server error for index 6635 with batch size 32\n",
      "Server error for index 6635 with batch size 16\n",
      "Server error for index 6635 with batch size 8\n",
      "Server error for index 6635 with batch size 4\n",
      "Server error for index 6637 with batch size 64\n",
      "Server error for index 6637 with batch size 32\n",
      "Server error for index 6637 with batch size 16\n",
      "Server error for index 6637 with batch size 8\n",
      "Server error for index 6637 with batch size 4\n",
      "Server error for index 6637 with batch size 2\n",
      "Server error for index 6638 with batch size 64\n",
      "Server error for index 6638 with batch size 32\n",
      "Server error for index 6638 with batch size 16\n",
      "Server error for index 6638 with batch size 8\n",
      "Server error for index 6638 with batch size 4\n",
      "Server error for index 6638 with batch size 2\n",
      "Server error for index 6638 with batch size 1\n",
      "Had to remove 6638 which is 144537450\n",
      "Server error for index 6704 with batch size 64\n",
      "Server error for index 6736 with batch size 64\n",
      "Server error for index 6896 with batch size 64\n",
      "Server error for index 6928 with batch size 64\n",
      "Server error for index 6960 with batch size 64\n",
      "Server error for index 6992 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 7024 with batch size 64\n",
      "Server error for index 7088 with batch size 64\n",
      "Server error for index 7184 with batch size 64\n",
      "Server error for index 7216 with batch size 64\n",
      "Server error for index 7248 with batch size 64\n",
      "Server error for index 7280 with batch size 64\n",
      "Server error for index 7280 with batch size 32\n",
      "Server error for index 7280 with batch size 16\n",
      "Server error for index 7288 with batch size 64\n",
      "Server error for index 7288 with batch size 32\n",
      "Server error for index 7288 with batch size 16\n",
      "Server error for index 7288 with batch size 8\n",
      "Server error for index 7292 with batch size 64\n",
      "Server error for index 7292 with batch size 32\n",
      "Server error for index 7292 with batch size 16\n",
      "Completed searches for 7292 out of 116395, but reached limit\n",
      "Server error for index 7292 with batch size 8\n",
      "Server error for index 7292 with batch size 4\n",
      "Server error for index 7292 with batch size 2\n",
      "Server error for index 7293 with batch size 64\n",
      "Server error for index 7293 with batch size 32\n",
      "Server error for index 7293 with batch size 16\n",
      "Server error for index 7293 with batch size 8\n",
      "Server error for index 7293 with batch size 4\n",
      "Server error for index 7293 with batch size 2\n",
      "Server error for index 7293 with batch size 1\n",
      "Had to remove 7293 which is 152228926\n",
      "Server error for index 7295 with batch size 64\n",
      "Server error for index 7327 with batch size 64\n",
      "Server error for index 7327 with batch size 32\n",
      "Server error for index 7343 with batch size 64\n",
      "Server error for index 7375 with batch size 64\n",
      "Server error for index 7407 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 7439 with batch size 64\n",
      "Server error for index 7503 with batch size 64\n",
      "Server error for index 7503 with batch size 32\n",
      "Server error for index 7519 with batch size 64\n",
      "Server error for index 7519 with batch size 32\n",
      "Server error for index 7519 with batch size 16\n",
      "Server error for index 7519 with batch size 8\n",
      "Server error for index 7523 with batch size 64\n",
      "Server error for index 7523 with batch size 32\n",
      "Server error for index 7523 with batch size 16\n",
      "Server error for index 7523 with batch size 8\n",
      "Server error for index 7523 with batch size 4\n",
      "Server error for index 7525 with batch size 64\n",
      "Server error for index 7525 with batch size 32\n",
      "Server error for index 7525 with batch size 16\n",
      "Server error for index 7525 with batch size 8\n",
      "Server error for index 7525 with batch size 4\n",
      "Server error for index 7525 with batch size 2\n",
      "Server error for index 7526 with batch size 64\n",
      "Server error for index 7526 with batch size 32\n",
      "Server error for index 7526 with batch size 16\n",
      "Server error for index 7526 with batch size 8\n",
      "Server error for index 7526 with batch size 4\n",
      "Server error for index 7526 with batch size 2\n",
      "Server error for index 7526 with batch size 1\n",
      "Had to remove 7526 which is 123553428\n",
      "Server error for index 7592 with batch size 64\n",
      "Server error for index 7592 with batch size 32\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 7608 with batch size 64\n",
      "Server error for index 7672 with batch size 64\n",
      "Server error for index 7768 with batch size 64\n",
      "Server error for index 7800 with batch size 64\n",
      "Server error for index 7800 with batch size 32\n",
      "Server error for index 7800 with batch size 16\n",
      "Server error for index 7808 with batch size 64\n",
      "Server error for index 7808 with batch size 32\n",
      "Server error for index 7808 with batch size 16\n",
      "Server error for index 7808 with batch size 8\n",
      "Server error for index 7808 with batch size 4\n",
      "Server error for index 7810 with batch size 64\n",
      "Server error for index 7810 with batch size 32\n",
      "Server error for index 7810 with batch size 16\n",
      "Server error for index 7810 with batch size 8\n",
      "Server error for index 7810 with batch size 4\n",
      "Server error for index 7810 with batch size 2\n",
      "Server error for index 7810 with batch size 1\n",
      "Had to remove 7810 which is 1381110904\n",
      "Server error for index 7812 with batch size 64\n",
      "Server error for index 7812 with batch size 32\n",
      "Server error for index 7828 with batch size 64\n",
      "Server error for index 7924 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 8020 with batch size 64\n",
      "Server error for index 8148 with batch size 64\n",
      "Server error for index 8244 with batch size 64\n",
      "Server error for index 8276 with batch size 64\n",
      "Server error for index 8308 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 8340 with batch size 64\n",
      "Server error for index 8404 with batch size 64\n",
      "Server error for index 8436 with batch size 64\n",
      "Server error for index 8596 with batch size 64\n",
      "Server error for index 8628 with batch size 64\n",
      "Server error for index 8660 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 8756 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 8820 with batch size 64\n",
      "Server error for index 8884 with batch size 64\n",
      "Server error for index 8884 with batch size 32\n",
      "Server error for index 8884 with batch size 16\n",
      "Server error for index 8884 with batch size 8\n",
      "Server error for index 8888 with batch size 64\n",
      "Server error for index 8888 with batch size 32\n",
      "Server error for index 8888 with batch size 16\n",
      "Server error for index 8888 with batch size 8\n",
      "Server error for index 8888 with batch size 4\n",
      "Server error for index 8890 with batch size 64\n",
      "Server error for index 8890 with batch size 32\n",
      "Server error for index 8890 with batch size 16\n",
      "Server error for index 8890 with batch size 8\n",
      "Server error for index 8890 with batch size 4\n",
      "Server error for index 8890 with batch size 2\n",
      "Server error for index 8890 with batch size 1\n",
      "Had to remove 8890 which is 46711589\n",
      "Server error for index 9084 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 9116 with batch size 64\n",
      "Server error for index 9244 with batch size 64\n",
      "Server error for index 9340 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 9372 with batch size 64\n",
      "Completed searches for 9500 out of 116395, but reached limit\n",
      "Server error for index 9500 with batch size 64\n",
      "Server error for index 9532 with batch size 64\n",
      "Server error for index 9564 with batch size 64\n",
      "Server error for index 9596 with batch size 64\n",
      "Server error for index 9628 with batch size 64\n",
      "Server error for index 9660 with batch size 64\n",
      "Server error for index 9692 with batch size 64\n",
      "Server error for index 9852 with batch size 64\n",
      "Server error for index 9948 with batch size 64\n",
      "Server error for index 9980 with batch size 64\n",
      "Server error for index 10012 with batch size 64\n",
      "Server error for index 10044 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 10204 with batch size 64\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 10268 with batch size 64\n",
      "Server error for index 10332 with batch size 64\n",
      "Server error for index 10364 with batch size 64\n",
      "Server error for index 10396 with batch size 64\n",
      "Server error for index 10396 with batch size 32\n",
      "Server error for index 10412 with batch size 64\n",
      "Server error for index 10412 with batch size 32\n",
      "Server error for index 10412 with batch size 16\n",
      "Server error for index 10420 with batch size 64\n",
      "Server error for index 10420 with batch size 32\n",
      "Server error for index 10420 with batch size 16\n",
      "Server error for index 10420 with batch size 8\n",
      "Server error for index 10424 with batch size 64\n",
      "Server error for index 10424 with batch size 32\n",
      "Server error for index 10424 with batch size 16\n",
      "Server error for index 10424 with batch size 8\n",
      "Server error for index 10424 with batch size 4\n",
      "Server error for index 10424 with batch size 2\n",
      "Server error for index 10424 with batch size 1\n",
      "Had to remove 10424 which is 145108677\n",
      "Something is wrong with this person (usually it is a None value somehow?)\n",
      "The index is 10746 with batch size 64\n",
      "Formatted author dataframe\n",
      "Saved author dataframe\n"
     ]
    }
   ],
   "source": [
    "# Generate all dataframes \n",
    "n_files, nin_ids = get_data_from_ids(ids, load_previous=True, verbose=True)\n",
    "\n",
    "print(f\"Of the {len(ids)} authors, {len(nin_ids)} could not be found in the dataset\")\n",
    "print(f\"There are {n_files} dataframes of authors, papers and paper abstracts that needs to be merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to merge dataframes \n",
    "def load_and_merge(id, number, front=\"df_\", end=\".csv\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        id (str): auther, paper, paper_abstract\n",
    "        number (int): number of files \n",
    "        front (str, optional):  Defaults to \"df_\".\n",
    "        end (str, optional):  Defaults to \".csv\".\n",
    "    \"\"\"\n",
    "    # Load the first data file\n",
    "    final_dataframe = pd.read_csv(front + id + str(0) + end)\n",
    "    # Merge with the rest \n",
    "    for i in range(1,number):\n",
    "        new_df = pd.read_csv(front + id + str(i) + end)\n",
    "        final_dataframe = pd.concat([final_dataframe, new_df])\n",
    "    \n",
    "    final_dataframe.drop_duplicates()\n",
    "    \n",
    "    # Save dataframe \n",
    "    pd.DataFrame.to_csv(final_dataframe, f\"{front}{id}{end}\")\n",
    "    return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (77861, 6) elements in author\n",
      "There are (77861, 6) elements in paper_abstract\n",
      "There are (77861, 6) elements in paper\n"
     ]
    }
   ],
   "source": [
    "# Merge dataframes \n",
    "types = [\"author\", \"paper_abstract\", \"paper\"]\n",
    "\n",
    "for id in types:\n",
    "    df = load_and_merge(id=id, number=n_files)\n",
    "    print(f\"There are {df.shape} elements in {id}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from week 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1) \n",
    "Share the number of authors you will use as starting point in this exercises. Add a comment clarifying how many IC2S2 editions you included and if the collaborators were included or not.\n",
    "\n",
    "We used all 2133 authors form week 1 as a starting point, but 181 of these could not be found in the semantic scholar database. We found 118 951 coauthors (or collaborators) to these 1952 (2133 - 181) with themselves included.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3) \n",
    "How long is your final Author dataframe? How long is your final Paper dataframe?\n",
    "\n",
    "XXXXX of the authors returned errors when we asked semantic scholar for their papers and they have been removed. This leaves us with a dataframe of authers with XXXX entries. </br>\n",
    "Together these XXXXXX have written XXXXX unique papers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions (week 3)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for generating distributions and plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot function that makes histograms and cumulative mean and median plots. \n",
    "def create_plot(data, mean, median, show_hist=False, plot=True, safeFig = False, n = 10000, name = \"Distribution name\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (list): List of datapoints sampled from a distribution\n",
    "        mean (int): The mean value of the distribution \n",
    "        median (int): The median value of the distribution \n",
    "        show_hist (bool): If True create a histogram showing the data given\n",
    "        plot (bool): If True create a two plots one showing the cummulative mean and with error bars and the distribution mean\n",
    "        safeFig (bool): If True safe the figure locally. Only works if show_hist or plot is True\n",
    "        n (int): Number of sampled from the distribution\n",
    "        name (str): Name of the distribution\n",
    "    \"\"\"\n",
    "    # Data saved as dataframe\n",
    "    Norm_p = pd.DataFrame(data)\n",
    "\n",
    "    if show_hist:\n",
    "        # Create histogram\n",
    "        plt.hist(data, histtype='step')\n",
    "        plt.title(\"Histogram\")\n",
    "        plt.show()\n",
    "\n",
    "    if plot:\n",
    "        # Setup for the plots, calculation of key numbers\n",
    "        Cum_Mean = [Norm_p[0][:i].mean() for i in range(len(data))]\n",
    "        Cum_std_err = [Norm_p[0][:i].std() / np.sqrt(i) for i in range(1, len(data) + 1)]\n",
    "        \n",
    "        # starts at one as when viewed it makes sense to have the first element be 1\n",
    "        x_vals = list(range(1, len(Cum_Mean) + 1))\n",
    "\n",
    "        # Setup for the plots\n",
    "        fig, axs = plt.subplots(2, figsize = (12,8))\n",
    "        fig.tight_layout(pad= 3)\n",
    "\n",
    "        # First value is excluded as standard error of a single point is Nan\n",
    "        ye = np.array(Cum_Mean)[1:]\n",
    "        yerr = np.array(Cum_std_err[1:])\n",
    "\n",
    "        # Creating plot for mean\n",
    "        axs[0].set_title(\"Mean for \" + name +\" distribution\")\n",
    "        axs[0].errorbar(x_vals[1:], ye, yerr=yerr, label='Error bars', linewidth=0.5, ecolor = \"blue\", fmt='none')\n",
    "        axs[0].plot(x_vals, Cum_Mean, '-', label = \"Cumulative mean\", linewidth=1, color = \"red\")\n",
    "        axs[0].plot([1, n], [mean, mean], label = \"Distribution mean\", linewidth=2, color=\"orange\")\n",
    "        axs[0].legend(loc = \"upper left\", ncol = 3)\n",
    "\n",
    "        Cum_Median = Norm_p.rolling(len(Norm_p), min_periods=2).median()\n",
    "        Cum_Median[0][0] = Norm_p[0][0]\n",
    "        ye = np.array(Cum_Median)\n",
    "\n",
    "        # Creating plot for median\n",
    "        axs[1].set_title(\"Median for \" + name +\" distribution\")\n",
    "        axs[1].plot(x_vals, ye, label = \"Cumulative median\", color=\"red\")\n",
    "        axs[1].plot([1, n], [median, median], label = \"Distribution mean\", color=\"orange\")\n",
    "        axs[1].legend(ncol = 2)\n",
    "\n",
    "        # Give plots more information\n",
    "        fig.supxlabel(\"Number of elements used\")\n",
    "        plt.setp(axs[0], ylabel = \"Mean\")\n",
    "        plt.setp(axs[1], ylabel = \"Median\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if safeFig:\n",
    "            # Save image of the plot locally\n",
    "            plt.savefig(\"my_data/\" + name +\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data, log = False, remove_empty = False, n_bins = 50):\n",
    "    \"\"\"\n",
    "    Creates a histogram from the data given\n",
    "    \n",
    "    Args:\n",
    "        data(List): List with datapoints\n",
    "        log(Bool): If True the plots will be in log-log scale\n",
    "        remove_empty(Bool): Removes empty bins from the plot\n",
    "        n_bins(Int): Number of bins for the hist\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup. Creating bins according to the given data\n",
    "    if log:\n",
    "        bins = np.logspace(np.log10(max(1, int(min(data)))), max(1, np.log10(np.ceil(max(data)) + int(max(data)/20))), n_bins)\n",
    "        hist, edges = np.histogram(data, bins = bins, density = True) # Probability density to not penalise the first bins for being smaller.\n",
    "    else:\n",
    "        bins = np.linspace(int(min(data)), np.ceil(max(data)) + int(max(data)/20), n_bins)\n",
    "        hist, edges = np.histogram(data, bins = bins)\n",
    "\n",
    "    x = (edges[:-1] + edges[1:])/2\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Remove bins without any points in\n",
    "    if remove_empty:\n",
    "        xx,yy = zip(*[(i,j) for (i,j) in zip(x, hist) if j > 0])\n",
    "        # ax.bar(xx, yy, width = width)\n",
    "        ax.plot(xx,yy, marker = '.')\n",
    "    else:\n",
    "        #ax.bar(x, hist, width = width * 0.98)\n",
    "        ax.plot(x, hist, marker = '.')\n",
    "\n",
    "    ax.set_xlabel('Citation counts')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.title('Probability density function for citation counts')\n",
    "    \n",
    "    #Sets the histogram to log-log scale\n",
    "    if log:\n",
    "        ax.set_ylabel('Probability density')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data distributions \n",
    "# Generate distributions \n",
    "np.random.seed(10)\n",
    "n = 10000\n",
    "n2 = 500\n",
    "\n",
    "# Normal distribution \n",
    "normal_mu = 0 \n",
    "normal_std = 4\n",
    "normal = np.random.normal(normal_mu, normal_std, size=(n))\n",
    "\n",
    "# pareto \n",
    "pareto_alpha = 0.5 \n",
    "pareto = np.random.pareto(pareto_alpha, size=n)\n",
    "pareto_mean = None \n",
    "pareto_median = 1*(2)**(1/pareto_alpha)\n",
    "\n",
    "# lognormal \n",
    "lognormal_mu = 0 \n",
    "lognormal_std = 4\n",
    "lognormal = np.random.lognormal(lognormal_mu, lognormal_std, size= n)\n",
    "lognormal_mean = np.exp(lognormal_mu + lognormal_std**2 / 2)\n",
    "lognormal_median = np.exp(lognormal_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normal\n",
    "create_plot(normal, mean=normal_mu, median=normal_mu, n=n, name=\"normal\")\n",
    "create_plot(normal[:n2], mean=normal_mu, median=normal_mu, n=n2, name=\"normal (500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pareto \n",
    "create_plot(pareto, mean=pareto_mean, median=pareto_median, n=n, name=\"pareto\")\n",
    "create_plot(pareto[:n2], mean=pareto_mean, median=pareto_median, n=n2, name=\"pareto (500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lognormal\n",
    "create_plot(lognormal, mean=lognormal_mean, median=lognormal_median, n=n, name=\"lognormal\")\n",
    "create_plot(lognormal[:n2], mean=lognormal_mean, median=lognormal_median, n=n2, name=\"logmormal (500)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample citation counts from the 2009 computational social science papers\n",
    "df_papers = pd.read_csv(\"DataFoundPreviously/df_paper.csv\")\n",
    "mask_2009 = df_papers['year'] == 2009\n",
    "citationCount_all = np.array(df_papers['citationCount'][mask_2009])\n",
    "citations_mean = np.mean(citationCount_all)\n",
    "citations_median = np.median(citationCount_all)\n",
    "sampledCitationCount = np.random.choice(citationCount_all, n)\n",
    "\n",
    "create_plot(sampledCitationCount, mean=citations_mean, median=citations_median, n=n, name=\"citations\")\n",
    "create_plot(sampledCitationCount[:n2], mean=citations_mean, median=citations_median, n=n2, name=\"citations (500)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) \n",
    "Compare the evolution of the cumulative average for the Gaussian, Pareto and LogNormal distribution. What do you observe? Would you expect these results? Why?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) \n",
    "Compare the cumulative median vs the cumulative average for the three distributions. What do you observe? Can you draw any conclusions regarding which statistics (the mean or the median) is more usfeul in the different cases?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3) \n",
    "Consider the plots you made using the citation count data in point 14. What do you observe? What are the implications?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4) \n",
    "What do you think are the main take-home message of this exercise?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eedd9fc418f36e2614580cbf31de2386521dabe9ec8a80375aa0fadc40a33bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
